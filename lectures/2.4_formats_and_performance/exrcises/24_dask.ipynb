{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b6a924-a619-49b8-b26f-c586578f4604",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/EO-College/cubes-and-clouds/main/icons/cnc_3icons_process_circle.svg\"\n",
    "     alt=\"Cubes & Clouds logo\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf518a7-f847-460c-b411-81b100cc7954",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/pangeo-data/pangeo.io/refs/heads/main/public/Pangeo-assets/pangeo_logo.png\"\n",
    "     alt=\"Pangeo logo\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {},
   "source": [
    "# 2.4 Scaling with Pangeo\n",
    "\n",
    "## Parallel computing with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Overview</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is Dask?</li>\n",
    "        <li>How can I parallelize my data analysis with Dask?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about Dask</li>\n",
    "        <li>Learn about Dask Gateway, Dask Client, Scheduler, Workers</li>\n",
    "        <li>Understand out-of-core and speed-up limitations</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "We will be using [Dask](https://docs.dask.org/) with [Xarray](https://docs.xarray.dev/en/stable/) to parallelize our data analysis.  We continue to use the snow index example.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c600794-dd9e-400b-bd09-dbb6e7039dad",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following Python packages:\n",
    "\n",
    "- pooch {cite:ps}`e-pooch-Uieda2020`\n",
    "- s3fs {cite:ps}`e-s3fs-2016`\n",
    "- xarray {cite:ps}`e-xarray-hoyer2017` with [`netCDF4`](https://pypi.org/project/h5netcdf/) and [`h5netcdf`](https://pypi.org/project/h5netcdf/) engines\n",
    "- hvplot {cite:ps}`e-holoviews-rudiger2020`\n",
    "- dask {cite:ps}`e-dask-2016`\n",
    "- graphviz {cite:ps}`e-graphviz-Ellson2003`\n",
    "- numpy {cite:ps}`e-numpy-harris2020`\n",
    "- pandas {cite:ps}`e-pandas-reback2020`\n",
    "- geopandas {cite:ps}`e-geopandas-jordahl2020`\n",
    "\n",
    "In the jupyter-hub configuration you use for this MOOC series, all the package necessary for executing this notebook are already installed. In case you create your own enviroment, for example on your PC, please install these packages if not already available in your Python environmen.\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd73e3-0120-4457-b84b-ccd9325a72a6",
   "metadata": {},
   "source": [
    "## Parallelize with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35db696-501a-4110-a7fa-fafe92dfc0a2",
   "metadata": {},
   "source": [
    "We know from the previous excerise [2.4_chunking](./chunking.ipynb) that chunking is key to analyzing large data sets. In this excersise, we will learn to parallelize our data analysis using [Dask](https://docs.dask.org/) on our chunked dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f3544-8b08-462d-b8a4-c6c0c2f68a26",
   "metadata": {},
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** scales the existing Python ecosystem: with very or no changes in your code, you can speed-up computation using Dask or process bigger than memory datasets.\n",
    "\n",
    "- Dask is a flexible library for parallel computing in Python.\n",
    "- It is widely used for handling large and complex Earth Science datasets and speed up science.\n",
    "- Dask is powerful, scalable and flexible. It is the leading platform today for data analytics at scale.\n",
    "- It scales natively to clusters, cloud, HPC and bridges prototyping up to production.\n",
    "- The strength of Dask is that is scales and accelerates the existing Python ecosystem e.g. Numpy, Pandas and Scikit-learn with few effort from end-users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44c561-da09-4051-b562-bca3c276659e",
   "metadata": {},
   "source": [
    "#### How does Dask scale and accelerate your data analysis?\n",
    "\n",
    "[Dask proposes different abstractions to distribute your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html). In this _Dask Introduction_ section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in Pangeo ecosystem as a back end of Xarray.\n",
    "\n",
    "As shown in the [previous section](./chunking_introduction.ipynb) Dask Array is based on chunks.\n",
    "Chunks of a Dask Array are well-known Numpy arrays. By transforming big datasets to Dask Array, making use of chunk, a large array is handled as many smaller Numpy ones and we can compute each of these chunks independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595d939-199f-4cbd-8107-2b4480a993ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Note</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>`Xarray` uses Dask Arrays instead of Numpy when chunking is enabled, and thus all Xarray operations are performed through Dask, which enables distributed processing. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9fd40-08f4-43e2-897f-b1e745fd215a",
   "metadata": {},
   "source": [
    "#### How does Xarray with Dask distribute data analysis?\n",
    "\n",
    "When we use chunks with `Xarray`, the real computation is only done when needed or asked for, usually when invoking `compute()` or `load()` functions. Dask generates a **task graph** describing the computations to be done. When using [Dask Distributed](https://distributed.dask.org/en/stable/) a **Scheduler** distributes these tasks across several **Workers**.\n",
    "\n",
    "![Xarray with dask](../assets/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11377e82-113a-4260-8bc6-cc6973a6bab0",
   "metadata": {},
   "source": [
    "#### What is a Dask Distributed cluster ?\n",
    "\n",
    "A Dask Distributed cluster is made of two main components:\n",
    "\n",
    "- a Scheduler, responsible for handling computations graph and distributing tasks to Workers.\n",
    "- One or several (up to 1000s) Workers, computing individual tasks and storing results and data into distributed memory (RAM and/or worker's local disk).\n",
    "\n",
    "A user usually needs __Client__ and __Cluster__ objects as shown below to use Dask Distributed.    \n",
    "\n",
    "![Dask Distributed Cluster](https://user-images.githubusercontent.com/306380/66413985-27111600-e9be-11e9-9995-8f418ff48f8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f6d9c-5927-4d53-9125-432cfe42fef9",
   "metadata": {},
   "source": [
    "#### Where can we deploy a Dask distributed cluster?\n",
    "\n",
    "[Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.).](https://docs.dask.org/en/stable/deploying.html)  Dask distributed `Cluster` object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n",
    "\n",
    "![Dask Cluster deployment](https://docs.dask.org/en/stable/_images/dask-cluster-manager.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4ff9d-b390-49f9-a0e6-622170d955af",
   "metadata": {},
   "source": [
    "> Tip  \n",
    "> A Dask `cluster` can be created on a single machine (e.g. your laptop), which means you do not need dedicated computing resources. This corresponds to **Vertical scaling** as described in [2.4 Formats and Performance](../2.4_formats_and_performance.md).\n",
    ">\n",
    "> However, the speedup is limited to the resources of your single machine. This is where **horizontal scaling** comes in.  By adding resources from other machines, you can expand your computing power!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52be3e-3924-4d0b-9a46-abd9009e2a3b",
   "metadata": {},
   "source": [
    "### Dask distributed Client\n",
    " \n",
    "The Dask distributed `Client` is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a `Client` object. Once a `Client` has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n",
    "\n",
    "No matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n",
    "\n",
    "Using `Client()` function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the 'local' machine. Here, during the workshop, we are running this notebook in Pangeo EOSC cloud deployment, so the 'local' machine is the jupyterlab you are using at the Cloud, and the number of cores is the number of cores on the cloud computing resources you've been given (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879754c-8ede-43ed-86ef-05cd9c288e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()   # create a local dask cluster on the local machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eb7fa-76b6-4a22-a139-3eb53989645d",
   "metadata": {},
   "source": [
    "Inspecting the `Cluster Info` section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736568ed-7e5b-4184-bf7a-26e1d2426f93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> You can also create a local cluster with the `LocalCluster` constructor and use `n_workers` \n",
    "        and `threads_per_worker` to manually specify the number of processes and threads you want to use. \n",
    "        For instance, we could use `n_workers=2` and `threads_per_worker=2`.  </li>\n",
    "        <li> This is sometimes preferable (in terms of performance), or when you run this tutorial on your PC, \n",
    "        you can avoid dask to use all your resources you have on your PC!  </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d1b16-99b6-4b21-939d-b4a3bf264cbf",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "\n",
    "Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separate tab of your browser.\n",
    "\n",
    "We will learn here how to use it through the [dask jupyterlab extension](https://github.com/dask/dask-labextension).  \n",
    "\n",
    "To use Dask Dashboard through jupyterlab extension on EOX infrastructure,\n",
    "you just need to click on the orange icon shown in the following figure,\n",
    "\n",
    "![Dask.array](../assets/dashboardlink.png)\n",
    "\n",
    "You can click several buttons marked with blue arrows in the following images, and then drag and drop them wherever you want.  \n",
    "\n",
    "![Dask.array](../assets/dasklab.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Dask.array](../assets/exampledasklab.png)\n",
    "\n",
    "It's really helpful to understand your computation and how it's distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171f9b8-eb49-43ee-9d3f-5b8d74434e1f",
   "metadata": {},
   "source": [
    "## Dask Distributed computations on our dataset\n",
    "\n",
    "Let's open the virtual dataset we've prepared as in previous episode, select a single location over time, visualize the task graph generated by Dask, and observe the Dask Dashboard.\n",
    "\n",
    "### Read from online kerchunked consolidated dataset\n",
    "\n",
    "We will access Long Term TimeSeries of NDVI statistics from OpenStack Object Storage using the Zarr metadata generated with kerchunk, prepared in [previous chunking_introduction](./chunking_introduction.ipynb) section using intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf8517-c4bc-412d-af68-9013c6c169bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog('../data/LTS.yaml')\n",
    "LTS=cat.LTS.to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e050c-6182-406b-aa40-9c17ee77f834",
   "metadata": {},
   "source": [
    "By inspecting any of the variables on the representation above, you'll see that each data array represents __about 85GiB of data__, so much more than the availabe memory on this notebook server, and even on the Dask Cluster we created above. But thanks to chunking, we can still analyze it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7190e-5596-413c-a066-cfe214e933f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save = LTS.sel(lat=45.50, lon=9.36, method='nearest')['min'].mean()\n",
    "save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b90d-22d6-404f-8c3d-3c557410d601",
   "metadata": {},
   "source": [
    "Did you notice something on the Dask Dashboard when running the two previous cells?\n",
    "\n",
    "We didn't 'compute' anything. We just built a Dask task graph with it's size indicated as count above, but did not ask Dask to return a result.\n",
    "\n",
    "Here, you can check 'Dask graph' with how many layers of graph you have, to estimate the complexity of your computation.\n",
    "\n",
    "It is indicated that you have '7 graph'.  this can be optimized with following step\n",
    "\n",
    "Let's try to plot the dask graph before computation and understand what dask workers will do to compute the value we asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77254f5a-7001-4fcd-a88e-af4e9a6d96fd",
   "metadata": {},
   "source": [
    "### Optimize the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8c5ab-eda3-4d1c-a2dd-2e616c0d9ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "(save,) = dask.optimize(save)\n",
    "save.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cd461-8f9d-4651-9190-73d5eb6a40ef",
   "metadata": {},
   "source": [
    "Now our graph is reduced 1. Lets try to visualise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395eadb-769e-4d49-9b75-dbaf0e1360ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698f61f-32bf-4ccc-bfcf-2f58de316431",
   "metadata": {},
   "source": [
    "### Compute on the dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45148205-16b9-42a4-aaba-938c5cea593c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6539a-0433-481b-a080-6e9eb7b80c1d",
   "metadata": {},
   "source": [
    "Calling compute on our Xarray object triggered the execution on Dask Cluster side. \n",
    "\n",
    "You should be able to see how Dask is working on Dask Dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c54df-87c4-4494-bed2-2a82fb00aa24",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go Further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>You can re-open the LTS with chunks=({\"time\":-1}) option, and try to visualize the task graph, size of each chunk.  How has it changed?  Do you see the difference of task graph using chunks keyword argument when opening the dataset?  </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4e8f1-fc6b-4c6f-b7bb-b7a61a3d944d",
   "metadata": {},
   "source": [
    "### Close client to terminate local dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8d36a-a4cd-434c-a1bf-cc1850d2a15a",
   "metadata": {},
   "source": [
    "The `Client` and associated `LocalCluster` object will be automatically closed when your Python session ends. When using Jupyter notebooks, we recommend to close it explicitely whenever you are done with your local Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11f2cf-7418-43de-9b45-847556474b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e06ec-f40a-4ed4-b0b0-61db5232e115",
   "metadata": {},
   "source": [
    "## Scaling your Computation using Dask Gateway.\n",
    "\n",
    "For this workshop, according to the Pangeo EOSC deployment, you will learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing to run our data analysis in parallel e.g. distribute tasks across several workers.\n",
    "\n",
    "Lets set up your Dask cluster through Dask Gateway.  \n",
    "As Dask Gateway is configured by default on this infrastructure, you just need to execute the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab00293-fc51-409b-8bb6-10a6e04c7249",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c4cab-a236-4501-ab38-d7fd2e3830dc",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "#WARNING In case you already created gateway cluster, you will see list of your clusters. \n",
    "#And this cell will kill all your orphan clusters.\n",
    "#Please clean them before you make a new cluster using following command \n",
    "clusters = gateway.list_clusters()\n",
    "print(clusters)\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster = gateway.connect(cluster.name)\n",
    "    cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980455f-e639-47e9-8d53-5f47b00370d2",
   "metadata": {},
   "source": [
    "### Create a new Dask cluster with the Dask Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624b389-1add-4476-8aa2-58087c409571",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster()\n",
    "cluster.scale(4)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78693fbb-c012-47da-8a7b-4f2c7256f9e1",
   "metadata": {},
   "source": [
    "Let's setup the Dask Dashboard with your new cluster.\n",
    "\n",
    "***This time, just click on the link to open the dashboard into another tab.  Then copy and past the link of web site appearing to the dask lab-extension***\n",
    "\n",
    "![daskgateway click](../figures/daskgatewayclick.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3742e-2d4b-48a3-a2da-ee67d019dc51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get a client from the Dask Gateway Cluster\n",
    "\n",
    "As stated above, creating a Dask `Client` is mandatory in order to perform following Daks computations on your Dask Cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f035e50-744e-4ba3-addd-cfe0f3fc9616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Please don't execute this cell, it is needed for building the Jupyter Book\n",
    "cluster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f05e7-0827-4f8a-a6d3-a8c8abfbf42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "if cluster:\n",
    "    client = Client(cluster) # create a dask Gateway cluster\n",
    "else:\n",
    "    client = Client()   # create a local dask cluster on the machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4818f-2b6b-44c1-af77-97daf8a1c2a1",
   "metadata": {},
   "source": [
    "## Global LTS computation\n",
    "\n",
    "In the previous episode, we used Long-term Timeseries for the region of Lombardy e.g. a very small area that was extracted upfront for simplicity. Now we will use the original dataset that has a global coverage, and work directly on it to extract our AOI and perform computations.\n",
    "\n",
    "Let's check our LTS data we have loaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a89018c-ade6-4e73-a02d-c98347089a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c6e096",
   "metadata": {},
   "source": [
    "### Fix time coordinate\n",
    "\n",
    "As observed data are coming with a predefined year. To let xarray automatically align the LTS with the latest NDVI values, the time dimension needs to be shifted to the NDVI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49330709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dates_2022 = pd.date_range('20220101', '20221231')\n",
    "time_list = dates_2022[np.isin(dates_2022.day, [1,11,21])]\n",
    "LTS = LTS.assign_coords(time=time_list)\n",
    "LTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893cdc32",
   "metadata": {},
   "source": [
    "### Clip LTS over Lombardia\n",
    "As in previous episodes, we use a shapefile over Italy to select data over this Area of Interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e61dab-f88b-4f97-b3c6-ec3f4e0a2ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "try:\n",
    "    GAUL = gpd.read_file('../data/Italy.geojson')\n",
    "except:\n",
    "    GAUL = gpd.read_file('zip+https://mars.jrc.ec.europa.eu/asap/files/gaul1_asap.zip') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955a5cc-1e38-4e1a-bd42-587da11731b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AOI_name = 'Lombardia'\n",
    "AOI = GAUL[GAUL.name1 == AOI_name]\n",
    "AOI_poly = AOI.geometry\n",
    "AOI_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cdd8f",
   "metadata": {},
   "source": [
    "We first select a geographical area that covers Lombardia (so that we have a first reduction from the global coverage) and then clip using the shapefile to avoid useless pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760227fe-a042-4d05-a23d-5cf0f6a5585d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LTS_AOI = LTS.sel(lat=slice(46.5,44.5), lon=slice(8.5,11.5))\n",
    "LTS_AOI.rio.write_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c309349-4b90-4447-a48f-9ad03c3679c2",
   "metadata": {},
   "source": [
    "We apply a mask using rio.clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011f8f6-12f2-4b5e-85dd-59c6c1de8fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LTS_AOI = LTS_AOI.rio.clip(AOI_poly, crs=4326)\n",
    "LTS_AOI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335361d6-7389-4b97-a3c4-eb706692dba9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Lets keep our LTS_Lombardia on the memory of your Dask cluster, distributed on every workers.</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> To do that we will use `.persist()`. Please look at the dashboard during the computation.\n",
    " </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6c8be-448c-43f0-bcac-b40faa02a2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "LTS_AOI.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18285df-862d-487c-ab2f-57823907a3f1",
   "metadata": {},
   "source": [
    "## Get NDVI for 2022 over Lombardia\n",
    "\n",
    "We re-use the file we created during the first episode. If the file is missing it will be downloaded from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d333a4-228c-4eff-9252-94e13c0a7828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pooch\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    cgls_ds = xr.open_dataset('../data/C_GLS_NDVI_20220101_20220701_Lombardia_S3_2_masked.nc')\n",
    "except:\n",
    "    cgls_file = pooch.retrieve(\n",
    "        url=\"https://zenodo.org/record/6969999/files/C_GLS_NDVI_20220101_20220701_Lombardia_S3_2_masked.nc\",\n",
    "        known_hash=\"md5:be3f16913ebbdb4e7af227f971007b22\",\n",
    "        path=f\".\",)    \n",
    "    cgls_ds = xr.open_dataset(cgls_file)\n",
    "cgls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224f971-6ab1-4db3-992f-a64dce84d04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NDVI_AOI = cgls_ds.NDVI.rio.write_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d385fbe-26a2-4f67-9ec7-026beb52dfba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NDVI_AOI = NDVI_AOI.rio.clip(AOI_poly, crs=4326)\n",
    "NDVI_AOI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b3d48-a0ed-4ae8-8b7f-48e55823e8ee",
   "metadata": {},
   "source": [
    "The nominal spatial resolution of the Long term statistics is 1km. As the current NDVI product has a nominal spatial resolution of 300m a re projection is needed. RioXarray through RasterIO that wraps the GDAL method can take care of this. More info about all the options can be found [here](https://rasterio.readthedocs.io/en/stable/api/rasterio.warp.html#rasterio.warp.reproject)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80a2e8-80ba-44e3-a51b-e8476b465a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NDVI_1k = NDVI_AOI.rio.reproject_match(LTS_AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017d9ac-5d82-4280-9f4c-92ce7622ef56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NDVI_1k = NDVI_1k.rename({'x': 'lon', 'y':'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229b0ef-13d1-4443-b5bd-708eaf8b8be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VCI = ((NDVI_1k - LTS_AOI['min']) / (LTS_AOI['max'] - LTS_AOI['min'])) * 100\n",
    "VCI.name = 'VCI'\n",
    "VCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bfc7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from hvplot import xarray\n",
    "VCI.hvplot(x = 'lon', y = 'lat',\n",
    "           cmap='RdYlGn', clim=(-200,+200), alpha=0.7,\n",
    "           geo=True, tiles= 'CartoLight',\n",
    "           title=f'CGLS VCI {AOI_name} {VCI.isel(time=-1).time.dt.date.data}',\n",
    "           width=400, height=300,\n",
    "           widget_location='left_top'\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56545676-100a-4acd-82e0-70f661f8a956",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Exercise</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> Try moving time slider to see how Dask computes and load data on the fly (observe well the dask dashboard!) </li>          \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a5138-57f9-4afa-a445-79cc2cbaf74c",
   "metadata": {},
   "source": [
    "### Visualize LTS statistics\n",
    "\n",
    "Let's try to scale out the visualization of LTS statistic datas.  We will set an arbitaly size to see how dask behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596c901-56ec-4ceb-8732-e2f0a6ea554c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size=0  # You can try later, for example size=10, 50, 100 \n",
    "LTS_plot=LTS.sel(lat=slice(80,20), lon=slice(-15,30+size))#.min(dim='time')\n",
    "LTS_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa301ba0-637d-41b8-bbe9-a3733e69c2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import hvplot.xarray    \n",
    "plots = [LTS_plot[z].hvplot.image(x = 'lon', y = 'lat',\n",
    "           cmap='RdYlGn', clim=(0.0,0.9)\n",
    "           , alpha=0.7,rasterize=True,\n",
    "           geo=True, tiles= 'CartoLight',\n",
    "           width=400, height=300)  for z in ['min','max']]\n",
    "hv.Layout(plots).cols(1).opts(title='LTS NDVI statistics (Minimum and Maximum)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05dbb2-42b5-4973-be58-3b5d7c5e5b9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go Further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Compare the data size and 'used' data size for each worker in dask dashboard  </li>   \n",
    "        <li>Lets try to zoom.  What happened with your plot? How was the dask dashboard reacted with zooming?   </li>\n",
    "        <li>What is rastersize=True ? (Hint: https://hvplot.holoviz.org/user_guide/Customization.html#datashading-options)   </li>  \n",
    "        <li>Let's try to scale out using 'cluster.scale(6)' and use size=10 (or 50, 100...) </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67cb31-b8f2-4d91-a37f-95d2a4ca328b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499dae2-f69e-4764-b1fd-fb3aef29e29d",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56304d1c",
   "metadata": {},
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"dask\" and topic % \"package\"\n",
    ":keyprefix: e-\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurodatacube21-pangeo",
   "language": "python",
   "name": "conda-env-eurodatacube21-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
