{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b6a924-a619-49b8-b26f-c586578f4604",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/EO-College/cubes-and-clouds/main/icons/cnc_3icons_process_circle.svg\"\n",
    "     alt=\"Cubes & Clouds logo\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf518a7-f847-460c-b411-81b100cc7954",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/pangeo-data/pangeo.io/refs/heads/main/public/Pangeo-assets/pangeo_logo.png\"\n",
    "     alt=\"Pangeo logo\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {},
   "source": [
    "# 2.4 Scaling with Pangeo\n",
    "\n",
    "## Parallel computing with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Overview</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is Dask?</li>\n",
    "        <li>How can I parallelize my data analysis with Dask?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about Dask</li>\n",
    "        <li>Learn about Dask Gateway, Dask Client, Scheduler, Workers</li>\n",
    "        <li>Understand out-of-core and speed-up limitations</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "We will be using [Dask](https://docs.dask.org/) with [Xarray](https://docs.xarray.dev/en/stable/) to parallelize our data analysis.  We will continue to use the NDVI example.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd73e3-0120-4457-b84b-ccd9325a72a6",
   "metadata": {},
   "source": [
    "## Parallelize with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35db696-501a-4110-a7fa-fafe92dfc0a2",
   "metadata": {},
   "source": [
    "We know from the previous exercise **2.4_chunking** that chunking is key to analyzing large data sets. In this exercise, we will learn how to parallelize our data analysis using [Dask](https://docs.dask.org/) on our chunked dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f3544-8b08-462d-b8a4-c6c0c2f68a26",
   "metadata": {},
   "source": [
    "### What is [Dask](https://docs.dask.org/) ?\n",
    "\n",
    "**Dask** is a powerful and flexible library for parallel computing in Python. It enables users to scale their computations effortlessly while working with large datasets that exceed available memory.\n",
    "\n",
    "- With minimal code changes, Dask accelerates existing Python libraries such as NumPy, Pandas, and Scikit-learn.\n",
    "- Designed for processing massive datasets, Dask is widely used in Earth Science and other data-intensive fields.\n",
    "- It scales effortlessly from a single laptop to clusters, cloud environments, and HPC systems.\n",
    "- Dask bridges the gap between exploratory analysis and large-scale production workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44c561-da09-4051-b562-bca3c276659e",
   "metadata": {},
   "source": [
    "#### How does Dask scale and accelerate your data analysis?\n",
    "\n",
    "[Dask proposes different abstractions to distribute your computation](https://docs.dask.org/en/stable/10-minutes-to-dask.html). In this _Dask Introduction_ section, we will focus on [Dask Array](https://docs.dask.org/en/stable/array.html) which is widely used in the Pangeo ecosystem as a backend of Xarray.\n",
    "\n",
    "The chunks of a Dask Array are standard NumPy arrays. By transforming large datasets into Dask Arrays and utilizing chunks, a large array is handled as many smaller NumPy arrays. This allows us to compute each of these chunks independently.\n",
    "\n",
    "![Dask and Numpy](https://examples.dask.org/_images/dask-array-black-text.svg)  \n",
    "*Image source: Dask documentation (© 2025 Dask core developers, BSD-3 Licensed).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595d939-199f-4cbd-8107-2b4480a993ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Note</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Xarray uses Dask Arrays instead of Numpy when chunking is enabled, and thus all Xarray operations are performed through Dask, which enables distributed processing. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9fd40-08f4-43e2-897f-b1e745fd215a",
   "metadata": {},
   "source": [
    "#### How does Xarray with Dask distribute data analysis?\n",
    "\n",
    "When we use chunks with Xarray, the real computation is only done when needed or explicitly requested, usually by invoking the `compute()` or `load()` functions. Dask generates a **task graph** describing the computations to be performed. When using [Dask Distributed](https://distributed.dask.org/en/stable/), a **Scheduler** distributes these tasks across several **Workers**.\n",
    "\n",
    "![Xarray with dask](https://raw.githubusercontent.com/EO-College/cubes-and-clouds/refs/heads/main/lectures/2.4_formats_and_performance/exercises/assets/dask-xarray-explained.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11377e82-113a-4260-8bc6-cc6973a6bab0",
   "metadata": {},
   "source": [
    "#### What is a Dask Distributed cluster?\n",
    "\n",
    "A Dask Distributed cluster is made up of two main components:\n",
    "\n",
    "- A __Scheduler__, responsible for handling the computation graph and distributing tasks to Workers.\n",
    "- One or several __Workers__, which compute individual tasks and store results and data in distributed memory (RAM and/or the worker's local disk).\n",
    "\n",
    "A user typically needs __Client__ and __Cluster__ objects, as shown below, to use Dask Distributed.\n",
    "\n",
    "\n",
    "![Dask Distributed Cluster](https://user-images.githubusercontent.com/306380/66413985-27111600-e9be-11e9-9995-8f418ff48f8a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f6d9c-5927-4d53-9125-432cfe42fef9",
   "metadata": {},
   "source": [
    "#### Where can we deploy a Dask distributed cluster?\n",
    "\n",
    "[Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.).](https://docs.dask.org/en/stable/deploying.html)  Dask distributed `Cluster` object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n",
    "\n",
    "![Dask Cluster deployment](https://docs.dask.org/en/stable/_images/dask-cluster-manager.svg)*Image source: Dask documentation (© 2025 Dask core developers, BSD-3 Licensed).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4ff9d-b390-49f9-a0e6-622170d955af",
   "metadata": {},
   "source": [
    "> Tip  \n",
    "> A Dask `Cluster` can be created on a single machine (e.g. your laptop), which means you do not need dedicated computing resources. This corresponds to **Vertical scaling** as described in lecture [2.4 Formats and Performance](../2.4_formats_and_performance.md).\n",
    ">\n",
    "> However, the speed up is limited to the resources of your single machine. This is where **horizontal scaling** comes in.  By adding resources from other machines, you can expand your computing power!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa52be3e-3924-4d0b-9a46-abd9009e2a3b",
   "metadata": {},
   "source": [
    "### Dask distributed Client\n",
    " \n",
    "The Dask distributed `Client` is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a `Client` object. Once a `Client` has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n",
    "\n",
    "No matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n",
    "\n",
    "Using `Client()` function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the 'local' machine. The 'local' machine is the jupyterlab you are using in the Cloud, and the number of cores is the number of cores available in the cloud instance (not on your laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879754c-8ede-43ed-86ef-05cd9c288e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)   # create a local dask cluster on the local machine specifying 4\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934eb7fa-76b6-4a22-a139-3eb53989645d",
   "metadata": {},
   "source": [
    "Inspecting the `Cluster Info` section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736568ed-7e5b-4184-bf7a-26e1d2426f93",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go further</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li> You can also create a local cluster with the `LocalCluster` constructor and use `n_workers` \n",
    "        and `threads_per_worker` to manually specify the number of processes and threads you want to use. \n",
    "        For instance, we could use `n_workers=2` and `threads_per_worker=2`.  </li>\n",
    "        <li> This is sometimes preferable (in terms of performance), or when you run this tutorial on your PC, \n",
    "        you can avoid dask to use all your resources you have on your PC!  </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d1b16-99b6-4b21-939d-b4a3bf264cbf",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "\n",
    "Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separate tab of your browser.\n",
    "\n",
    "We will learn how to use it through the [dask jupyterlab extension](https://github.com/dask/dask-labextension).  \n",
    "\n",
    "To use Dask Dashboard through jupyterlab extension,\n",
    "you just need to click on the orange icon shown in the following figure,\n",
    "\n",
    "![Dask.array](https://raw.githubusercontent.com/EO-College/cubes-and-clouds/refs/heads/main/lectures/2.4_formats_and_performance/exercises/assets/dashboardlink.png)\n",
    "\n",
    "You can click several buttons marked with blue arrows in the following images, and then drag and drop them wherever you want.  \n",
    "\n",
    "![Dask.array](https://raw.githubusercontent.com/EO-College/cubes-and-clouds/refs/heads/main/lectures/2.4_formats_and_performance/exercises/assets/dasklab.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Dask.array](https://raw.githubusercontent.com/EO-College/cubes-and-clouds/refs/heads/main/lectures/2.4_formats_and_performance/exercises/assets/exampledasklab.png)\n",
    "\n",
    "It's really helpful to understand your computation and how it's distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50cc16-4187-401c-b033-e96a7dca8ce3",
   "metadata": {},
   "source": [
    "## Dask Distributed computations on our dataset\n",
    "\n",
    "Let's open the Zarr dataset we've prepared in [previous chunking excersise](./24_chunking.ipynb), select a single location over time, visualize the task graph generated by Dask, and observe the Dask Dashboard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e862a-0f68-4835-9e97-457177a1a19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ndvi = xr.open_zarr('test.zarr')\n",
    "ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3448b-5b5d-4133-9f8b-9d98e8e4693e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi['data'].mean(dim='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08b90d-22d6-404f-8c3d-3c557410d601",
   "metadata": {},
   "source": [
    "## Did you notice something on the Dask Dashboard when running the two previous cells?\n",
    "\n",
    "We didn't actually 'compute' anything. Instead, we built a Dask task graph, with its size indicated as a count above, but we didn't ask Dask to return a result.\n",
    "\n",
    "You can check the 'Dask graph' to see how many layers it contains, which will help you estimate the complexity of your computation.\n",
    "\n",
    "It shows that you have '7 graphs'. This can be optimized in the following steps.\n",
    "\n",
    "Let's try to plot the Dask graph before computation and understand what the Dask workers will do to compute the result we asked for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b50b7-c96e-4c6e-aa16-61ed6c8dd543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi['data'].mean(dim='x').data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8120d-03b6-4722-b094-7f8d888ae171",
   "metadata": {},
   "source": [
    "Let's average the time dimension and see how it differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940ca93-95c3-48ee-9db9-18f9ae07a76a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi['data'].mean(dim='time').data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698f61f-32bf-4ccc-bfcf-2f58de316431",
   "metadata": {},
   "source": [
    "### Let's compute on the Dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45148205-16b9-42a4-aaba-938c5cea593c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi['data'].mean(dim='time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a6539a-0433-481b-a080-6e9eb7b80c1d",
   "metadata": {},
   "source": [
    "Calling compute on our Xarray object triggered the execution on Dask Cluster side. \n",
    "\n",
    "You should be able to see how Dask is working on Dask Dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4997ee-320a-4bf9-80ce-b9f3fd2201e5",
   "metadata": {},
   "source": [
    "### Let's try a bigger computation\n",
    "We will re-use the procedure we've learned in the previous chunking notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a46ed-7101-4d81-b584-0e08d72d7a19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load data using stackstac (with specific chunk) \n",
    "import stackstac\n",
    "import pystac_client\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import numpy as np\n",
    "import warnings\n",
    "import rioxarray\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "aoi = gpd.read_file('./assets/catchment_outline.geojson', crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(aoi.iloc[0].geometry)\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    intersects=aoi_geojson,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2019-02-01/2019-03-28\",\n",
    "    query= {\"proj:epsg\": dict(eq=32632)}\n",
    ").item_collection()\n",
    "\n",
    "ds = stackstac.stack(items,\n",
    "                     assets=['red','nir'],\n",
    "                     chunksize=(1,2,1024,1024)\n",
    "                    )\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d30d6-e3fb-4e4d-bb2a-25d96593b965",
   "metadata": {},
   "source": [
    "By inspecting any of the variables in the representation above, you'll see that the data array represents **over 188 GiB of data**, which is much more than the available memory on this notebook server or the Dask Local Cluster we created earlier. But thanks to chunking, we can still analyze it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbf09e-bbfb-4428-b261-c203fab2e618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Computing the NDVI\n",
    "def NDVI(data):\n",
    "    red = data.sel(band=\"red\")\n",
    "    nir = data.sel(band=\"nir\")\n",
    "    ndvi = (nir - red)/(nir + red)\n",
    "    return ndvi\n",
    "\n",
    "ndvi_xr = NDVI(ds)\n",
    "\n",
    "## Clip the data\n",
    "aoi_utm32 = aoi.to_crs(epsg=32632)\n",
    "geom_utm32 = aoi_utm32.iloc[0]['geometry']\n",
    "ndvi_xr.rio.write_crs(\"EPSG:32632\", inplace=True)\n",
    "ndvi_xr.rio.set_nodata(np.nan, inplace=True)\n",
    "ndvi_xr = ndvi_xr.rio.clip([geom_utm32])\n",
    "ndvi_xr.rio.set_nodata(np.nan, inplace=True)\n",
    "\n",
    "ndvi_xr = ndvi_xr.groupby(ndvi_xr.time.dt.floor('D')).max(skipna=True)\n",
    "ndvi_xr = ndvi_xr.chunk(chunks = {'x':'auto', 'y': 'auto'})\n",
    "ndvi_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c93fe7-d193-433f-aa03-2b6bdfc3ad04",
   "metadata": {},
   "source": [
    "Perform the same steps as in the previous exercise to convert to a Dataset and clean out problematic attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e782c2b-97c9-4a12-9ecf-3e97ba52c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_xr = ndvi_xr.to_dataset(name='data')\n",
    "\n",
    "def remove_attrs(obj, to_remove):\n",
    "    new = obj.copy()\n",
    "    new.attrs = {k: v for k, v in obj.attrs.items() if k not in to_remove}\n",
    "    return new\n",
    "\n",
    "def encode(obj):\n",
    "    object_coords = [name for name, coord in obj.coords.items() if coord.dtype.kind == \"O\"]\n",
    "    return obj.drop_vars(object_coords).pipe(remove_attrs, [\"spec\", \"transform\"])\n",
    "\n",
    "ndvi_xr = ndvi_xr.pipe(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c51464-8667-454f-ba23-92f88c36bcd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy\n",
    "with np.errstate(all=\"ignore\"):\n",
    "    ndvi_xr.to_zarr('ndvi.zarr',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4e8f1-fc6b-4c6f-b7bb-b7a61a3d944d",
   "metadata": {},
   "source": [
    "### Close client to terminate local dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8d36a-a4cd-434c-a1bf-cc1850d2a15a",
   "metadata": {},
   "source": [
    "The `Client` and associated `LocalCluster` object will be automatically closed when your Python session ends. When using Jupyter notebooks, we recommend to close it explicitely whenever you are done with your local Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11f2cf-7418-43de-9b45-847556474b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb032d4a-2c18-43f8-b6f6-0d0ba349f174",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1767a9-d5da-4e98-9c86-5048aec4d6b3",
   "metadata": {},
   "source": [
    "In this exercise, we created a Dask cluster on a single machine, which is equivalent to vertical scaling as described in **2.4 Formats and Performance**. \n",
    "\n",
    "However, the speedup is limited to the resources of your single machine. This is where horizontal scaling comes in. By adding resources from other machines, you can expand your computing power! In the case of dask, you can use the [Dask Gatway](https://gateway.dask.org/#dask-gateway).   \n",
    "The Pangeo community hosts tutorials using [Pangeo@EOSC]('https://pangeo-eosc.vm.fedcloud.eu/hub/'). With the Pangeo EOSC deployment, you can learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing us to run our data analysis in parallel, e.g. distribute tasks across multiple workers.\n",
    "If you are a user of HPC center, you can also consider using [Dask-jobque](https://docs.dask.org/en/stable/deploying-hpc.html#high-performance-computers) or Dask HPC for this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac0518-5e8d-479c-b2b5-f8e438238527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubes-and-clouds-pangeo",
   "language": "python",
   "name": "conda-env-cubes-and-clouds-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
