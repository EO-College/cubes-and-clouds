{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42620ff9-45f3-4e4c-aa76-f0036ddab315",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/EO-College/cubes-and-clouds/main/icons/cnc_3icons_process_circle.svg\"\n",
    "     alt=\"Cubes & Clouds logo\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a0cab-e26e-4942-96ee-78de70890ad9",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/pangeo-data/pangeo.io/refs/heads/main/public/Pangeo-assets/pangeo_logo.png\"\n",
    "     alt=\"Pangeo logo\"\n",
    "     style=\"float: center; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbae7a-12f1-4787-a520-c3de7529168d",
   "metadata": {},
   "source": [
    "# 2.4 Data chunking with Pangeo\n",
    "\n",
    "## Data chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245decb-8706-4b55-aead-79dd7a621bdd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Overview</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is chunking and why does it matter?</li>\n",
    "        <li>How can we utilize chunking to make our processing more efficient?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Explore chunking of data</li>\n",
    "        <li>Learn about the Zarr file format</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c3e5a-1ddf-4178-a05e-2ce711ab1b8b",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "As explained in Section 2.4 - Formats and Performance, when working with large data files or collections, it is often impractical to load the entire dataset into the memory of a single computer at once. This is where the Pangeo ecosystem is particularly useful. In Section 2.3 - Data Access, we discussed the concept of lazy loading. Xarray enables lazy processing of data in __chunks__, meaning the dataset is divided into manageable pieces. By reading and processing the data in these chunks, we can efficiently handle large datasets on a single computer or scale the processing to a distributed computing cluster using Dask (e.g., on the cloud or high-performance computing environments).\n",
    "\n",
    "How we process these chunks in a parallel environment to scale our computation vertically is discussed in [2.4 dask](./dask.ipynb). In this notebook, you will explore the concept of chunks through various exercises.\n",
    "\n",
    "Processing data piece by piece is more efficient when both our input and output data are also stored in chunks. As introduced in Section 2.4 - Formats and Performance, [Zarr](https://zarr.readthedocs.io/en/stable/) is a cloud-native data format and serves as the reference library in the Pangeo ecosystem for storing Xarray multi-dimensional datasets in __chunks__.\n",
    "\n",
    "## Data\n",
    "We'll begin with the same sample data retrieval method from the Sentinel-2 STAC collection, as described in Exercise 2.3 - Data Access: Lazy Loading with Pangeo.\n",
    "\n",
    "The analysis will be similar to our previous exercises, but this time, we’ll use a larger spatial extent to demonstrate the scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df41af-2c58-4665-9260-5e92fb7f8265",
   "metadata": {},
   "source": [
    "We start by copying the data files needed to complete the exercise using the following shell commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24273c3-6b3d-4af1-b2c3-5b7207d9b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r ${DATA_PATH%/*/*}/notebooks/cubes-and-clouds/lectures/2.4_formats_and_performance/exercises/assets $HOME/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb32e7-cb5f-47cd-be1b-ec668266bec5",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab40279-d857-437d-9405-3a0dfc9aa5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import stackstac\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import rioxarray as rio\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c445c2-f44c-4705-aeec-e29ba845ee66",
   "metadata": {},
   "source": [
    "## Load Sentinel-2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85914868-6215-43ae-88d2-fbec027f9e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi = gpd.read_file('./assets/catchment_outline.geojson', crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(aoi.iloc[0].geometry)\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    intersects=aoi_geojson,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2019-02-01/2019-04-28\",\n",
    "    query= {\"proj:epsg\": dict(eq=32632)}\n",
    ").item_collection()\n",
    "sentinel2_l2a = stackstac.stack(items)\n",
    "sentinel2_l2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d2938-d0ea-41fe-b32c-af81a8055416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is a __Chunk__?\n",
    "\n",
    "If you closely examine the `sentinel2_l2a` dataset, you'll notice that the `xarray.DataArray` is backed by a `dask.array` with a chunk size of `(1, 1, 1024, 1024)`. The full dataset consists of arrays with dimensions `(101, 32, 20982, 10980)`, totaling 746,592 chunks, which amounts to 5.42 TiB of data loaded into the computer's RAM.\n",
    "\n",
    "You can view the `dask.array` information by clicking the blue-circled icon in the image below.\n",
    "\n",
    "![Dask.array](./assets/datasize.png)\n",
    "\n",
    "By clicking the red-circled triangle icon, you'll see detailed information about the `xarray.DataArray`, including its Coordinates, Indexes, and Attributes.\n",
    "\n",
    "When creating an `Xarray` object using `stackstac`, we can easily convert a STAC collection into a lazily-loaded, chunked `xarray.DataArray` backed by Dask.\n",
    "\n",
    "The size and shape of the chunks determine the level of parallelization performed by Dask. Therefore, selecting an appropriate chunk size can significantly impact the performance of your computation.\n",
    "\n",
    "This is where understanding and effectively using chunking becomes crucial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821343b6-0b89-4bae-afe4-07892e8943d2",
   "metadata": {},
   "source": [
    "In our case, for the moment, we used `stackstac` without specifying the 'chunk' explicitly. The dataset is composed of 8 MiB chunks, each containing 1 time step, 1 band, and a resolution of 1024 x 1024 in the x and y directions.\n",
    "\n",
    "![chunk_original](./assets/chunk_original.png)\n",
    "\n",
    "If the chunk size is too small, our workflow will be divided into too many tiny pieces, which can lead to excessive communication and increased distribution overhead.\n",
    "\n",
    "On the other hand, if the chunk size is too large, there may not be enough memory available to handle the workload, causing the workflow to fail.\n",
    "\n",
    "The optimal chunk size depends on both your computation and the machine you're using.\n",
    "\n",
    "For example, 8 MiB is relatively small compared to the typical RAM size available. Dask’s default array chunk size, for instance, is 128 MiB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3cf9c-583b-46a8-b6eb-38083af17bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.get('array.chunk-size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a95b34-8ecb-4dca-8c4a-861d9b078012",
   "metadata": {},
   "source": [
    "## Modifying chunks\n",
    "\n",
    "Let's try to modify our chunk size.\n",
    "\n",
    "To modify chunks on your existing `xarray.DataArray` we can use the `chunk` method.\n",
    "We know that we only need 2 bands to compute the Normalized Difference Vegetation Index (NDVI) example, so we select only `red` and `nir` to simplify our example.\n",
    "\n",
    "We would like to have each time series separated in each chunk, then keep all band information on one chunk, and let dask to compute x and y coordinate's chunk size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b458c55c-d93e-4f5c-a7ed-e4ff258cbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDVI(data):\n",
    "    red = data.sel(band=\"red\")\n",
    "    nir = data.sel(band=\"nir\")\n",
    "    ndvi = (nir - red)/(nir + red)\n",
    "    return ndvi\n",
    "\n",
    "ndvi_xr = NDVI(sentinel2_l2a)\n",
    "ndvi_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35188d-fe2d-4977-aa3d-7a0c49a2d75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentinel2_l2a = sentinel2_l2a.sel(\n",
    "    band=['red','nir']).chunk(\n",
    "    chunks={'time': 1, 'band':2, 'x':'auto','y':'auto'})\n",
    "sentinel2_l2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f05ca7-1439-4a44-aad5-0dda313eeb5f",
   "metadata": {},
   "source": [
    "If you look into the details of any variable in the representation above, you'll see that each x and y coordinate's chunk is bigger, and we have less chunks than the example before.\n",
    "\n",
    "Note here from the chunk size, the auto option computed the optimal chunk size for y and x if we want to keep the chunk size of time and band as 1 and 2 respectively.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac448da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Go Further</b>\n",
    "    <br>\n",
    "    <br>\n",
    "    You can try to apply different ways for specifying chunk.\n",
    "    <ul>\n",
    "        <li> chunks = -1 -> the entire array will be used as a single chunk\n",
    "        <li> chunks = {'x':-1, 'y': 1000} -> chunks of entire _x_ dimension, but splitted every 1000 values on _y_ dimension</li>\n",
    "        <li> chunks = {'x':-1, 'y': 'auto'} -> Xarray relies on Dask to use an ideal size according to the preferred chunk sizes for _y_ dimension</li>\n",
    "        <li> chunks = { 'x':-1 ,'y':\"500MiB\" } -> Xarray seeks the size according to a specific memory target expressed in MiB</li>\n",
    "        <li> chunks = ( 1, 3, 12048,2048) -> Specifying chunk size in the order of dimension. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f923b-6e45-4494-af6c-c537e1791f99",
   "metadata": {},
   "source": [
    "## Defining the chunk size at the creation with Xarray\n",
    "\n",
    "We can define the chunk size when we create the object.  \n",
    "This is usually done with Xarray using the `chunks` kwarg when opening a file with `xr.open_dataset` or with `xr.open_mfdataset`, if you create Xarray from your local file.  \n",
    "\n",
    "In our NDVI example, we create Xarray from `stackstac`. As `stackstac`'s default 'chunksize' definition is 1024 for the x and y dimensions, we had that chunk size. We can pass the `chunksize` option to `stackstac` and make it larger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fd516-7366-4b22-9a6a-87ad615da69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sentinel2_l2a = stackstac.stack(items,\n",
    "                                assets=['red','nir'],\n",
    "                                chunksize=( 1, 2, 2048,2048)\n",
    ")\n",
    "sentinel2_l2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df00ba1-46ea-46a6-92e6-433187e8f433",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## So, why chunks?\n",
    "\n",
    "As explained in **Section 2.4 - Formats and Performance**, chunks are mandatory for accessing files or datasets that are larger than a single computer's memory. If all the data has to be accessed, it can be done sequentially (i.e., chunks are processed one after the other).\n",
    "\n",
    "Moreover, chunks allow for distributed processing and increased speed for your data analysis, as seen in the next section.\n",
    "\n",
    "### Chunks and files\n",
    "\n",
    "Xarray chunking capabilities also depend on the underlying input or output file format used. Most modern file formats allow datasets or single files to be stored using chunks. For example, **NetCDF4** uses chunks when storing a file on the disk via HDF5. Any read of data from a NetCDF4 file will load at least one chunk of that file. So when reading one of its chunks as defined in the `open_dataset` call, Xarray will take advantage of native file chunking and won't need to read the entire file.\n",
    "\n",
    "However, it is important to note that **Xarray chunks and file chunks are not necessarily the same**. It is a good practice to configure Xarray chunks so that they align well with the input file format chunks (ideally, Xarray chunks should contain one or several input file chunks).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a0ddb-3b89-4392-9822-dfaa80111c52",
   "metadata": {},
   "source": [
    "## Zarr storage format\n",
    "\n",
    "This brings us to our next subject: [Zarr](https://zarr.readthedocs.io/en/stable/).\n",
    "\n",
    "If we can have our original dataset already 'chunked' and accessed in an optimized way according to its actual byte storage on disk, we won't need to load the entire dataset every time. This can greatly optimize our data analysis, even when working with the entire dataset.\n",
    "\n",
    "Let's convert our intermediate data into Zarr format so that we can learn what it is. We can keep the data as a `DataArray` or convert it into a `Dataset` before storing it.\n",
    "\n",
    "We start again by loading data using `stackstac`, but this time, we proceed to the next step: clipping the data and computing the NDVI. Then, let's try to save those intermediate results in a Zarr file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c711573-b93b-4979-a28c-5e8ce3fb6f84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "Load data using stackstac (with specific chunk size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833d382-e110-4ba8-a911-8c9ce7cb18d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi = gpd.read_file(\"./assets/catchment_outline.geojson\", crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(aoi.iloc[0].geometry)\n",
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    intersects=aoi_geojson,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    datetime=\"2019-02-01/2019-04-28\",\n",
    "    query= {\"proj:epsg\": dict(eq=32632)}\n",
    ").item_collection()\n",
    "ds = stackstac.stack(items,\n",
    "                     assets=['red','nir'],\n",
    "                     chunksize=( 1, 2, 1024,1024)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d282f0-c4f4-493d-9144-292885edb720",
   "metadata": {},
   "source": [
    "## NDVI computation\n",
    "\n",
    "Compute the NDVI as in 2.3 Data Access - Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22cf6a-282d-463a-82a1-723bd2490e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NDVI(data):\n",
    "    red = data.sel(band=\"red\")\n",
    "    nir = data.sel(band=\"nir\")\n",
    "    ndvi = (nir - red)/(nir + red)\n",
    "    return ndvi\n",
    "\n",
    "ndvi_xr = NDVI(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3ce48-30dd-4fc8-8491-1c7124016063",
   "metadata": {},
   "source": [
    "## Spatial clipping\n",
    "\n",
    "Restrict the data to the area of interest from the loaded polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939b76f-32f3-48da-bce9-4a082fd0f473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi_utm32 = aoi.to_crs(epsg=32632)\n",
    "geom_utm32 = aoi_utm32.iloc[0]['geometry']\n",
    "ndvi_xr.rio.write_crs(\"EPSG:32632\", inplace=True)\n",
    "ndvi_xr.rio.set_nodata(np.nan, inplace=True)\n",
    "ndvi_xr = ndvi_xr.rio.clip([geom_utm32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740cd1a-dda2-407b-b58d-e626aa11fb64",
   "metadata": {},
   "source": [
    "## Save to Zarr\n",
    "\n",
    "Select just a few days, to reduce the amount of data for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86826bf9-baae-45a3-9caf-a0ac784e77bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_small = ndvi_xr.isel(time=slice(0,3))\n",
    "ndvi_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c06cc-64f1-4a19-9048-d494eaa47a82",
   "metadata": {},
   "source": [
    "Before saving, we can modify the chunk shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445791b6-3676-4237-8098-3ff65713c441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_small = ndvi_small.chunk(chunks = {'x':'auto', 'y': 'auto'}).to_dataset(name='data')\n",
    "ndvi_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc47e6b-18fc-49fc-b0fb-395c9964af6f",
   "metadata": {},
   "source": [
    "Then clean attributes that might create issues while writing and save to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987063c7-156a-4d0f-9243-adf6e59a48f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def remove_attrs(obj, to_remove):\n",
    "    new = obj.copy()\n",
    "    new.attrs = {k: v for k, v in obj.attrs.items() if k not in to_remove}\n",
    "    return new\n",
    "\n",
    "def encode(obj):\n",
    "    object_coords = [name for name, coord in obj.coords.items() if coord.dtype.kind == \"O\"]\n",
    "    return obj.drop_vars(object_coords).pipe(remove_attrs, [\"spec\", \"transform\"])\n",
    "\n",
    "ndvi_small.pipe(encode).to_zarr('test.zarr',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fa945",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Exercise</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>What about saving the data in Netcdf format? `ls -la test.zarr` and  `ls -la test.zarr/nobs `</li>\n",
    "        <li>You can try to explore the zarr file you just created using `ls -la test.zarr` and  `ls -la test.zarr/nobs `</li>\n",
    "        <li>You can explore zarr metadata file by `cat test.zarr/.zmetadata` </li>\n",
    "        <li>Did you find the __chunks__ we defined previously in your zarr file? </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558b8ac-0885-4369-acfa-de23c84b4dae",
   "metadata": {},
   "source": [
    "## Compare Zarr and netCDF\n",
    "Lets compare how the zarr and NetCDF files are stored.  \n",
    "We read our sample dataset from Zarr and store it as netCDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0670d-11cf-4521-abd4-515cb2dd51fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xr.open_zarr('test.zarr').to_netcdf('test.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b92aa7-c87b-4978-aa33-5fed788318b0",
   "metadata": {},
   "source": [
    "Compare the disk space used by the two formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446367c-4d32-4d0d-8d60-72c3c9d87818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!du -sh test.zarr/ test.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac40262-86e8-4703-ac61-5266e807ccae",
   "metadata": {},
   "source": [
    "List the content of the Zarr directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613a930-bc5d-48a3-93a3-c324ac104223",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls  -la test.zarr/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf601269-7ab1-4fac-9f0e-3f9e7e85d5e1",
   "metadata": {},
   "source": [
    "List the content of the Zarr data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b75a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls  -la test.zarr/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1dcee-aeea-456e-b94b-d3e43a972e04",
   "metadata": {},
   "source": [
    "Print the content of the Zarr metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0f70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat test.zarr/.zmetadata | head -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bed29-4e53-4957-8b37-7e97b110dedc",
   "metadata": {},
   "source": [
    "### Zarr format main characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d49493-d272-4034-ac6d-7c03506e4a02",
   "metadata": {},
   "source": [
    "- Every chunk of a Zarr dataset is stored as a single file (see x.y files in `ls -al test.zarr/data`)\n",
    "- Each Data array in a Zarr dataset has a two unique files containing metadata:\n",
    "  - .zattrs for dataset or dataarray general metadatas\n",
    "  - .zarray indicating how the dataarray is chunked, and where to find them on disk or other storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665722c-4c65-4334-987e-1f5bc4841036",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Understanding chunking is key to optimize your data analysis when dealing with large datasets. In this exercise, we learned how to optimize data access time and memory resources by using native file chunks loaded by stackstac and instructing Xarray to modify the chunk. Computing on large datasets can be very slow on a single machine, and to optimize your time we may need to parallelize your computations. This is what you will learn in the next exercise with Dask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubes-and-clouds-pangeo",
   "language": "python",
   "name": "conda-env-cubes-and-clouds-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
