# 3.2 Scaling

## Learning Objectives

- Learn why scaling design is important
- Scaling options - speed/data size, horizontally/vertically
- How to estimate the costs of scaling

## Introduction

This lesson is focusing on applying knowledge obtained in the previous lesson to a larger scale. First, we will look into different scaling options and terminology. After that, we will talk about code optimization and the possibilities of scaling on platforms. Lastly, we will look at cost estimates. After going through the theory, we will apply newly gained knowledge to practice - directly on our EO workflow.

**VIDEO: Scaling importance and challenges**

- a reminder why not to run locally
- what are the benefits of easy scaling
- challenges

### Scaling

Scaling refers to the process of increasing or decreasing the capacity or size of a system to handle a larger or smaller workload or data volume. Scaling does not necessarily means only in the direction of larger and bigger but also saving unnecessary costs in times when there is no traffic. In our context, scaling involves managing the growth of data, traffic, or processing requirements to ensure optimal performance and availability.

## Horizontal vs vertical scaling

Two classical approaches to scaling are horizontal and vertical:

- **Horizontal scaling:** Also known as scaling out, horizontal scaling involves adding more machines or nodes to distribute the workload across a larger number of resources. This could mean adding more servers or instances to handle increased traffic or data processing demands. Horizontal scaling offers the advantage of improved fault tolerance and increased capacity, as the workload is spread across multiple resources.
- **Vertical scaling:** Also known as scaling up, vertical scaling involves increasing the capacity of an individual machine or resource. This can be achieved by upgrading the hardware, such as adding more powerful processors, memory, or storage, to handle the growing demands of the geospatial application. Vertical scaling is often suitable for applications with single-node architectures or when the workload cannot be easily distributed across multiple machines.

Both horizontal and vertical scaling have their advantages and considerations. Horizontal scaling provides better scalability and fault tolerance, as it can handle increased traffic and processing by adding more resources. However, it may require additional effort to distribute and synchronize data across multiple nodes. Vertical scaling, on the other hand, simplifies data management as all resources are contained within a single node, but it may have limitations in terms of the maximum capacity a single machine can handle.

In common workflows, a combination of both approaches is used to ensure optimal speed and resource utilization while being able to keep the simplicity of a workflow.

### Cloud native scaling

TODO: basics covered in the platform lesson?
This term refers to the ability of a platform, program, or application to dynamically adjust in the cloud. It is using same principles of horizontal and vertical scaling, but instead of the need of your machines, it means typically adding more nodes for processing.

Common aspects of cloud-native scaling are:

**Elasticity:** Cloud platforms offer the capability to scale resources up or down based on demand. Elastic scaling ensures that the application can automatically adjust its capacity to handle fluctuations in workload. This means that additional resources can be provisioned during peak periods and scaled-down during periods of low demand. It allows for efficient resource utilization and cost optimization.

**Auto Scaling or Flexibility:** Cloud platforms often provide built-in auto-scaling mechanisms that automatically adjust the number of instances or containers based on predefined rules or metrics. These rules can be defined based on factors like CPU utilization, network traffic, or response time. Auto-scaling ensures that the system can dynamically adapt to changing load conditions without manual intervention.

**Containerization and Orchestration:** Cloud native scaling heavily relies on containerization technologies, such as Docker, and container orchestration platforms like Kubernetes. Containers provide lightweight and portable units of deployment that can be easily scaled up or down. Container orchestration tools allow for efficient management of containerized applications, including automatic scaling of container instances based on defined policies.

**Monitoring and Metrics:** Cloud native scaling requires effective monitoring and metrics gathering. Cloud platforms typically provide monitoring services that track various metrics related to resource usage, performance, and application health. These metrics are used to drive the auto-scaling decisions and ensure the application can scale in response to changes in demand.

These features ensure that optimal resources are used.

### What to avoid and what are the limitations

While scaling is providing many options and is essential for achieving results on a larger scale, there are some limitations to keep in mind and activities to even avoid.

Costs: One of the main characteristics to consider are costs of computing. Scaling resources dynamically can lead to increased costs, especially if not properly managed. It is essential to monitor resource usage and set appropriate maximum scaling policies to ensure cost optimization. Failure to do so may result in unnecessary provisioning of resources, leading to higher expenses. The purchase of many computational resources can be easy, but very costly. Code optimization is important to ensure there are no memory leaks, unnecessary data storage, and other expensive operations.

Data Access: In geospatial cloud workflows, one of the big challenges lies in data access and optimal data storage. The easy trap is in loading large portions of unnecessary data without applying correct filters ahead. Such data volumes can lead to more requirements on RAM or disk space resulting in higher costs of processing or longer times (and more computational time) just to load the data.

Accessing data as files: Geospatial data are stored in many formats as discussed in [lesson 2.2](../2.2_data_properties/2.2_data_properties.md) of some are more appropriate to access in the cloud. The opportunity of first evaluating metadata before loading the whole dataset is great for saving time and money.

Latency and Data Transfer: In distributed and scaled-out architectures, managing data transfer and minimizing latency can be crucial. Moving data between services or instances across different locations can introduce network overhead and impact application performance. Efficient data caching, or data partitioning strategies can help mitigate these challenges.

Scaling Limits in the platform: While cloud platforms offer high scalability, there are still practical limits to consider. Every service or resource has its scalability limits, such as maximum instance count, storage capacity, or network throughput. It is important to understand these limitations and design your programs and applications accordingly.

To mitigate these challenges and limitations, it's advisable to thoroughly plan and architect your application for scalability, leverage cloud-native tools and services, monitor resource usage and costs, and regularly test and optimize your scaling strategies. Additionally, staying updated with the latest advancements in cloud technologies and best practices will help you navigate the complexities of cloud-native scaling more effectively.

## How to scaling

TODO: how much technology do we want to go? The target audience is mainly focusing on "platform usage"

There are many approaches how to handle scaling properly.

todo: parallel computing section

### Subscription vs. On-Demand usage

**Subscription:** A subscription model involves a fixed, recurring payment made by the user to access and utilize the cloud platform's services over a specific period, typically monthly or annually. Under a subscription model, users typically commit to a predetermined level of resources and pay a regular fee regardless of the actual usage during that period. This model often provides cost predictability and may offer discounts or benefits for long-term commitments. Users can usually choose from various options and combinations of resources (eg. GPU, CPU, disk storage combinations).

Advantages of the Subscription Model:

- Cost Predictability: Users have a clear understanding of the ongoing costs as they pay a fixed fee.
- Potential Cost Savings: Subscriptions may offer discounts or cost benefits for longer-term commitments.
- Continuous Service Access: Users have continuous access to the subscribed services without the need for frequent renewal or payment management.

**On-Demand Usage:** In an on-demand model, users pay for the cloud platform's services based on actual usage and consumption. Users are charged on a pay-as-you-go basis, where they pay for the resources or services they utilize in a given period. There are no long-term commitments or fixed fees. This model offers flexibility and scalability, allowing users to scale resources up or down as per their needs.

Advantages of On-Demand Usage:

- Flexibility: Users have the flexibility to adjust resources based on their varying demands, scaling up or down as required.
- Cost Efficiency: Users only pay for what they use, making it suitable for workloads with unpredictable or fluctuating resource needs.
- No Long-Term Commitments: On-demand models do not require users to commit to a specific period or predefined resource levels, allowing for agility and quick adjustments.

Choosing between subscription and on-demand models depends on various factors, including the nature of your workloads, budget considerations, and usage patterns. Based on this (and data availability) users can choose a platform that suits their needs best. Reviewing the pricing details is an important part before selecting a working environment.

### Cost of scalability

Direct examples of computing on a workflow
(todo: based on actual workflow)

### Memory consumption

limitations and

### Difference between platform usage and cloud directly

TODO: Is this covered already in the platform lesson?
Using platforms removes complexity and adds abstraction layers

## Exam

- Based on the previous exercises, calculate how much resources it would take to produce the snow result for:
  - Europe,
  - the whole world,
  - one-time step,
  - one year,
  - multi-year
- Things to avoid:
  - loading the whole dataset
  - listing big directories in jupyterLab
  - letting your code run unattended
- Datacube chunking - when to chunk by time and when to chunk by area
- Concrete examples of costs of scaling for previously estimated resource requests
- What are the limitations of scaling
- What do not affect speed of processing?
- You have X processors with XXX amount of RAM  - what can you do to improve it?

## Further Readings

[eduWRENCH](https://eduwrench.ics.hawaii.edu/)

## References
