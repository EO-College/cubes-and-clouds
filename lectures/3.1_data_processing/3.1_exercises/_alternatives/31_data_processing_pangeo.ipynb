{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33781d3b-2a87-4b9e-99ba-4a4fc1937844",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/EO-College/cubes-and-clouds/main/icons/cnc_3icons_process_circle.svg\"\n",
    "     alt=\"Cubes & Clouds logo\"\n",
    "     style=\"float: center; margin-right: 10px; margin-left: 10px; max-height: 250px;\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861a363-a851-4e76-9692-251d567e6128",
   "metadata": {},
   "source": [
    "# 3.1 Data Processing with Pangeo\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pangeo-data/pangeo.io/refs/heads/main/public/Pangeo-assets/pangeo_logo.png\"\n",
    "     alt=\"Pangeo logo\"\n",
    "     style=\"float: center; margin-right: 10px; max-height: 80px;\"/>\n",
    "     \n",
    "In this exercise we will build a complete EO workflow using the Pangeo ecosystem on a cloud platform; from data access to obtaining the result. In this example we will analyse snow cover in the Alps.\n",
    "\n",
    "We are going to follow these steps in our analysis:\n",
    "\n",
    "-   Load satellite collections\n",
    "-   Specify the spatial, temporal extents and the features we are interested in\n",
    "-   Process the satellite data to retrieve snow cover information\n",
    "-   aggregate information to get catchment statistics over time\n",
    "-   Visualize and analyse the results\n",
    "\n",
    "More information on the Pangeo ecosystem: https://pangeo.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f702f-eeea-48f2-8056-540fe8f089d7",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb085d-37ba-4e83-98fe-ac6b480f0a8d",
   "metadata": {},
   "source": [
    "We start by creating the shared folders and data files needed to complete the exercise using the following shell commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90f785-a3bd-4704-8e72-234decc1e389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r $DATA_PATH/31_results/ .\n",
    "!cp -r $DATA_PATH/31_data/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd904a69-d874-49f2-8cb3-9e6d6a6a2a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# platform libraries\n",
    "# utility libraries\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "import pyproj\n",
    "\n",
    "# STAC Catalogue Libraries\n",
    "import pystac_client\n",
    "import stackstac\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import folium\n",
    "\n",
    "# Dask library\n",
    "from dask.distributed import Client, progress, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7fe490-aecc-40b8-8634-591c9838c038",
   "metadata": {},
   "source": [
    "### Get a client from the Dask  Cluster\n",
    "\n",
    "Creating a Dask `Client` is mandatory in order to perform following Dask computations on your local Dask Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f5a4e-a6bd-4ae8-9336-41d4159919e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=2)\n",
    "client = Client(cluster)  # create a local dask cluster on the machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29ffe6-0102-47aa-9c17-1a4a43f16032",
   "metadata": {},
   "source": [
    "## Region of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8362b-d6a3-43f1-8e5e-d3e516a7f3dd",
   "metadata": {},
   "source": [
    "We will use the catchment as our area of interest (AOI) for the analysis. Our region of interest is the Val Passiria Catchment in the South Tyrolian Alps (Italy). Let's load the catchment area. \n",
    "The catchment is defined by a polygon, which we will load from a GeoJSON file. \n",
    "The GeoJSON file contains the geometry of the catchment in the WGS84 coordinate reference system (EPSG:4326) and that has to be defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bd2a8-92aa-4c8e-aea2-b14cc4e08254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "catchment_outline = gpd.read_file('../31_data/catchment_outline.geojson', crs=\"EPGS:4326\")\n",
    "aoi_geojson = mapping(catchment_outline.iloc[0].geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1df440-b1c1-4df7-bd2f-4b3fc7d782d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "center_loc = catchment_outline.to_crs('+proj=cea').centroid.to_crs(epsg=\"4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764a043-c443-4f5c-b6d7-e172a56213d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenStreetMap\n",
    "map = folium.Map(location=[float(center_loc.y.iloc[0]), float(center_loc.x.iloc[0])], tiles=\"OpenStreetMap\", zoom_start=9)\n",
    "for _, r in catchment_outline.iterrows():\n",
    "    sim_geo = gpd.GeoSeries(r[\"geometry\"]).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"fillColor\": \"orange\"})\n",
    "    folium.Popup(r[\"HYBAS_ID\"]).add_to(geo_j)\n",
    "    geo_j.add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74885a74-eee6-445e-80ca-0ab460c4a7f4",
   "metadata": {},
   "source": [
    "**Quiz hint: Look closely at the end of the displayed catchment area to identify the outlet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c8f6e-bb4f-4135-8b36-74abff72225e",
   "metadata": {},
   "source": [
    "### Satellite collections\n",
    "\n",
    "#### Search for satellite data using STAC\n",
    "\n",
    "We will utilize the `pystac_client` to search for satellite data in this exercise, specifically leveraging data provided by AWS/Element84. When querying the satellite data we can add various filters such as spatial range, time period, and other specific metadata. This API is constructed based on the STAC specification, a collaborative, community-driven standard aimed at enhancing the discoverability and usability of satellite data. Numerous data providers, including AWS, Google Earth Engine, and Planet (Copernicus Data Space Ecosystem (CDSE) is coming soon**), among others, have implemented the STAC API, exemplifying its widespread adoption and utility in accessing diverse satellite datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750bb56-b134-4211-b15f-e8361707aa94",
   "metadata": {},
   "source": [
    "##### Set query filters \n",
    "We define all extents before querying satellite data. For the purposes of this exercise, we will limit the search to the Sentinel 2 L2A collection, which is a collection of Sentinel 2 data that has been processed to surface reflectance (Top Of Canopy). \n",
    "\n",
    "We are only interested in the green and short wave infrared band, band 3 and 11. And we directly remove time slices with a cloud cover >= 90 %. We will also limit the search to the time period between 1st February 2019 and 10th June 2019 and to the extent of the catchment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0ed60-ad5d-40aa-a8eb-4b6118a95327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox = catchment_outline.bounds.iloc[0]\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294cc24a-e9ca-47eb-9cfb-454ff69e3ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#                  West,     South,     East,      North\n",
    "spatial_extent = [bbox[\"minx\"], bbox[\"miny\"], bbox[\"maxx\"], bbox[\"maxy\"]]\n",
    "temporal_extent = ['2018-02-01T00:00:00Z','2018-06-30T00:00:00Z']\n",
    "\n",
    "bands = ['green', 'swir16', 'scl']\n",
    "cloud_coverage = [\"eo:cloud_cover<=90\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b951e-ad59-42ed-9071-335ae66a9bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"https://earth-search.aws.element84.com/v1\"\n",
    "catalog = pystac_client.Client.open(URL)\n",
    "items = catalog.search(\n",
    "    bbox=spatial_extent,\n",
    "    datetime=temporal_extent,\n",
    "    query=cloud_coverage,\n",
    "    collections=[\"sentinel-2-l2a\"]\n",
    ").item_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e547e51-f3ef-443a-a739-b0ac6c55555e",
   "metadata": {},
   "source": [
    "## Inspect Metadata\n",
    "We need to set the following configurations to define the content of the data cube we want to access:\n",
    "- dataset name\n",
    "- band names\n",
    "- time range\n",
    "- the area of interest specifed via bounding box coordinates\n",
    "- spatial resolution\n",
    "\n",
    "To select the correct dataset we can first list all the available datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55c204-13a5-4f74-9643-a6ddd21f37ad",
   "metadata": {},
   "source": [
    "#### Get bands information\n",
    "As the original data provides bands with different names than the original Sentinel 2 bands, we need to get the information about the bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf62d7-32fa-4868-ba73-f48cb8505036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get bands information\n",
    "selected_item = items[1]\n",
    "for key, asset in selected_item.assets.items():\n",
    "    print(f\"{key}: {asset.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27e183-7fe1-4078-9204-fba3aea854d5",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "We will use the stackstac library to load the data. The stackstac library is a library that allows loading data from a STAC API into an xarray dataset.\n",
    "Here we will load the green and swir16 bands (on the original dataset named B03 and B11), which are the bands we will use to calculate the snow cover. We will also load the scl band, which is the scene classification layer, which we will use to mask out clouds.\n",
    "Spatial resolution of 20m is selected for the analysis. The data is loaded in chunks of 2048x2048 pixels.\n",
    "\n",
    "[Stackstac](https://stackstac.readthedocs.io/en/latest/) is not the only way to create a xarray dataset from a STAC API. Other libraries can be used, such as [xpystac](https://github.com/stac-utils/xpystac) or [odc.stac](https://github.com/opendatacube/odc-stac). The choice of the library depends on the use case and specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca84a715-3b53-498a-b2e5-971fbfafbc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s2_cube = stackstac.stack(items,\n",
    "                     bounds_latlon=spatial_extent,\n",
    "                     assets=bands\n",
    ")\n",
    "s2_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3550205-a1c3-4d88-81d6-96cf706e02bf",
   "metadata": {},
   "source": [
    "## Calculate snow cover\n",
    "\n",
    "We will calculate the Normalized Difference Snow Index (NDSI) to calculate the snow cover. The NDSI is calculated as the difference between the green and the swir16 bands divided by the sum of the green and the swir16 bands:\n",
    "\n",
    "$$ NDSI = \\frac {GREEN - SWIR} {GREEN +SWIR} $$\n",
    "\n",
    "\n",
    "For a matter of clarity we will define the green and the swir16 bands as variables. Other approaches can be used to manage the data, but this is the one we will use in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7e423-28d2-49e4-8879-584a1a59b3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "green = s2_cube.sel(band='green')\n",
    "swir = s2_cube.sel(band='swir16')\n",
    "scl = s2_cube.sel(band='scl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c68ec-7652-4780-a0fe-d9e1123cc8ae",
   "metadata": {},
   "source": [
    "Let's compute the NDSI and mask out the clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b8946-0977-40f4-b01d-5447358889ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndsi = (green - swir) / (green + swir).where((green + swir) != 0) \n",
    "ndsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80424d82-2308-4229-8623-25c0258d0360",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Dask Method Differences: `.compute()` vs `.persist()`\n",
    "\n",
    "Dask provides two primary methods for executing computations: `.compute()` and `.persist()`. Below is an overview of each method and their typical use cases.\n",
    "\n",
    "#### `.compute()`\n",
    "- **Functionality**: Executes the Dask computation and blocks until the result is available. It then collects and returns the final result to the local process.\n",
    "- **Use Case**: Invoke `.compute()` when you need to bring the computed result into your local memory. It is typically used as the final step in a Dask workflow after all transformations and computations have been defined.\n",
    "- **Evaluation**: Eager - runs immediately and provides results.\n",
    "\n",
    "#### `.persist()`\n",
    "- **Functionality**: Begins computing the result in the background while immediately returning a new Dask object that represents the ongoing computation.\n",
    "- **Use Case**: Utilize `.persist()` in a distributed environment when working with large datasets or complex computations that have expensive intermediate steps. This will keep the intermediate results in the cluster’s distributed memory, improving performance for subsequent computations.\n",
    "- **Evaluation**: Lazy - computations are started but the method returns a reference to the future result without waiting for the completion.\n",
    "\n",
    "Each method plays a crucial role in optimizing and managing the execution of large-scale computations using Dask, particularly when balancing memory usage and computational efficiency in a distributed setting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f644aa5a-3a49-4012-b96f-2c7f967b963b",
   "metadata": {
    "tags": []
   },
   "source": [
    "ndsi = ndsi.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45637b1-8f61-41c2-88fe-9847a2fbedaa",
   "metadata": {},
   "source": [
    "### Creating the Snow Map\n",
    "So far we have a time series map of NDSI values. We are intereseted in the presence of snow though. Ideally in a binary classification: snow and no snow.\n",
    "To achieve this we are setting a threshold of 0.4 on the NDSI. This gives us a binary snow map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc60ed-bb64-410e-87c1-8a6d661f8614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow = xr.where((ndsi > 0.42) & ~np.isnan(ndsi), 1, ndsi)\n",
    "snowmap = xr.where((snow <= 0.42) & ~np.isnan(snow), 0, snow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408e63f-e4a3-4ce3-b892-553ebd329992",
   "metadata": {},
   "source": [
    "### Creating a cloud mask\n",
    "We are going to use the Scene Classification of Sentinel-2, called the \"SCL\" band,  for creating a cloud mask and then applying it to the NDSI. We will mask out the clouds, which are identified by the values 8 (*cloud medium probability*), 9 (*cloud high probability*) and 3 (*cloud shadow*) in the scl layer.\n",
    "\n",
    "More detailed info can be found here: https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a1d49-e0c8-4497-bf4e-ff252026ecff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud_mask = np.logical_not(scl.isin([8, 9, 3])) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a68281-4240-41a7-983d-f414954299be",
   "metadata": {},
   "source": [
    "### Applying the cloud mask to the snowmap\n",
    "We will mask out all pixels that are covered by clouds. This will result in: 0 = no_snow, 1 = snow, 2 = cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159259e3-a9e8-44d1-bb88-3dac46b572e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snowmap_cloudfree = xr.where(cloud_mask, snowmap, 2)\n",
    "snowmap_cloudfree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd49342-d1c9-4200-bc59-c3a6d7023cbb",
   "metadata": {},
   "source": [
    "## Process snow cover data\n",
    "\n",
    "### Mask data\n",
    "\n",
    "As we are only interested to the snow cover in the catchment, we will mask out the data outside the catchment. To achieve it we need to convert the catchment geometry to the same coordinate reference system as the data. The data is in the UTM32N coordinate reference system (EPSG:32632)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c3a2c-3e8a-44b6-89b5-b2e087dccc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi_utm32 = catchment_outline.to_crs(epsg=32632)\n",
    "geom_utm32 = aoi_utm32.iloc[0]['geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c18b4-5577-4a6a-a695-1f72909a5321",
   "metadata": {},
   "source": [
    "As we are going to use the `rioXarray` library to mask out the data, we need to add some more information to the data. The rioXarray library is a library that allows to manipulate geospatial data in xarray datasets. Underneath it uses the rasterio library that is a library built on top of GDAL.\n",
    "\n",
    "We need first to specify the coordinate reference system and the nodata value. Both information can be found in the metadata of the data but we need to reinforce it so that `rioXarray` can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e171a-a9fc-4818-a7d7-1dbe6be4b691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snowmap_cloudfree.rio.write_crs(\"EPSG:32632\", inplace=True)\n",
    "snowmap_cloudfree.rio.set_nodata(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568da9d-68b3-40a5-8a0c-bb5355e55be7",
   "metadata": {},
   "source": [
    "Let's clip the snow_cloud object using the catchment geometry in the UTM32N coordinate reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f53037-c703-44e0-bfd1-26438a140cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snowmap_clipped = snowmap_cloudfree.rio.clip([geom_utm32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db20f4f-1eb7-4057-935e-a34e85de4289",
   "metadata": {},
   "source": [
    "It's time to persist the data in memory. We will use the persist method to load the data in memory and keep it there until the end of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d2bfe-d2cd-46ae-a9dc-4a659a017696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date = snowmap_clipped.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f788c18-86a2-41fe-b61c-3c6a8adae516",
   "metadata": {},
   "source": [
    "### Aggregate data\n",
    "\n",
    "Data aggregation is a very important step in the analysis. It allows to reduce the amount of data and to make the analysis more efficient. Moreover, as in this case, we are going to aggregate the date to daily values, this will allow use to compute statistic on the data at the basin scale later on.\n",
    "\n",
    "The `groupby` method allows to group the data by a specific dimension. We will group the data by the time dimension, aggregating to the date and removing the time information, once the group is obtained we will aggregate the data by taking the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6366e-5636-4a5a-a2a4-7896338b05de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92176d7f-e442-4977-baf8-963e0c30f615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date = snowmap_clipped.groupby(snowmap_clipped.time.dt.floor('D')).max(skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e997f7-56ec-4417-b6ea-ecc7d0c6085a",
   "metadata": {},
   "source": [
    "As the data has been aggregated to daily values, we need to rename the floor method to something more meaningful as date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29fc6f-d90f-449b-895d-74774aad5ce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date = clipped_date.rename({'floor': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3a6b2-608a-4b84-9c49-ce53f7634873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date = clipped_date.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c0b11-c625-4563-96dc-43cf479ddbb3",
   "metadata": {},
   "source": [
    "### Visualize data\n",
    "We will use the `hvplot` library to visualize the data. The library allows to visualize data in `xarray` datasets. It is based on the holoviews library, which is a library that allows to visualize multidimensional data.\n",
    "To visualize the data on a map, we need to specify the coordinate reference system of the data. The data is in the UTM32N coordinate reference system (EPSG:32632). This will allow the library to project the data on a map.\n",
    "More info on the hvplot library can be found here: https://hvplot.holoviz.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22534301-c625-4010-b33d-0d846edd72a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_date.hvplot.image(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    groupby='date',\n",
    "    crs=pyproj.CRS.from_epsg(32632),\n",
    "    cmap='Pastel2',\n",
    "    clim=(-1, 2),\n",
    "    frame_width=500,\n",
    "    frame_height=500,\n",
    "    title='Snowmap',\n",
    "    geo=True, tiles='OSM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5da410-fdbb-49a3-9bd0-7e2aab290374",
   "metadata": {},
   "source": [
    "### Calculate snow cover with apply_ufunc\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Calculate snow cover using Xarray's apply_ufunc </b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>The procedure for computing snow cover can also be summed up as following python function.  \n",
    "</li> \n",
    "        <li>We first verify that Green, swir16 and scl are in the order of 0,1,2 the variable in band variable. Then we simply copy and past all the python codes in a function.  </li>\n",
    "            </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21418351-fb96-4675-af96-d984854c2e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_ndsi_snow_cloud(data):\n",
    "    green = data[0]\n",
    "    swir = data[1]\n",
    "    scl = data[2]\n",
    "    ndsi = np.where((green + swir) == 0, np.nan, (green - swir) / (green + swir)) \n",
    "    ndsi_mask = ( ndsi > 0.4 )& ~np.isnan(ndsi)\n",
    "    snow = np.where(ndsi_mask, 1, ndsi)\n",
    "    snowmap = np.where((snow <= 0.42) & ~np.isnan(snow), 0, snow)\n",
    "    mask = ~( (scl == 8) | (scl == 9) | (scl == 3) )\n",
    "    snow_cloud = np.where(mask, snowmap, 2)\n",
    "    return snow_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4d53b-8754-4596-a12c-c9431d8cf924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "da = stackstac.stack(items,\n",
    "                    bounds_latlon=catchment_outline.iloc[0].geometry.bounds,\n",
    "                    resolution=20,\n",
    "                    chunksize=(1,-1,-1,-1),\n",
    "                    assets=bands)\n",
    "#Mask data\n",
    "geom_utm32 = catchment_outline.to_crs(epsg=32632).iloc[0]['geometry']\n",
    "da.rio.write_crs(\"EPSG:32632\", inplace=True)\n",
    "da.rio.set_nodata(np.nan, inplace=True)\n",
    "da = da.rio.clip([geom_utm32])\n",
    "\n",
    "snow_cloud_clipped=xr.apply_ufunc(\n",
    "    calculate_ndsi_snow_cloud\n",
    "    ,da\n",
    "    ,input_core_dims=[[\"band\",\"y\",\"x\"]]\n",
    "    ,output_core_dims=[[\"y\",\"x\"]]\n",
    "    ,exclude_dims=set([\"band\"])\n",
    "    ,vectorize=True\n",
    "    ,dask=\"parallelized\"\n",
    "    ,output_dtypes=[da.dtype]\n",
    "    ).assign_attrs({'long_name': 'snow_cloud'}).to_dataset(name='snow_cloud')\n",
    "\n",
    "snow_cloud_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23a7f5-ac7d-4b11-92ee-71e7af8e3bcf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <i class=\"fa-check-circle fa\" style=\"font-size: 22px;color:#666;\"></i> <b>Inspect the data dimentions! </b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>How did changed from input (da) to output(snow_cloud_clipped)?\n",
    "        </li>\n",
    "        <li>What is setted as input_core_dims?    \n",
    "</li> \n",
    "        <li>What is setted as output_core_dims?   \n",
    "</li> \n",
    "        <li>What is setted as exclude_dims? \n",
    "</li> \n",
    "        <li>Did you see 'time' dimension?\n",
    "</li> \n",
    "        <li>We will get back to apply_ufunc with next OpenEO example.  \n",
    "</li> \n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babae57f-3e53-4939-8d66-177f46b4029c",
   "metadata": {},
   "source": [
    "## Compute statistics\n",
    "\n",
    "Our objective is to monitor a specific area over a period of time, ensuring the data quality meets our standards. To achieve this, we determine the proportion of clouds in the watershed at each interval. This cloud coverage data serves to refine our timeline: we exclude any interval where cloud cover exceeds 25%.\n",
    "\n",
    "Our primary focus is on quantifying the Snow Covered Area (SCA) in the watershed. We tally the number of pixels depicting snow for each interval and calculate the SCA by multiplying the snowy pixels by the pixel's area. To determine the extent of snow coverage, we calculate the percentage of snow-covered area by comparing the number of snowy pixels to the total pixel count within the watershed. This percentage is a key metric in our analysis.\n",
    "\n",
    "We need to gather the total pixel counts for the entire watershed, as well as those specific to cloud and snow coverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c37fb-5cae-4ed9-8725-77bf8b69ec59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of cloud pixels\n",
    "cloud = xr.where(clipped_date == 2, 1, np.nan).count(dim=['x', 'y']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c6928-8102-4284-b40a-115fd6b6c6f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of all pixels per each single date\n",
    "aot_total = clipped_date.count(dim=['x', 'y']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4ccd7-b74c-4543-bfd9-595d1ae7b7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cloud fraction per each single date expressed in % \n",
    "cloud_fraction = (cloud / aot_total * 100).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6474de-a8e8-4812-8b68-628a5ac12a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize cloud fraction\n",
    "cloud_fraction.hvplot.line(title='Cloud cover %', ylabel=\"&\") * hv.HLine(25).opts(\n",
    "    color='red',\n",
    "    line_dash='dashed',\n",
    "    line_width=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99341566-d8bc-4fab-a62b-7551106fe286",
   "metadata": {},
   "source": [
    "We are going to get the same information for the snow cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeef39f-736e-471b-b263-184b7d36aec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow = xr.where(clipped_date == 1, 1, np.nan).count(dim=['x', 'y']).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e1f00-a45d-4c87-b48f-ea822c092cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow_fraction = (snow / aot_total * 100).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae5b0e-b006-43e1-a5d3-c1864aaf8b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize snow fraction\n",
    "snow_fraction.hvplot.line(title='Snow cover area (%)', ylabel=\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644ffef-2997-446b-9cee-4d742523da70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mask out cloud fraction > 30% \n",
    "masked_cloud_fraction = cloud_fraction < 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91db275-8bde-4378-88b7-3baef866fcc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow_selected = snow_fraction.sel(date=masked_cloud_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784f8a6-ecdc-4e69-a310-154fae67fcfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow_selected.name = 'SCA'\n",
    "snow_selected.hvplot.line(title=\"Snow fraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb75ad9-cbb7-4c0b-b867-8d1c43f5faeb",
   "metadata": {},
   "source": [
    "Save the **cloud filtered snow fraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59282b5-3ee8-4e42-9556-c333b2922881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snow_selected.to_dataframe().to_csv(\"31_results/filtered_snow_fraction_pangeo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed7dd8-b72b-4c9f-88ee-7aa840e8b8ec",
   "metadata": {},
   "source": [
    "## Shutdown and Close local Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f253418b-75d9-44e8-91f9-f7f3d7eb5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbaa2d5-0780-4b4c-af80-a18a310ee1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ac7c0-8224-485c-8531-5aee790c71f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cubes-and-clouds-pangeo",
   "language": "python",
   "name": "conda-env-cubes-and-clouds-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1e85ce72ba894ef683e1cc95b248db05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "82d131f02e124db0a68c4988a87f3ae2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_1e85ce72ba894ef683e1cc95b248db05"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
