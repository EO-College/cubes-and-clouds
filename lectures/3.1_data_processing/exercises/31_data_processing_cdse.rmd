---
title: "openEO tutorial OEMC 2023"
author: "peterjames.zellner@eurac.edu, michele.claus@eurac.edu"
date: "2023-09-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In this exercise we will build a complete EO workflow on a cloud platform; from data access to obtaining the result. In this example we will analyse snow cover in the Alps.

We are going to follow these steps in our analysis:

-   Load satellite collections
-   Specify the spatial, temporal extents and the features we are interested in
-   Process the satellite data to retreive snow cover information
-   aggregate information in data cubes
-   Visualize and analyse the results

More information on the R-Client: <https://open-eo.github.io/openeo-r-client/>

# Libraries

```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(openeo)
library(stars)
library(sf)
library(lubridate)
library(mapview)
library(ggplot2)
library(scales)
library(plotly)
library(jsonlite)
library(dplyr)
library(tidyr)
library(knitr)
```

# Login

Connect to the copernicus dataspace ecosystem.

```{r login, echo=FALSE}
host = "https://openeo.dataspace.copernicus.eu/"
conn = connect(host)
```

And login.

```{r login, echo=FALSE, eval=FALSE}
login()
```

Check if the login worked.

```{r check_login, eval = FALSE}
conn$isConnected()
conn$isLoggedIn()
```

# Region of Interest

Load the catchment area.

```{r load_catch}
catchment = sf::st_read("data/catchment_outline.geojson")
mapview(catchment)
```

# Inspect Metadata

We need to set the following configurations to define the content of the data cube we want to access:

-   dataset name
-   band names
-   time range
-   the area of interest specifed via bounding box coordinates
-   spatial resolution

To select the correct dataset we can first list all the available datasets.

```{r collections}
openeo::list_collections()
```

We want to use the Sentinel-2 L2A product. It's name is `SENTINEL2_L2A`.

We get the metadata for this collection as follows.

```{r desc_coll}
openeo::describe_collection("SENTINEL2_L2A")
```

# Define a workflow

We will define our workflow now. And chain all the processes together we need for analyzing the snow cover in the catchment.

## Define the data cube

We define all extents of our data cube. We use the catchment as spatial extent. As a time range we will focus on the snow melting season 2018, in particular from Febraury to June 2018.

```{r bbox}
bbox = sf::st_bbox(obj = catchment)
bbox
```

```{r dc_defs}
collection = "SENTINEL2_L2A"
spatial_extent = bbox
temporal_extent = list("2018-02-01", "2018-06-30")
bands = list('B03', 'B11', 'SCL') 
```

## Load the data cube

We have defined the extents we are interested in. Now we use these definitions to load the data cube. First we need to load the openEO processes that are available at the backend. The object p holds all the processes available on the backend now! You can adress them via `p$...`. Auto completion is also available for getting the arguments.

```{r load_processes, eval=FALSE}
p = processes()
head(names(p))
```

Then we load our data cube.

```{r load_col, eval=FALSE}
s2 = p$load_collection(id = collection, 
                       spatial_extent = spatial_extent,
                       temporal_extent = temporal_extent, 
                       bands = bands)
```

```{r load_pg, eval=FALSE}
openeo::as.Process.Graph(s2)
```

## NDSI - Normalized Difference Snow Index

The Normalized Difference Snow Index (NDSI) is computed as:

$$ NDSI = {GREEN - SWIR \over GREEN + SWIR} $$

We have created a Sentinel-2 data cube with bands B03 (green), B11 (SWIR) and the cloud mask (CLM). We will use the green and SWIR band to calculate a the NDSI. This process is reducing the band dimension of the data cube to generate new information, the NDSI.

We define the function. The input x will be our Sentinel 2 data cube `s2` and we index the first and second band we have chosen in `load_collection`, the green and swir band.

```{r ndsi_function}
ndsi_function = function(x, context) {
  green = x[1]
  swir = x[2]
  (green - swir) / (green + swir)
}
```

And apply it along the bands dimension.

```{r ndsi_reduce, eval=FALSE}
ndsi = p$reduce_dimension(data = s2, reducer = ndsi_function, dimension = "bands")
```

```{r ndsi_pg, eval=FALSE}
openeo::as.Process.Graph(ndsi)
```

## Creating the snow map

So far we have a timeseries of NDSI values. We are intereseted in the presence of snow though. Ideally in a binary classification: snow and no snow. To achieve this we are setting a threshold of 0.4 on the NDSI. This gives us a binary snow map.

```{r thresh_function}
thresh_function = function(x, context){
  ndsi = x[1]
  (ndsi > 0.4)*1 
}
```

```{r snowmap, eval=FALSE}
snowmap = p$apply(data = ndsi, process = thresh_function)
```

```{r snowmap_pg, eval=FALSE}
openeo::as.Process.Graph(snowmap)
ras = get_sample(graph = snowmap, spatial_extent = bbox_sample, format = "netCDF", as_stars = TRUE)
```

```{r snowmap_smpl, eval=FALSE}
ras = get_sample(graph = snowmap, spatial_extent = bbox_sample, format = "netCDF", as_stars = TRUE)
```

## Creating a cloud mask

We are going to use "SCL" band for creating a cloud mask and then applying it to the NDSI. Here is more information on the [Scene Classification](https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm-overview). `8 cloud medium probability`, `9 cloud high probability`, `3 cloud shadow`

```{r mask_function}
mask_function = function(x, context){
  scl = x[3]
  (scl == 8 | scl == 9 | scl == 3) * 1 
}
```

```{r cloudmask, eval=FALSE}
#cloudmask = p$apply(data = s2, process = mask_function)
cloudmask = p$reduce_dimension(data = s2, reducer = mask_function, dimension = "bands")
```

```{r cloudmask_pg, eval=FALSE}
openeo::as.Process.Graph(cloudmask)
```

```{r cloudmask_smpl, eval=FALSE}
ras = get_sample(graph = cloudmask, spatial_extent = bbox_sample, format = "netCDF", as_stars = TRUE)
```

## Applying the cloud mask to the snow map

We will mask out all pixels that are covered by clouds. This will result in: 0 = no_snow, 1 = snow, NA = cloud

```{r snowmap_cloudfree, eval=FALSE}
snowmap_cloudfree = p$mask(data = snowmap, mask = cloudmask)
```

```{r cloudmask_pg, eval=FALSE}
openeo::as.Process.Graph(snowmap_cloudfree)
```

## From bounding box to catchment

So far we have been working on a rectangular bounding box. But we are interested in the processes within a catchment. So we clip to that cachment (setting all values outside of the catchment to NA).

```{r crop_catchment, eval=FALSE}
snowmap_cloudfree_catchment = p$mask_polygon(data = snowmap_cloudfree, mask = catchment)
```

```{r crop_catchment_pg, eval=FALSE}
openeo::as.Process.Graph(snowmap_cloudfree_catchment)
```

## Download and visualize a time slice of the snow cube

Let's download the whole image time series as a netcdf file to have a look how our first results look like. \`get_sample()\` is the ideal function in the R-Client to investigate intermediate results quickly.

```{r get_sample, eval=FALSE}
bbox_sample = catchment %>% st_transform(crs = 3035) %>%  st_centroid() %>% st_buffer(50) %>% st_transform(crs = 4326) %>% st_bbox()
ras = get_sample(graph = snowmap_cloudfree_catchment_1d, spatial_extent = bbox_sample, format = "netCDF", as_stars = TRUE)
```

```{r plot_sample, eval=FALSE}
mapview(ras) + mapview(catchment)
```

## Calculate Catchment Statistics

We are looking at a region over time. We need to make sure that the information content meets our expected quality. Therefore, we calculate the **cloud percentage** for the catchment for each timestep. We use this information to filter the timeseries. All timesteps that have a cloud coverage of over 25% will be discarded.

Ultimately we are interested in the **snow covered area (SCA)** within the catchment. We count all snow covered pixels within the catchment for each time step. Multiplied by the pixel size that would be the snow covered area. Divided the pixel count by the total number of pixels in the catchment is the percentage of pixels covered with snow. We will use this number.

Get number of pixels in catchment.

```{r percentages, eval=FALSE}

# number of all pixels
n_catch_fun = function(x, context){(x > -1) * 1} # p$gt(y = -1)
n_catch = p$apply(data = snowmap, process = n_catch_fun) %>% p$add_dimension(name = "bands", type = "bands", label = "n_catch")

# number of cloud pixels (no function needed, mask already created before)
n_cloud = p$add_dimension(data = cloudmask, name = "bands", type = "bands", label = "n_cloud")

# number of snow pixels
n_snow_fun = function(x, context){(x == 1) * 1} # p$eq(y = 1)
n_snow = p$apply(snowmap_cloudfree, process = n_snow_fun) %>% p$add_dimension(name = "bands", type = "bands", label = "n_snow")

# combine the binary data cubes into one data cube
n_cc = p$merge_cubes(cube1 = n_catch, cube2 = n_cloud)
n_ccs = p$merge_cubes(cube1 = n_cc, cube2 = n_snow)

# aggregate to catchment
n_ccs_catch = p$aggregate_spatial(data = n_ccs, geometries = catchment, reducer = p$sum)
```

Get a sample of the 3 band binary data cube we have created before the aggregation.

```{r n_ccs, eval=FALSE}
ras = get_sample(graph = n_ccs, spatial_extent = bbox_sample, format = "netCDF", as_stars = TRUE)
```

Create batch job to start processing on the backend.

```{r n_ccs_job, eval=FALSE}
n_ccs_catch_json = p$save_result(data = n_ccs_catch, format = "JSON")
job = create_job(graph = n_ccs_catch_json, title = "n_ccs_catch_json")

start_job(job = job)

list_jobs() %>% as.data.frame() %>%  filter(id == job$id)

pth = download_results(job = job, folder = "data/")
pth
```

Load the result. It contains the number of pixels in the catchment, clouds and snow. We can calculate the percentages of cloud and snow pixels in the catchment.

```{r percentages, eval=TRUE}

# load result
perc = jsonlite::read_json(pth[[1]], simplifyVector = TRUE)

# clean
perc = lapply(X = perc, FUN = as.data.frame) %>% 
  bind_rows(.id = "time") %>% 
  rename(n_catch = V1, n_cloud = V2, n_snow = V3) %>% 
  mutate(time = as.Date(time), 
         perc_cloud = round(n_cloud / n_catch, 2), 
         perc_snow = round(n_snow / n_catch, 2))

# peak
perc # careful unsorted dates!
```

Add some filters for the cases where there are not enough pixels in the catchment (catchment crossing s2 tiles and where cloud cover is higher than 25%.

```{r percentages_filter, eval=TRUE}
perc = perc %>% mutate(filter_catch = if_else(n_catch < max(n_catch), FALSE, TRUE), 
                       filter_cloud = if_else(perc_cloud > 0.25, FALSE, TRUE))
```

Plot the timeseries and the cloud threshold of 25%. If the cloud cover is higher the timestep will be excluded later on.

Check the filters.

```{r cloud_perc_tbl, eval=TRUE}
ftable(perc$filter_cloud, perc$filter_catch, dnn = c("filter_cloud", "filter_catch"))

```

Plot the cloud percentage with the threshold.

```{r cloud_perc_plt, eval=TRUE}
ggplot(data = perc, mapping = aes(x=time, y=perc_cloud)) + 
  geom_point() + geom_line() + geom_hline(yintercept = 0.25)
```

Plot the unfiltered snow percentage

```{r snow_perc_plt1, eval=TRUE}
# no filter - mess
ggplot(data = perc, mapping = aes(x=time, y=perc_snow)) + 
  geom_point() + geom_line()
```

Plot the cloud filtered snow percentage

```{r snow_perc_plt2, eval=TRUE}
# filter clouds > 25%
ggplot(data = perc %>% filter(filter_cloud == TRUE), mapping = aes(x=time, y=perc_snow)) + geom_point() + geom_line()
```

Plot the cloud and catchment size filtered snow percentage

```{r snow_perc_plt3, eval=TRUE}
# filter clouds and small tile
ggplot(data = perc %>% filter(filter_cloud == TRUE & filter_catch == TRUE), mapping = aes(x=time, y=perc_snow)) + geom_point() + geom_line()

```

## Compare to discharge data

Load the discharge data at Meran. The main outlet of the catchment.

```{r discharge_load, eval=TRUE}
disch = readr::read_csv("data/ADO_DSC_ITH1_0025.csv") %>% rename(time = Time) %>% filter(between(time, as.Date("2018-02-01"), as.Date("2018-06-30"))) %>%  mutate(time = as.Date(time))
head(disch)
```

Compare the discharge data to the snow covered area.

```{r discharge_plot, eval=TRUE}
# data cleaning
snow = perc %>% filter(filter_cloud == TRUE) %>% mutate(index = "perc_snow") %>% select(time, index, val = perc_snow) %>% mutate(val = val/0.015) # for scaling
disch = disch %>% mutate(index = "disch_m3_s") %>% select(time, index, val = discharge_m3_s)
snow_disch = rbind(snow, disch)

# plot
ggplot(data = snow_disch, mapping = aes(x = time, y = val, color = index)) + 
  geom_point() + 
  geom_line() +
  scale_y_continuous(name = "discharge_m3_s", sec.axis = sec_axis(~.*0.015, name = "perc_snow"))

```
