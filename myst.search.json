{"version":"1","records":[{"hierarchy":{"lvl1":"How to contribute"},"type":"lvl1","url":"/contributing","position":0},{"hierarchy":{"lvl1":"How to contribute"},"content":"This is a online course about Open Science on EO Cloud Platforms. By it‚Äôs nature, the topic is fast paced and keeping up with the development of cloud technologies, standards and concepts isn‚Äôt easy. To keep up to date and to maintain a functioning exercises we would really appreciate your contributions!","type":"content","url":"/contributing","position":1},{"hierarchy":{"lvl1":"How to contribute","lvl2":"Types of contributions"},"type":"lvl2","url":"/contributing#types-of-contributions","position":2},{"hierarchy":{"lvl1":"How to contribute","lvl2":"Types of contributions"},"content":"We‚Äôre happy to receive contributions to improve the quality of the course. Here are some potential areas where help would be greatly appreciated","type":"content","url":"/contributing#types-of-contributions","position":3},{"hierarchy":{"lvl1":"How to contribute","lvl3":"Review","lvl2":"Types of contributions"},"type":"lvl3","url":"/contributing#review","position":4},{"hierarchy":{"lvl1":"How to contribute","lvl3":"Review","lvl2":"Types of contributions"},"content":"Read through the material through the \n\nrendered web pages. However, bear in mind that the official content of the course is available on the EO-College platform at \n\nhttps://‚Äãeo‚Äã-college‚Äã.org‚Äã/courses‚Äã/cubes‚Äã-and‚Äã-clouds/ and you may find slightly differences between the two material. We strongly encourage you to check both content before suggesting changes.\n\nIf you want to fix something directly: Open a pull request to fix it. Don‚Äôt add too much new information though. The lessons have a certain length.\n\nIf you have comments: Open an issue for your review. Name it for example ‚ÄúReview Section 1.3 Open Science‚Äù. Add all comments you have in that issue with links to the file where they apply.","type":"content","url":"/contributing#review","position":5},{"hierarchy":{"lvl1":"How to contribute","lvl3":"Other","lvl2":"Types of contributions"},"type":"lvl3","url":"/contributing#other","position":6},{"hierarchy":{"lvl1":"How to contribute","lvl3":"Other","lvl2":"Types of contributions"},"content":"Adding new content: If you have an idea for new content, please open an issue to discuss this.\n\nReport Bugs: If any of the exercises are not working. Please report a bug here by opening an issue.\n\nFix Bugs: Make a pull request with your fix.","type":"content","url":"/contributing#other","position":7},{"hierarchy":{"lvl1":"How to contribute","lvl3":"Generate Rendered Web Pages Locally","lvl2":"Types of contributions"},"type":"lvl3","url":"/contributing#generate-rendered-web-pages-locally","position":8},{"hierarchy":{"lvl1":"How to contribute","lvl3":"Generate Rendered Web Pages Locally","lvl2":"Types of contributions"},"content":"We recommend you render the web pages locally to check your changes. Below is a short guide on how to generate the rendered web pages locally:\n\nIf conda is installed on your platform, you can skip this first step. Otherwise, we recommend you install \n\nminiconda.\n\nCreate the cubes-and-clouds conda environment (environment.yml file is located in the root folder of the cubes-and-clouds github repository:conda env create -f environment.yml\n\nBuild the rendered web pages using \n\nJupyter Book:jupyter-book build lectures\n\nTo visualize the rendered web pages, open lectures/_build/html/index.html in your preferred web browser from the root folder of the cubes-and-clouds GitHub repository.","type":"content","url":"/contributing#generate-rendered-web-pages-locally","position":9},{"hierarchy":{"lvl1":"How to contribute","lvl2":"Acknowledgement"},"type":"lvl2","url":"/contributing#acknowledgement","position":10},{"hierarchy":{"lvl1":"How to contribute","lvl2":"Acknowledgement"},"content":"We want every conrtibution to be acknwoledged. That‚Äôs why we have the \n\nall-contributors bot installed. It allows you to acknowledge your own contributions and to appear in the \n\nContributors. If you want your contribution to be acknowledged Comment on your Issue or Pull Request, asking the  bot to add a contributor (likely yourself):@all-contributors please add @<username> for <contributions>\n\nHere‚Äôs the list of \n\npossible contribution types\n\nFor more detailed information check the \n\nbot usage documentation\nA real example would look like this@all-contributors please add @przell for maintenance","type":"content","url":"/contributing#acknowledgement","position":11},{"hierarchy":{"lvl1":"Contributors"},"type":"lvl1","url":"/contributors","position":0},{"hierarchy":{"lvl1":"Contributors"},"content":"","type":"content","url":"/contributors","position":1},{"hierarchy":{"lvl1":"Contributors"},"type":"lvl1","url":"/contributors#contributors","position":2},{"hierarchy":{"lvl1":"Contributors"},"content":" ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section  prettier-ignore-start  markdownlint-disable \n\nprzell\n\nüñã\n\nUndeadFairy\n\nüñã \n\nüíª\n\nAnca Anghelea\n\nüëÄ \n\nüñã\n\nkvrouwenvelder\n\nüëÄ\n\nMatthias Mohr\n\nüëÄ\n\nJoris C\n\nüêõ markdownlint-restore  prettier-ignore-end  ALL-CONTRIBUTORS-LIST:END ","type":"content","url":"/contributors#contributors","position":3},{"hierarchy":{"lvl1":"Cubes and Clouds"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Cubes and Clouds"},"content":"\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Cubes and Clouds"},"type":"lvl1","url":"/#cubes-and-clouds","position":2},{"hierarchy":{"lvl1":"Cubes and Clouds"},"content":"This is the official content repository for the online course \n\n‚ÄòCubes & Clouds - Cloud Native Open Data Sciences for Earth Observation‚Äô hosted on EO College.","type":"content","url":"/#cubes-and-clouds","position":3},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Description"},"type":"lvl2","url":"/#description","position":4},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Description"},"content":"The Massive Open Online Course ‚ÄòCubes & Clouds - Cloud Native Open Data Sciences for Earth Observation‚Äô teaches the concepts of data cubes, cloud platforms and open science in the context of earth observation. Here‚Äôs the video on \n\nyoutube in case it doesn‚Äôt play here.\n\nhttps://‚Äãuser‚Äã-images‚Äã.githubusercontent‚Äã.com‚Äã/51962348‚Äã/234564489‚Äã-98a56dd2‚Äã-1359‚Äã-4972‚Äã-ad3c‚Äã-0f0f2d4b450e‚Äã.mp4","type":"content","url":"/#description","position":5},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Target Group","lvl2":"Description"},"type":"lvl3","url":"/#target-group","position":6},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Target Group","lvl2":"Description"},"content":"It targets Earth Science students and researchers who want to increase their technical capabilities onto the newest standards in EO computing, as well as Data Scientists who want to dive into the world of EO and apply their technical background to a new field.  Before starting, prerequisites are a general knowledge of EO and python programming.","type":"content","url":"/#target-group","position":7},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Content","lvl2":"Description"},"type":"lvl3","url":"/#content","position":8},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Content","lvl2":"Description"},"content":"The course explains the concepts of data cubes, EO cloud platforms and open science by applying them to a typical EO workflow from data discovery, data processing up to sharing the results in an open and FAIR (Findable, Accessible, Interoperable, Reusable) way. An engaging mixture of videos, animated content, lectures, hands-on exercises and quizzes transmits the content.flowchart LR\n      subgraph Concepts\n      A[1.1 What is a Cloud Platform]-->B[1.2 What is a Data Cube];\n      B-->C[1.3 What is Open Science];\n      end\n      subgraph Discovery\n      D[2.1 Data Discovery]-->E[2.2 Data Properties];\n      E-->F[2.3 Data Access];\n      F-->G[2.4 Formats and Performance]\n      end\n      subgraph Process&Share\n      H[3.1 Processing]-->I[3.2 Validation];\n      I-->J[3.3 Sharing];\n\n      end\n      Concepts --> Discovery\n      Discovery --> Process&Share\n      ","type":"content","url":"/#content","position":9},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Learning Objectives","lvl2":"Description"},"type":"lvl3","url":"/#learning-objectives","position":10},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Learning Objectives","lvl2":"Description"},"content":"After finishing the participant will understand the theoretical concepts of cloud native EO processing and have gained practical experience by conducting an end-to-end EO workflow. The participant will be capable of independently using cloud platforms to approach EO related research questions and be confident in how to share research by adhering to the concepts of open science.","type":"content","url":"/#learning-objectives","position":11},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Interactive Cubes and Clouds Map produced by the participants"},"type":"lvl2","url":"/#interactive-cubes-and-clouds-map-produced-by-the-participants","position":12},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Interactive Cubes and Clouds Map produced by the participants"},"content":"Check out the interactive cubes and clouds map! It‚Äôs produced by the participants of the course. Every participant adds their contribution to a community mapping project: mapping the snow cover of mountainous regions together!\n\nCubes and Clouds: Snow Cover STAC Collection","type":"content","url":"/#interactive-cubes-and-clouds-map-produced-by-the-participants","position":13},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Sign-On Guide"},"type":"lvl2","url":"/#sign-on-guide","position":14},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Sign-On Guide"},"content":"To follow the course completely, especially to execute the hands-on exercises on cloud platforms you will sign in to some free services.\n\nEOCollege: Access to the e-learning platform and the jupyterhub coding environment hosted by EOX.\n\nCopernicus Data Space Ecosystem: A cloud platform offering the data collections and computing resources for the exercises of the course.\n\nYou can find all the necessary information in the lecture \n\nIntroduction.","type":"content","url":"/#sign-on-guide","position":15},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Contributing"},"type":"lvl2","url":"/#contributing","position":16},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Contributing"},"content":"We‚Äôre happy to receive your contributions to lessons, exercises, bug reports etc.\n\nCheck out the \n\nHow to contribute contributing guide to learn how you can contribute!\n\nThe full list of all contributors is in the \n\nContributors","type":"content","url":"/#contributing","position":17},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Availability of the course"},"type":"lvl2","url":"/#availability-of-the-course","position":18},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Availability of the course"},"content":"The course is openly available on EOCollege, a cloud learning platform for EO content. EOCollege is your one-stop-shop for this course. You will have an integrated experience of all the components: lectures, exercises, quizzes and get a certificate upon successful completion: \n\nhttps://‚Äãeo‚Äã-college‚Äã.org‚Äã/courses‚Äã/cubes‚Äã-and‚Äã-clouds\n\nThe course material is available on zenodo. Each lecture can be accessed individually: \n\nCubes and Clouds Zenodo Community\n\nAlternative Rendering of the Course\n\nJupyter Book: web page of the course deployed using GitHub pages and Jupyter Books, updated at every merge of PR via GitHub actions.\n\nLIAScript: Use the link to the .md files and copy them into LIAscript. It will give you an online lecture directly.\n\nObsidian: Obsidian is a knowledge management system. Add obsidian file toplevel to create your void, or add the course to an existing one.\n\nBookdown: You can render the course using bookdown or similar to have a rendered version of the markdown files in the repo. Some work needed to do that.","type":"content","url":"/#availability-of-the-course","position":19},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Authors and Acknowledgement"},"type":"lvl2","url":"/#authors-and-acknowledgement","position":20},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Authors and Acknowledgement"},"content":"","type":"content","url":"/#authors-and-acknowledgement","position":21},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Reviewers","lvl2":"Authors and Acknowledgement"},"type":"lvl3","url":"/#reviewers","position":22},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Reviewers","lvl2":"Authors and Acknowledgement"},"content":"1.1 What is a Platform: \n\nJeroen Dries, VITO\n\n1.2 What is a Data Cube: \n\nPontus Lurcock, Brockmann Consult; \n\nDavid Montero, University of Leipzig; \n\nGunnar Brandt, Brockann Consult\n\n1.3 What is Open Science (1.3.1, 1.3.2, 1.3.3): \n\nKristina Vrouwenvelder, American Geoscience Union; \n\nShelley Stall, American Geoscience Union\n\n2.1 Data Discovery: \n\nMatthias Mohr, Matthias Mohr Softwareentwicklung\n\n2.2 Data Properties: \n\nAngelos Tzotsos, Open Source Geospatial Foundation; \n\nTom Kralidis, Meteorological Service of Canada\n\n2.3 Data Access and Basic Processing: \n\nEdzer Pebesma, University of M√ºnster\n\n2.4 Formats and Performance: \n\nAimee Barciauskas, Development Seed; \n\nRyan Avery, Development Seed\n\n3.1 Processing: \n\nMattia Callegari, Eurac Research\n\n3.2 Validation: \n\nHanna Meyer, Universtiy of M√ºnster\n\n3.3 Sharing: \n\nLeandro Parente, Open Geo Hub Foundation","type":"content","url":"/#reviewers","position":23},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Authors","lvl2":"Authors and Acknowledgement"},"type":"lvl3","url":"/#authors","position":24},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl3":"Authors","lvl2":"Authors and Acknowledgement"},"content":"   \n\n   \n\n   \n\n   \n\n","type":"content","url":"/#authors","position":25},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Citation"},"type":"lvl2","url":"/#citation","position":26},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Citation"},"content":"Please refer to the whole course as described in the \n\nCITATION.cff file\n\nZellner, P. J., Dolezalova, T., Claus, M., Eberle, J., Balogun, R. O., Mei√ül, S., Eckardt, R., Hodam, H., Jacob, A., & Anghelea, A. (2024). Cubes & Clouds - Cloud Native Open Data Sciences for Earth Observation (v1.0.0). Zenodo. \n\nZellner et al. (2024)\n\nThe individual chapters can be reused and explicitly cited as listed in the \n\nCubes and Clouds Zenodo Community.","type":"content","url":"/#citation","position":27},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"License"},"type":"lvl2","url":"/#license","position":28},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"License"},"content":"Where not stated explicitly otherwise this work is licensed under a \n\nCreative Commons Attribution 4.0 International License.","type":"content","url":"/#license","position":29},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Funding"},"type":"lvl2","url":"/#funding","position":30},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Funding"},"content":"This project is funded by the European Space Agency through the program EXPRO+ with the contract number 4000138850/22/I-DT","type":"content","url":"/#funding","position":31},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Project Status"},"type":"lvl2","url":"/#project-status","position":32},{"hierarchy":{"lvl1":"Cubes and Clouds","lvl2":"Project Status"},"content":"The project is currently work in progress.","type":"content","url":"/#project-status","position":33},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/lectures/introduction/introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"This short sections will familiarize you with some free external services that are going to be used throughout the MOOC.\n\nHow to sign up for the Copernicus Data Space Ecosystem (this is where the cloud processing will happen)\n\nFirst steps in the Coding Environment","type":"content","url":"/lectures/introduction/introduction","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Sign up for EO College"},"type":"lvl2","url":"/lectures/introduction/introduction#sign-up-for-eo-college","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Sign up for EO College"},"content":"In case you haven‚Äôt already done it. Sign up for the MOOC ‚ÄòCubes & Clouds - Cloud Native Open Data Sciences for Earth Observation‚Äô\n\nhttps://‚Äãeo‚Äã-college‚Äã.org‚Äã/courses‚Äã/cubes‚Äã-and‚Äã-clouds","type":"content","url":"/lectures/introduction/introduction#sign-up-for-eo-college","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Sign up for Copernicus Data Space Ecosystem"},"type":"lvl2","url":"/lectures/introduction/introduction#sign-up-for-copernicus-data-space-ecosystem","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Sign up for Copernicus Data Space Ecosystem"},"content":"We are going to use the cloud processing platform Copernicus Data Space Ecosystem to carry out our exercises. It‚Äôs a cloud platform hosted by the European Commission, ESA and Copernicus.The processing will happen there and you will get 1000 free credits. That‚Äôs enough to complete the course.\n\nhttps://‚Äãdocumentation‚Äã.dataspace‚Äã.copernicus‚Äã.eu‚Äã/Registration‚Äã.html","type":"content","url":"/lectures/introduction/introduction#sign-up-for-copernicus-data-space-ecosystem","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Your first steps in the Coding Environment"},"type":"lvl2","url":"/lectures/introduction/introduction#your-first-steps-in-the-coding-environment","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Your first steps in the Coding Environment"},"content":"Here is a guide how to find your way around in the Coding Environment JupyterHub. You will see a button that forwards you there whenever there is a hands-on exercise to do.\n\n \n\nRegister on EOXHub\n\nhttps://‚Äãworkspace‚Äã.cubes‚Äã-and‚Äã-clouds‚Äã.earthcode‚Äã.eox‚Äã.at/\n\nAfter registering on EOX you are now ready to start your first exercise!\n\nExercise 0 Introduction","type":"content","url":"/lectures/introduction/introduction#your-first-steps-in-the-coding-environment","position":7},{"hierarchy":{"lvl1":"0. Introduction"},"type":"lvl1","url":"/lectures/introduction/exercises/login","position":0},{"hierarchy":{"lvl1":"0. Introduction"},"content":"\n\n","type":"content","url":"/lectures/introduction/exercises/login","position":1},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Login to openEO"},"type":"lvl2","url":"/lectures/introduction/exercises/login#login-to-openeo","position":2},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Login to openEO"},"content":"In order to access a cloud platfrom you need to login. In this notebook we will login to the Copernicus Data Space Ecosystem using openEO.\n\nImport the libraries we need to interact with the cloud platform\n\nMake sure we have the login credentials\n\nConnect to the cloud platform\n\nLogin to the cloud platform\n\nCheck that the login worked\n\n","type":"content","url":"/lectures/introduction/exercises/login#login-to-openeo","position":3},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/introduction/exercises/login#libraries","position":4},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Libraries"},"content":"We will import the openeo python client library. It is preinstalled in the jupyter workspace on EOX.\n\nopeneo: The openeo python client has all the functions available that we need to interact with the cloud platform using the openEO API.\n\nHere is more information on the openeo python client:\n\nhttps://‚Äãopen‚Äã-eo‚Äã.github‚Äã.io‚Äã/openeo‚Äã-python‚Äã-client/\n\n\n\nimport openeo\n\n\n\n","type":"content","url":"/lectures/introduction/exercises/login#libraries","position":5},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Connect to the cloud platform"},"type":"lvl2","url":"/lectures/introduction/exercises/login#connect-to-the-cloud-platform","position":6},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Connect to the cloud platform"},"content":"In a first step we connect to the cloud platform. We can only see information and use functionality that is available to everybody.\nWe can see for example which collections and processes are available, but we cannot process data. We will explore the platforms capabilities in an extra exercise in more depth later.\n\nNow let‚Äôs just connect to the platform...\n\nconn = openeo.connect('https://openeo.dataspace.copernicus.eu/')\n\n\n\n... and check if the connection has worked. You should see that you are connected, but not logged in.\n\nconn\n\n\n\n","type":"content","url":"/lectures/introduction/exercises/login#connect-to-the-cloud-platform","position":7},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Login to the cloud platform"},"type":"lvl2","url":"/lectures/introduction/exercises/login#login-to-the-cloud-platform","position":8},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Login to the cloud platform"},"content":"After we have connected to the platform and want more functionality, we need to login. This means we authenticate ourselfs to prove we are an registered user.\nAfter access is granted we can also process data. Every computation comes at a cost, this is why every user has an amount of credits (which usually have to be payed) for computing.\nEverytime you are going to use the cloud platform for processing you will have to login at the beginning of your workflow. We are going to learn how to create a workflow in a seperat exercise later on.\n\nNow let‚Äôs just log in... (this only works if you haver registered to CDSE as described in the lesson Introduction)\n\nconn.authenticate_oidc()\n\n\n\n... and check if the login has worked...\n\nconn\n\n\n\n... and let‚Äôs check our user information, which is possible since we have authenticated now.\n\nconn.describe_account()\n\n\n\n","type":"content","url":"/lectures/introduction/exercises/login#login-to-the-cloud-platform","position":9},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Return to EOCOllege"},"type":"lvl2","url":"/lectures/introduction/exercises/login#return-to-eocollege","position":10},{"hierarchy":{"lvl1":"0. Introduction","lvl2":"Return to EOCOllege"},"content":"This is all. We have verified that we can connect and login to the cloud platform. We will do this again later on when we‚Äôll start with some hands on exercises.\n\nFor now let‚Äôs return to EOCollege to get started with the lessons!\n\nReturn to Cubes and Clouds on EO College","type":"content","url":"/lectures/introduction/exercises/login#return-to-eocollege","position":11},{"hierarchy":{"lvl1":"Earth Observation cloud platforms"},"type":"lvl1","url":"/lectures/intro-platform/intro-platform","position":0},{"hierarchy":{"lvl1":"Earth Observation cloud platforms"},"content":"","type":"content","url":"/lectures/intro-platform/intro-platform","position":1},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/intro-platform/intro-platform#learning-objectives","position":2},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Learning Objectives"},"content":"Understand why using a platform is useful\n\nDifferentiate platform offerings\n\nGet to know the components and building blocks of a platform","type":"content","url":"/lectures/intro-platform/intro-platform#learning-objectives","position":3},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Why do we need EO cloud platforms?"},"type":"lvl2","url":"/lectures/intro-platform/intro-platform#why-do-we-need-eo-cloud-platforms","position":4},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Why do we need EO cloud platforms?"},"content":" \n\nVideo content in cooperation with \n\nJeroen Dries (VITO). \nNumbers based on \n\nESA Annual Sentinel Data Access Report 2022 \n\nTraditional approaches for the analysis of Earth Observation (EO) data typically involve several steps, including data discovery, data download, data pre-processing, and data analysis. Especially when working with multiple datasets, handling data discovery, download, and access is a tremendous task, where users need to navigate through different interfaces, adhere to varying access requirements, and manage the heterogeneity of data formats. This approach is often time-consuming and requires significant effort to aggregate and harmonize datasets from different providers for comprehensive analysis.\n\nFigure: EO research withouth cloud facilities.","type":"content","url":"/lectures/intro-platform/intro-platform#why-do-we-need-eo-cloud-platforms","position":5},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"EO data volume and the limits of your computer","lvl2":"Why do we need EO cloud platforms?"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#eo-data-volume-and-the-limits-of-your-computer","position":6},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"EO data volume and the limits of your computer","lvl2":"Why do we need EO cloud platforms?"},"content":"In the field of Earth Observation, satellite missions like Sentinel-2 provide vast amounts of data that play a crucial role in various applications, including environmental monitoring, land cover mapping, and climate analysis. Understanding the volume of data involved in an analysis is critical for efficient data processing. EO datasets can span terabytes and petabytes, making it impractical to store, manage, and process them entirely on a local computer.\n\nThe increasing availability of vast amounts of EO data from multiple satellites presents challenges in terms of the time required for data download and pre-processing on individual computers or infrastructures. Within the Copernicus program of the European Union, around 64 million products have been published, which sums up to more than 25 Petabyte of data volume. In the following slider we have collected some statistics about the amounts of EO data from the Sentinel satellites.\n\nFigure: EO data volume and the limits of your computer.\n\nThe following interactive exercise assists in estimating the data volume associated with Sentinel-2 data. This calculator allows users to gain insights into the data volumes involved in specific regions and time ranges, further emphasizing the relevance of using EO platforms for scientific analyses. Not convinced of clouds yet? Try the volume calculator below to asses how much space you have to free up on your hard drive for your next project.","type":"content","url":"/lectures/intro-platform/intro-platform#eo-data-volume-and-the-limits-of-your-computer","position":7},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"How can we handle such volumes of data?","lvl2":"Why do we need EO cloud platforms?"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#how-can-we-handle-such-volumes-of-data","position":8},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"How can we handle such volumes of data?","lvl2":"Why do we need EO cloud platforms?"},"content":"Cloud infrastructure and platforms have emerged as viable alternatives to the traditional approach of data analyses as described in Figure ‚ÄúEO research without cloud facilities‚Äù. These solutions combine data storage and compute resources, enabling users to conduct their data analysis in close proximity to the data itself. By leveraging cloud-based infrastructures, researchers and analysts can optimize their workflow by minimizing the time-consuming steps of data transfer and pre-processing, thereby allowing them to focus more efficiently on data analysis tasks.\n\nBy utilizing cloud-based resources, users can harness the scalability and flexibility of these platforms to handle the extensive datasets generated by EO missions. Cloud-based EO platforms represent a paradigm shift in EO data analysis, offering a comprehensive ecosystem that seamlessly integrates storage, processing, analysis tools, collaboration, and visualization. These platforms empower users to overcome the challenges posed by large-scale EO data and accelerate scientific advancements in various fields, including environmental monitoring, climate studies, natural resource management, and disaster response.","type":"content","url":"/lectures/intro-platform/intro-platform#how-can-we-handle-such-volumes-of-data","position":9},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Types of platforms"},"type":"lvl2","url":"/lectures/intro-platform/intro-platform#types-of-platforms","position":10},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Types of platforms"},"content":"Cloud-based EO infrastructures and platforms have emerged to meet the growing demand for efficient data processing, analysis, and visualization close to the data. We can distinguish between infrastructure providers and platform providers.","type":"content","url":"/lectures/intro-platform/intro-platform#types-of-platforms","position":11},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Infrastructure providers","lvl2":"Types of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#infrastructure-providers","position":12},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Infrastructure providers","lvl2":"Types of platforms"},"content":"EO-based infrastructure providers focus on offering the underlying infrastructure necessary for processing, storage, and dissemination of EO data. They provide the computing resources, storage capacity, and networking capabilities required to handle large-scale EO data processing and analysis. These providers often build and maintain data centers and server clusters, ensuring reliable and scalable infrastructure for EO applications.\n\nIn comparison to the traditional approach (Figure ‚ÄúEO research without cloud facilities‚Äù), this approach allow users to use computing resources close to the EO data (see Figure ‚ÄúEO cloud providers‚Äù). Users do not need to download and manage EO data on their own, they can simply use what is already available on the infrastructure. However, there are no EO-specific services for data discovery, access, visualization, and analysis.\n\nFigure: EO cloud providers.\n\nExamples of EO-based infrastructure providers\n\nAmazon Web Services (AWS): AWS offers a wide range of cloud services, including storage (Amazon S3) and computing (Amazon EC2), which can be leveraged for EO data processing and storage. Various open data are available on AWS (\n\nhttps://‚Äãregistry‚Äã.opendata‚Äã.aws/).\n\nGoogle Cloud Platform (GCP): GCP provides infrastructure services like Google Cloud Storage and Google Compute Engine, which can be utilized for EO data management and analysis. Various open data are available on GCP (\n\nhttps://‚Äãcloud‚Äã.google‚Äã.com‚Äã/datasets).\n\nMicrosoft Azure: Azure offers cloud-based services such as Azure Storage, Azure Virtual Machines, and Azure Machine Learning, enabling EO applications and workflows.\n\nOpen Telekom Cloud: Open Telekom Cloud is a cloud platform offered by Deutsche Telekom. It provides scalable infrastructure resources, including computing, storage, and networking capabilities, suitable for processing and storing large volumes of EO data.\n\nCloudferro: Cloudferro is a cloud infrastructure provider specializing in geospatial data processing and analysis. They offer scalable and secure cloud resources optimized for EO applications. Cloudferro provides high-performance computing, storage, and networking services tailored for EO data processing workflows. Various open data are available on Cloudferro (\n\nhttps://‚Äãcloudferro‚Äã.com‚Äã/en‚Äã/eo‚Äã-cloud‚Äã/storage‚Äã-big‚Äã-data/).","type":"content","url":"/lectures/intro-platform/intro-platform#infrastructure-providers","position":13},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Platform providers","lvl2":"Types of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#platform-providers","position":14},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Platform providers","lvl2":"Types of platforms"},"content":"Platform providers focus on delivering comprehensive EO platforms that combine infrastructure, tools, and services into a cohesive environment. These platforms typically offer a suite of integrated capabilities, including data storage, processing, analysis, visualization, and collaboration tools. They provide a user-friendly interface and simplify the EO data lifecycle, enabling users to access, process, and analyze EO data without managing the underlying infrastructure.\n\nOn top of providing the infrastructure which allows users to do the computations close to the EO data , making available a platform additionally enables the use of specific Application Programming Interfaces (APIs) for the discovery, access, visualization, exploitation, and analysis of EO data.  EO platforms are often made available on infrastructure providers to benefit from the EO data storage. Users of a platform can now use harmonized interfaces for all data, which is available on the platform.\n\nFigure: EO platform providers.\n\nExamples of cloud-based EO platform providers\n\nGoogle Earth Engine is a platform specifically designed for EO data analysis. It provides access to a vast amount of satellite imagery and geospatial datasets, along with powerful processing capabilities and built-in algorithms.\n\nSinergise Sentinel-Hub is a platform focused on accessing and processing satellite data. It provides APIs and easy-to-use tools for accessing, processing, and visualizing EO data.\n\nMicrosoft Planetary Computer is a platform that combines geospatial data and AI capabilities for Earth observation. It provides access to various global datasets, including satellite imagery, climate data, and environmental data. The platform aims to facilitate large-scale data analysis and support sustainable development and conservation efforts.\n\nEuro Data Cube is a platform on top of various cloud infrastructures to provide an interactive development environment with a standardized access to various EO data. It provides a JupyterLab environment for data exploration and analysis, as well as capabilities to run processing jobs.\n\nOpenEO Platform is a platform based on OpenEO, which aims to standardize and simplify the access and processing of EO data. It provides a unified API and common data model, enabling interoperability across multiple EO data providers and processing backends. The platform allows users to run EO workflows on various cloud-based infrastructure providers.\n\nPangeo is a community platform for big data geoscience built on the Pangeo ecosystem. It aims to foster collaboration among researchers, engineers, research software engineers, and innovators within the open-source scientific Python ecosystem, focusing on Ocean, Atmosphere, Land, and Climate science. There is a strong focus on portability and interoperability, enabling deployments of the Pangeo platform on various infrastructure (laptops, cloud providers, etc.) and providing APIs that allow users to prototype on their laptops and easily scale to the cloud or High-Performance Computers with minimal changes to their code. The current deployment of the Pangeo platform in Europe, \n\nPangeo@EOSC is hosted on the \n\nEuropean Open Science Cloud (EOSC), providing a scalable and collaborative environment for big data analysis and research for all European researchers and their collaborators.\n\nDifferent EO platform providers may offer end-users access to specific tools and packages that can be highly beneficial for particular workflows. However, users should carefully evaluate whether the convenience of these tools justifies the potential trade-off of being locked into a particular platform. In some cases, opting for platforms that prioritize portability and openness, like OpenEO or Pangeo, might be more advantageous, especially for those who value flexibility and long-term interoperability across various environments and infrastructures.","type":"content","url":"/lectures/intro-platform/intro-platform#platform-providers","position":15},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Summary","lvl2":"Types of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#summary","position":16},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Summary","lvl2":"Types of platforms"},"content":"In summary, EO-based infrastructure providers primarily focus on providing the underlying infrastructure and resources, while platform providers offer integrated environments with a wide range of tools and services to support EO data processing, analysis, and visualization. These two types of providers complement each other in the EO ecosystem, enabling users to access and leverage EO data effectively.","type":"content","url":"/lectures/intro-platform/intro-platform#summary","position":17},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Components of platforms"},"type":"lvl2","url":"/lectures/intro-platform/intro-platform#components-of-platforms","position":18},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Components of platforms"},"content":"Cloud-based EO platforms have transformed the way researchers and scientists analyze and utilize EO data. These platforms often follow a three-layered design (often named ‚Äútiers‚Äù) comprising infrastructure, services, and exploitation interfaces. An example architecture based on this approach is the ‚ÄúEarth Observation Exploitation Platform Common Architecture‚Äù (EOEPCA) of ESA (\n\nhttps://eoepca.org). Leveraging the power of cloud computing, EO platforms provide a comprehensive ecosystem that seamlessly integrates storage, processing, analysis tools, collaboration, visualization, and data exploitation capabilities.\n\nThe following overview will explore each layer of the three-layered design and provide examples to illustrate the functionalities and benefits of cloud-based EO platforms in real-world applications. They showcase the diverse range of tools, services, and interfaces (also named ‚Äúbuilding blocks‚Äù) available to store, process, analyze, collaborate, visualize, and exploit EO data effectively within the cloud environment.","type":"content","url":"/lectures/intro-platform/intro-platform#components-of-platforms","position":19},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Infrastructure & Resource Tier","lvl2":"Components of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#infrastructure-resource-tier","position":20},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Infrastructure & Resource Tier","lvl2":"Components of platforms"},"content":"‚ÄúThe Resource Tier represents the hosting infrastructure and provides the EO data, storage and compute upon which the exploitation platform is deployed.‚Äù (Source: \n\nEOEPCA Master System Design)\n\nData Storage: The data storage component may include distributed file systems like distributed parallel file systems (e.g., GPFS, Hadoop) or object storage services (e.g., Amazon S3, Google Cloud Storage) to securely store and manage EO datasets.\n\nComputing Resources: The computing component can provide virtual machines (e.g., Amazon EC2, Google Compute Engine, OpenStack), container environments (e.g., Docker-Engine, Kubernetes) or batch-computing systems (e.g., High Performance Computing) for executing data processing and analysis tasks on EO datasets.","type":"content","url":"/lectures/intro-platform/intro-platform#infrastructure-resource-tier","position":21},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Platform Tier","lvl2":"Components of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#platform-tier","position":22},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Platform Tier","lvl2":"Components of platforms"},"content":"‚ÄúThe Platform Tier represents the Exploitation Platform and the services it offers to end-users.‚Äù (Source: \n\nEOEPCA Master System Design)\n\nThe services of the platform tier can be grouped into data-related and processing-related services. The processing tools and services often rely on the data services to get discover and get access to data available on the platform.\n\nData Services\n\nData Catalog: Data available on the platform needs to be described with metadata to be findable by users. Often processing and analysis services, such as Open Data Cube or OpenEO, make use of the data catalog to ease the use of EO data. These services enable users to annotate, search, and discover EO datasets based on various metadata parameters.\n\nData Access Service: This enables users to retrieve and access EO datasets. This may involve APIs, protocols, or data transfer mechanisms like Open Geospatial Consortium (OGC) Web Services or HTTP services for efficient and secure data access.\n\nData Visualization Service: The visualization component provides standardized web services for the visualization of raster and vector data available on the platform. User interfaces like QGIS or web mapping tools can be used together with those services.\n\nData Processing and Analysis Tools: This service component may include widely used  processing tools like \n\nGDAL (Geospatial Data Abstraction Library), remote sensing software like SNAP (Sentinel Application Platform), data cube related tools (e.g., \n\nxarray & \n\nDask) and APIs (e.g., \n\nOpenEO API) for performing advanced analysis on EO data.","type":"content","url":"/lectures/intro-platform/intro-platform#platform-tier","position":23},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Exploitation Tier","lvl2":"Components of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#exploitation-tier","position":24},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Exploitation Tier","lvl2":"Components of platforms"},"content":"‚ÄúThe Exploitation Tier represents the end-users who exploit the services of the platform to perform analysis, or using high-level applications built-in on top of the platform‚Äôs services.‚Äù (Source: \n\nEOEPCA Master System Design)\n\nUser Interfaces: The exploitation interface component may include web-based interfaces like web portals (e.g., \n\nCopernicus Browser), dashboards (e.g., \n\nEarth Observation Dashboard from NASA, ESA, JAXA or web development environments (e.g., \n\nJupyterLab). All of them provide interactive interfaces for users to explore and analyze EO data through a user-friendly interface.","type":"content","url":"/lectures/intro-platform/intro-platform#exploitation-tier","position":25},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Exercise: Build a platform","lvl2":"Components of platforms"},"type":"lvl3","url":"/lectures/intro-platform/intro-platform#exercise-build-a-platform","position":26},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl3":"Exercise: Build a platform","lvl2":"Components of platforms"},"content":"Now it is time for you: Please Drag and drop the building blocks of a platform into a correct diagram.","type":"content","url":"/lectures/intro-platform/intro-platform#exercise-build-a-platform","position":27},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/intro-platform/intro-platform#quiz","position":28},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Quiz"},"content":"What types of ‚Äúlayers‚Äù or ‚Äútiers‚Äù are there in a platform architecture?[[x]] Infrastructure & Resource Tier\n[[ ]] Software Tier\n[[x]] Platform Tier\n[[ ]] Data Cube Tier\n[[x]] Exploitation Tier\n\nWhat does an infrastructure provider offer?[[x]] Virtual Machines\n[[ ]] Data discovery\n[[ ]] Data cubes\n[[ ]] Data visualization\n[[x]] Data storage\n\nWhat kind of provider is the Euro Data Cube?[[ ]] Infrastructure provider\n[[x]] Platform provider\n\nWhen should you consider to use an EO cloud platform?[[x]] I have limited internet bandwidth for data downloading\n[[ ]] I have all my data locally on my own servers with lots of computing resources\n[[x]] I want to collaborate with other and external users\n[[x]] I want to easily make use of processing services\n[[x]] I don't want to care about system administration and operations\n[[ ]] I do not often have access to internet","type":"content","url":"/lectures/intro-platform/intro-platform#quiz","position":29},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Further reading"},"type":"lvl2","url":"/lectures/intro-platform/intro-platform#further-reading","position":30},{"hierarchy":{"lvl1":"Earth Observation cloud platforms","lvl2":"Further reading"},"content":"Earth Observation Cloud Platform Concept Development Study Report: \n\nhttps://‚Äãdocs‚Äã.ogc‚Äã.org‚Äã/per‚Äã/21‚Äã-023‚Äã.html\n\nBig Earth data: disruptive changes in Earth observation data management and analysis? \n\nSudmanns et al. (2019)\n\nEnabling the Big Earth Observation Data via Cloud Computing and DGGS: Opportunities and Challenges: \n\nhttps://‚Äãwww‚Äã.mdpi‚Äã.com‚Äã/2072‚Äã-4292‚Äã/12‚Äã/1‚Äã/62\n\nAn Overview of Platforms for Big Earth Observation Data Management and Analysis: \n\nhttps://‚Äãwww‚Äã.mdpi‚Äã.com‚Äã/2072‚Äã-4292‚Äã/12‚Äã/8‚Äã/1253\n\nThe openEO API‚ÄìHarmonising the Use of Earth Observation Cloud Services Using Virtual Data Cube Functionalities: \n\nhttps://‚Äãwww‚Äã.mdpi‚Äã.com‚Äã/2072‚Äã-4292‚Äã/13‚Äã/6‚Äã/1125\n\nESA Earth Observation Exploitation Platform Common Architecture (EOEPCA): \n\nhttps://eoepca.org","type":"content","url":"/lectures/intro-platform/intro-platform#further-reading","position":31},{"hierarchy":{"lvl1":"What is a data cube?"},"type":"lvl1","url":"/lectures/data-cube/data-cube","position":0},{"hierarchy":{"lvl1":"What is a data cube?"},"content":"","type":"content","url":"/lectures/data-cube/data-cube","position":1},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Learning objectives"},"type":"lvl2","url":"/lectures/data-cube/data-cube#learning-objectives","position":2},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Learning objectives"},"content":"Understand what data cubes are and why they are needed\n\nLearn what dimensions are\n\nUnderstand the role of data cubes in EO cloud platforms","type":"content","url":"/lectures/data-cube/data-cube#learning-objectives","position":3},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Data Cubes in General"},"type":"lvl2","url":"/lectures/data-cube/data-cube#data-cubes-in-general","position":4},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Data Cubes in General"},"content":"When you think about data, most likely tables come to your mind, with features organized in rows and columns. Data cubes overcome these constraints: they are a data structure that allows to represent data in more than only two dimensions. Generally a data cube is a multi-dimensional data structure. Even though it‚Äôs called a cube, it is n-dimensional. A 1-d data cube is an array. A 2-d data cube is a table. A 3-d data cube is a cube. A 4-d data cube is hard to visualize. A feature within a data cube is explained by multiple dimensions and has a certain value. An example would be: A company has sold multiple products (Shirts, Shoes, Pants), in different countries (Sweden, USA, Tunisia), in different years (2021, 2022, 2023).\n\nDimensions: Products, Locations, Years\n\nLabels of the Dimensions:\n\nProducts: Shrits, Shoes, Pants\n\nCountries: Sweden, USA, Tunisia\n\nYears: 2021, 2022, 2023\n\nFeature: Revenue\n\nValue: $\n\nThis concept can be applied to many fields such as economics, medicine, biology, and also very well to EO data!","type":"content","url":"/lectures/data-cube/data-cube#data-cubes-in-general","position":5},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Data Cubes in EO"},"type":"lvl2","url":"/lectures/data-cube/data-cube#data-cubes-in-eo","position":6},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Data Cubes in EO"},"content":"The concept of representing multidimensional data as data cubes fits ideally to the challenges of representing satellite data that is usually dealing with multiple dimensions such as: lat, lon, time, bands, etc. In the video below it becomes very clear what the strengths of data cubes in EO are.","type":"content","url":"/lectures/data-cube/data-cube#data-cubes-in-eo","position":7},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"An example data cube","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#an-example-data-cube","position":8},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"An example data cube","lvl2":"Data Cubes in EO"},"content":"Data can be represented as datacubes in EO, which are multi-dimensional arrays with additional information about their dimensionality. Datacubes can provide a nice and tidy interface for spatiotemporal data as well as for the operations you may want to execute on them. As they are arrays, it might be easiest to look at raster data as an example, even though datacubes can hold vector data as well. Our example data however consists of a 6x7 raster with 4 bands [blue, green, red, near-infrared] and 3 timesteps [2020-10-01, 2020-10-13, 2020-10-25], displayed here in an orderly, timeseries-like manner:\n\nFigure: An examplary raster datacube with 4 dimensions: x, y, bands and time. Reference: \n\nopeneo.org (2022). What are Data Cubes.\n\nIt is important to understand that datacubes are designed to make things easier for us, and are not literally a cube, meaning that the above plot is just as good a representation as any other. That is why we can switch the dimensions around and display them in whatever way we want, including the view below:\n\nFigure: Raster datacube flat representation: The 12 imagery tiles are now laid out flat as a 4 by 3 grid (bands by timesteps). All dimension labels are depicted (The timestamps, the band names and the x, y coordinates). Reference: \n\nopeneo.org (2022). What are Data Cubes.","type":"content","url":"/lectures/data-cube/data-cube#an-example-data-cube","position":9},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Dimensions","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#dimensions","position":10},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Dimensions","lvl2":"Data Cubes in EO"},"content":"A dimension refers to a certain axis of a datacube. This includes all variables (e.g. bands), which are represented as dimensions. Our exemplary raster datacube has the spatial dimensions x and y, and the temporal dimension t. Furthermore, it has a bands dimension, extending into the realm of what kind of information is contained in the cube.\n\nThe following properties are usually available for dimensions:\n\nname\n\ntype (potential types include: spatial (raster or vector data), temporal and other data such as bands)\n\naxis (for spatial dimensions) / number\n\nlabels (usually exposed through textual or numerical representations, in the metadata as nominal values and/or extents)\n\nreference system / projection\n\nresolution / step size\n\nunit (either explicitly specified or implicitly given by the reference system)\n\nadditional information specific to the dimension type (e.g. the geometry types for a dimension containing geometries)\n\nHere is an overview of the dimensions contained in our example raster datacube above:\n\n#\n\nname\n\ntype\n\nlabels\n\nresolution\n\nreference system\n\n1\n\nx\n\nspatial\n\n466380, 466580, 466780, 466980, 467180, 467380\n\n200m\n\nEPSG:32627\n\n2\n\ny\n\nspatial\n\n7167130, 7166930, 7166730, 7166530, 7166330, 7166130, 7165930\n\n200m\n\nEPSG:32627\n\n3\n\nbands\n\nbands\n\nblue, green, red, nir\n\n4 bands\n\n-\n\n4\n\nt\n\ntemporal\n\n2020-10-01, 2020-10-13, 2020-10-25\n\n12 days\n\nGregorian calendar / UTC\n\nTable: Overview of the dimensions contained in our example raster datacube above. Reference: \n\nopeneo.org (2022). What are Data Cubes.\n\nDimension labels are usually either numerical or text (also known as ‚Äústrings‚Äù), which also includes textual representations of timestamps or geometries for example.\nFor example, temporal labels are usually encoded as \n\nISO 8601 compatible dates and/or times and similarly geometries can be encoded as \n\nWell-known Text (WKT) or be represented by their IDs.\n\nDimensions with a natural/inherent order (usually all temporal and spatial raster dimensions) are always sorted. Dimensions without inherent order (usually bands), retain the order in which they have been defined in metadata or processes (e.g. through \n\nfilter_bands), with new labels simply being appended to the existing labels.","type":"content","url":"/lectures/data-cube/data-cube#dimensions","position":11},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Geometry as a Dimension","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#geometry-as-a-dimension","position":12},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Geometry as a Dimension","lvl2":"Data Cubes in EO"},"content":"A geometry dimension is not included in the example raster datacube above and it is not used in the following examples, but to show how a vector dimension with two polygons could look like:\n\nname\n\ntype\n\nlabels\n\nreference system\n\ngeometry\n\nvector\n\nPOLYGON((-122.4 37.6,-122.35 37.6,-122.35 37.64,-122.4 37.64,-122.4 37.6)), POLYGON((-122.51 37.5,-122.48 37.5,-122.48 37.52,-122.51 37.52,-122.51 37.5))\n\nEPSG:4326\n\nTable: Geometry as a Dimension. Reference: \n\nopeneo.org (2022). What are Data Cubes.\n\nA dimension with geometries can consist of points, linestrings, polygons, multi points, multi linestrings, or multi polygons.\nIt is not possible to mix geometry types, but the single geometry type with their corresponding multi type can be combined in a dimension (e.g. points and multi points).\n\nEO datacubes contain scalar values (e.g. strings, numbers or boolean values), with all other associated attributes stored in dimensions (e.g. coordinates or timestamps). Attributes such as the CRS or the sensor can also be turned into dimensions. Be advised that in such a case, the uniqueness of pixel coordinates may be affected. When usually, (x, y) refers to a unique location, that changes to (x, y, CRS) when (x, y) values are reused in other coordinate reference systems (e.g. two neighboring UTM zones).","type":"content","url":"/lectures/data-cube/data-cube#geometry-as-a-dimension","position":13},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Coordinate Reference System as a Dimension","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#coordinate-reference-system-as-a-dimension","position":14},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Coordinate Reference System as a Dimension","lvl2":"Data Cubes in EO"},"content":"In the example above, x and y dimension values have a unique relationship to world coordinates through their coordinate reference system (CRS). This implies that a single coordinate reference system is associated with these x and y dimensions. If we want to create a data cube from multiple tiles spanning different coordinate reference systems (e.g. Sentinel-2: different UTM zones), we would have to resample/warp those to a single coordinate reference system. In many cases, this is wanted because we want to be able to look at the result, meaning it is available in a single coordinate reference system.\n\nResampling is however costly, involves (some) data loss, and is in general not reversible. Suppose that we want to work only on the spectral and temporal dimensions of a data cube, and do not want to do any resampling. In that case, one could create one data cube for each coordinate reference system. An alternative would be to create one single data cube containing all tiles that has an additional dimension with the coordinate reference system. In that data cube, x and y no longer point to a unique world coordinate, because identical x and y coordinate pairs occur in each UTM zone. Now, only the combination (x, y, crs) has a unique relationship to the world coordinates.\n\nOn such a crs-dimensioned data cube, several operations make perfect sense, such as apply or reduce_dimension on spectral and/or temporal dimensions. A simple reduction over the crs dimension, using sum or mean would typically not make sense. The ‚Äúreduction‚Äù (removal) of the crs dimension that is meaningful involves the resampling/warping of all sub-cubes for the crs dimension to a single, common target coordinate reference system.","type":"content","url":"/lectures/data-cube/data-cube#coordinate-reference-system-as-a-dimension","position":15},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Resolution","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#resolution","position":16},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Resolution","lvl2":"Data Cubes in EO"},"content":"The resolution of a dimension gives information about what interval lies between observations. This is most obvious with the temporal resolution, where the intervals depict how often observations were made. Spatial resolution gives information about the pixel spacing, meaning how many ‚Äòreal world meters‚Äô are contained in a pixel. The number of bands and their wavelength intervals give information about the spectral resolution.\n\nImages generated by LexCube - \n\nLeipzig Explorer of Earth Data Cubes by \n\nMaximilian S√∂chting\n\n \n\nVideo content in cooperation with \n\nGunnar Brandt (Brockmann Consult), \n\nPontus Lurcock (Brockmann Consult) and \n\nMiguel Mahecha (University of Leipzig).","type":"content","url":"/lectures/data-cube/data-cube#resolution","position":17},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Data Cubes in Cloud Platforms","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#data-cubes-in-cloud-platforms","position":18},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Data Cubes in Cloud Platforms","lvl2":"Data Cubes in EO"},"content":"One important feature of an EO cloud platform is to host satellite data. Usually vast amounts of satellite data, for example the whole archive of Landsat and the Sentinels which adds up to Petabytes of data. Users want to access this data efficiently. Therefore the storage system of an EO cloud platform needs to be optimized towards user requests. Data Cubes play an important role here. Organizing the raw files into virtual data cubes solves many issues for EO cloud platforms.\n\nData Management: Different Satellites have different formats and are structured differently. This can easily create confusion because the different sources have to be treated differently. After organizing these files into data cubes the interaction with all different satellite data follows the same rules.\n\nSeparated Collections: Every satellite missions images form an own data cube, also called collection, with its according metadata. Ideally cloud processing tools on the platform allow for easy data fusion of different collections.\n\nFlexible Subsetting: \n\nData Cube Management Systems are developed to facilitate subsetting operations along the different dimensions and return only the required subset. Without worrying about the different raw files that are involved (e.g. area of interest is crossing a tile border).\n\n:warning: Remember, a data cube is not an EO cloud platform. It‚Äôs only one part of it.","type":"content","url":"/lectures/data-cube/data-cube#data-cubes-in-cloud-platforms","position":19},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Further Reading","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#further-reading","position":20},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"Further Reading","lvl2":"Data Cubes in EO"},"content":"Montero, D., Kraemer, G., Anghelea, A., Aybar, C., Brandt, G., Camps-Valls, G., ‚Ä¶ Mahecha, M. D. (2024). Earth System Data Cubes: Avenues for advancing Earth system research. Environmental Data Science, 3, e27. doi:10.1017/eds.2024.22\n\n\nPopular Data Cube Management Systems:\n\nOpen Data Cube\n\nRasdaman:\n\nGDAL Cubes","type":"content","url":"/lectures/data-cube/data-cube#further-reading","position":21},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"References","lvl2":"Data Cubes in EO"},"type":"lvl3","url":"/lectures/data-cube/data-cube#references","position":22},{"hierarchy":{"lvl1":"What is a data cube?","lvl3":"References","lvl2":"Data Cubes in EO"},"content":"openeo.org (2022). What are Data Cubes.\n\nESA (2018). Earth System Data Lab.\n\nMaximilian S√∂chting (2022). LexCube - Leipzig Explorer of Earth Data Cubes","type":"content","url":"/lectures/data-cube/data-cube#references","position":23},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/data-cube/data-cube#quiz","position":24},{"hierarchy":{"lvl1":"What is a data cube?","lvl2":"Quiz"},"content":"What is a two dimensional data cube?[( )] vector, a one dimensional array\n[( )] atomic feature, a single value\n[(x)] table, in EO a raster\n\nCheck the typical dimensions of an EO data cube.[[x]] x\n[[ ]] rain\n[[x]] y\n[[ ]] function\n[[x]] time\n[[ ]] world\n[[x]] bands\n\nWhat is the main puorpose of a geometry dimension?[(x)] storing geometries (e.g. polygons, points) \n[( )] it's holding the bounding box of the data cube \n[( )] defining the shape of the pixels\n\nAttach the resolutions to their dimensions\n\n[[x and y] [time] [band]\n\n[(X)       ( )    ( )  ]  meter, kilometer, degrees\n\n[( )       (X)    ( )  ]  days, hours, years\n\n[( )       ( )    (X)  ]  micrometers, color\n\nWhat are the advantages of data cubes in EO?[[x]] EO data is multidimensional data cubes allow to represent this\n[[ ]] Data cubes introduce an unnecessary layer of complexity that is not needed in EO\n[[ ]] The file size of EO data is drastically reduced\n\nWhat are the advantages of data cubes in cloud platforms?[[x]] The user is not confronted with multiple files and file types\n[[x]] Different satellite missions can be stored in different data cubes that can be combined for analysis on the platform\n[[x]] subsetting operations become very powerful - users only receive the extents they are interested in.","type":"content","url":"/lectures/data-cube/data-cube#quiz","position":25},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles"},"type":"lvl1","url":"/lectures/openscience/openscienceandfair","position":0},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles"},"content":"","type":"content","url":"/lectures/openscience/openscienceandfair","position":1},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/openscience/openscienceandfair#learning-objectives","position":2},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"Learning Objectives"},"content":"Understand what Open Science is and why we need it\n\nGet an overview of what belongs to Open Science\n\nGet to know the FAIR principles","type":"content","url":"/lectures/openscience/openscienceandfair#learning-objectives","position":3},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"Why do we need Open Science"},"type":"lvl2","url":"/lectures/openscience/openscienceandfair#why-do-we-need-open-science","position":4},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"Why do we need Open Science"},"content":" \n\nVideo content in collaboration with \n\nAnca Anghelea (ESA) and \n\nChelle Gentemann (NASA).\n\nSpace Agencies and International Organisations across the globe promote Open Science and support its practice by the scientific community through dedicated programmes. This course, for example, is part of the support to Earth Observation Open Science by the European Space Agency (ESA).\n\nESA, just like other European organisations such as the European Commission, has a long standing commitment to Open Science. A prominent example is the provision of full free and open Earth Observation data from its science missions (i.e. the Earth Explorers) and from the Copernicus Programme (e.g. the Sentinel missions). However, Open Science goes much beyond open data. ESA‚Äôs vision for Open Science covers the full research cycle (see ESA vision figure). Simply put, a piece of research fully adheres to Open Science principles if:\n\nall the data, code, and documentation of the respective research are FAIR (see \n\nWhat is FAIR? section), Open and linked to one another\n\nthe research is reproducible across various platforms (i.e. cloud platforms or computational systems)\n\nthe research is maintained so that it is accessible to the community in the long-term\n\nThis requires that the scientific community adheres to the same common (or compatible) practices when writing and documenting code, preparing and sharing data or publishing their research in journals.\n\nRole of Open Science in ESA‚Äôs Agenda 2025\n\nNASA Transform to Open Science (TOPS) within \n\nNASA‚Äôs Open-Source Science Initiative (OSSI)\n\nAmerican Geoscience Union‚Äôs vision of open science\n\n","type":"content","url":"/lectures/openscience/openscienceandfair#why-do-we-need-open-science","position":5},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"What is Open Science?"},"type":"lvl2","url":"/lectures/openscience/openscienceandfair#what-is-open-science","position":6},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"What is Open Science?"},"content":"","type":"content","url":"/lectures/openscience/openscienceandfair#what-is-open-science","position":7},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"Open Science","lvl2":"What is Open Science?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#open-science","position":8},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"Open Science","lvl2":"What is Open Science?"},"content":"Open Science offers new opportunities in dealing with scientific knowledge and represents a kind of cultural change. Through transparency and openness, Open Science increases the use and further development of knowledge as well as the potential for collaboration and the credibility of science. The focus is on free access to scientific processes and findings for everyone via the internet and the right to re-use this content. The beneficiaries of this concept are not only science, but also society and the economy.\n\nOpen Science is realised through various strategies and procedures, such as free access to scientific publications (Open Access), computer programmes (Open Source Software), data (Open Data) and educational materials (Open Educational Resources).\n\n","type":"content","url":"/lectures/openscience/openscienceandfair#open-science","position":9},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"The 4 Rs of Open Science","lvl2":"What is Open Science?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#the-4-rs-of-open-science","position":10},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"The 4 Rs of Open Science","lvl2":"What is Open Science?"},"content":"When opening your science think about the 4 Rs:\n\nReliable. It is important to evaluate the research in two ways. First with respect to scientific principles and criteria like validity, second with respect to criteria out of the professional context. This will help ensure that your results are more reliable.\n\nReproducible. Transparency is critical when doing research. Open Science allows you to clearly show what you‚Äôve done to get the results you have. By being open about your methods, processes and decision making during your research, someone else doing the research again should get the same results.\n\nReusable. By making research results reusable, you allow others to build upon the solid foundation your research has already created in a given subject. This is the same R that is also going to be mentioned in the FAIR principles.\n\nRelevant. Research quality describes the measurable influence of academic research on the academic community. Research impact includes environmental, cultural and societal impact, economic returns and societal benefits. By adhering to open science you increase the chances of your research to be relevant, because you give others the chance to interact with it.","type":"content","url":"/lectures/openscience/openscienceandfair#the-4-rs-of-open-science","position":11},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"The four pillars of Open Science","lvl2":"What is Open Science?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#the-four-pillars-of-open-science","position":12},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"The four pillars of Open Science","lvl2":"What is Open Science?"},"content":"The four pillars of Open Science are:\n\nData. Data-driven research is fast becoming the norm in all disciplines. To support validation of your findings and allow others to build upon your work, you first need to make sure that others can find your data. This means giving them persistent and unique identifiers (such as DOIs); assigning appropiate metadata so that others can find and reuse your data; putting them into a repository that supports public searching; and being clear about what others can and can‚Äôt do with them by applying an appropriate license. In the Further Reading section you find links to courses about managing and sharing research data and licensing your outputs.\n\nCode. When sharing your software and code, be sure to make use of open source standards to support interoperability and their longer-term viability. Be sure to put your code somewhere where others can search for it and access it (e.g., Github). Additionally, you can give your code a DOI by registering your Github repo on Zenodo. You should also be clear about the license the code is being shared under. In the Further Reading section there‚Äôs a course about Open Source Software.\n\nPapers. Open Access (OA) to publications is a key component of Open Science. Free and instant access to publications improves the speed of innovation and leads better cooperation and progress in solving grand challenges. To publish openly, you‚Äôll need to be able to source an appropriate OA journal or discipline-specific repository and navigate your way through their publishing agreements. You should also consider sharing preprints of your work as a means of getting early feedback and community validation of your approaches. In some cases, you‚Äôll need to pay an Article Processing Charge to publish in an OA journal.\n\nReviews. The peer review process is evolving. By making the peer review process more transparent, researchers have better access to peer feedback at an earlier stage in the lifecycle and consumers of research outputs can have greater confidence in their quality.\n\nBut there is way more to discover about Open Science. The Open \n\nScience Taxonomy graphic shows the different terms behind Open Science.","type":"content","url":"/lectures/openscience/openscienceandfair#the-four-pillars-of-open-science","position":13},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"Further Reading","lvl2":"What is Open Science?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#further-reading","position":14},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"Further Reading","lvl2":"What is Open Science?"},"content":"FOSTER Open Science (2022). Managing and Sharing Research Data.\n\nFOSTER Open Science (2022). Open Licensing.\n\nFOSTER Open Science (2022). Open Source Software and Workflows.\n\nESA Earth System Data Lab\n\nESA Open Science Persisten Demonstrator\n\nNASA Common Metadata Repository\n\nNASA VEDA Platform","type":"content","url":"/lectures/openscience/openscienceandfair#further-reading","position":15},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"References","lvl2":"What is Open Science?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#references","position":16},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"References","lvl2":"What is Open Science?"},"content":"F√ºrst, Elena, G√§nsdorfer, Nikos, Kalov√°, Tereza, Macher, Therese, Schranzhofer, Hermann, Stork, Christiane, & Th√∂richt, Heike (2022). Open Educational Resources Research Data Management. DOI: 10.5281/zenodo.6923397\n\nFOSTER Open Science (2022). What is Open Science.\n\nLoek Brinkman, Elly Dijk, Hans de Jonge, Nicole Loorbach, & Daan Rutten. (2023). Open Science: A Practical Guide for Early-Career Researchers (1.0). Zenodo. https://doi.org/10.5281/zenodo.7716153","type":"content","url":"/lectures/openscience/openscienceandfair#references","position":17},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"What is FAIR?"},"type":"lvl2","url":"/lectures/openscience/openscienceandfair#what-is-fair","position":18},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"What is FAIR?"},"content":"","type":"content","url":"/lectures/openscience/openscienceandfair#what-is-fair","position":19},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"FAIR","lvl2":"What is FAIR?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#fair","position":20},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"FAIR","lvl2":"What is FAIR?"},"content":"In 2014, a group of researchers as well as employees of libraries, archives, publishers and funders established principles for the handling of research data in a workshop and published them on FORCE11 for reviews and comments. The so-called FAIR principles were born. They comprise four goals: the findability, accessibility, interoperability and re-usability of data. With the achievement of these goals, the sustainable re-usability of research data is meant to be guaranteed. FAIR is not binary: FAIR is a spectrum!\n\nThe OpenAIRE definition of Metadata: Metadata is data providing information about data that makes findable, trackable and (re)usable. It can include information such as contact information, geographic locations, details about units of measure, abbreviations or codes used in the dataset, instrument and protocol information, survey tool details, provenance and version information and much more. In earth observation satellite data that could be the spatial and temporal extent of the data, the sensor, the bands and their wavelenghts, etc. Section \n\n2.1 Data Discovery and \n\n2.2 Data Properties deal with EO metadata in particular.\n\nFindable\n\nThe first step in (re)using data is to find them. Metadata and data should be easy to find for both humans and computers. Machine-readable metadata are essential for automatic discovery of datasets and services, so this is an essential component of the FAIRification process.\n\nF1. (Meta)data are assigned a globally unique and persistent identifier\n\nF2. Data are described with rich metadata (defined by R1 below)\n\nF3. Metadata clearly and explicitly include the identifier of the data they describe\n\nF4. (Meta)data are registered or indexed in a searchable resource\n\nPersistent Identifier (PID): These are IDs that identify a data set, publication or software (any digital object) unambiguosly and persistently with a single link. It increases the findability of a resource drastically. A widely used identifier is the \n\nDigital Object Identifier (DOI).\n\nAccessible\n\nOnce the users find the required data, they need to know how they can be accessed, possibly including authentication and authorisation.\n\nA1. (Meta)data are retrievable by their identifier using a standardised communications protocol\n\nA1.1 The protocol is open, free, and universally implementable\n\nA1.2 The protocol allows for an authentication and authorisation procedure, where necessary\n\nA2. Metadata are accessible, even when the data are no longer available\n\nInteroperable\n\nThe data usually need to be integrated with other data. In addition, the data need to interoperate with applications or workflows for analysis, storage, and processing.\n\nI1. (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\n\nI2. (Meta)data use standards, formats and vocabularies that follow FAIR principles and allow it to be exchanged and combined across computer systems\n\nI3. (Meta)data include qualified references to other (meta)data\n\nReusable\n\nThe ultimate goal of FAIR is to optimise the reuse of data. To achieve this, metadata and data should be well-described so that they can be replicated and/or combined in different settings\n\nR1. (Meta)data are richly described with a plurality of accurate and relevant attributes\n\nR1.1. (Meta)data are released with a clear and accessible data usage license\n\nR1.2. (Meta)data are associated with detailed provenance\n\nR1.3. (Meta)data meet domain-relevant community standards\n\nLicenses: To make your data reusable the use of appropiate licenses is key. Here is a good starting point to learn about the widely used \n\ncreative commons licenses.\n\n","type":"content","url":"/lectures/openscience/openscienceandfair#fair","position":21},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl4":"Animated Content: FAIR (drag and drop)","lvl3":"FAIR","lvl2":"What is FAIR?"},"type":"lvl4","url":"/lectures/openscience/openscienceandfair#animated-content-fair-drag-and-drop","position":22},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl4":"Animated Content: FAIR (drag and drop)","lvl3":"FAIR","lvl2":"What is FAIR?"},"content":"","type":"content","url":"/lectures/openscience/openscienceandfair#animated-content-fair-drag-and-drop","position":23},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"Further Reading","lvl2":"What is FAIR?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#further-reading-1","position":24},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"Further Reading","lvl2":"What is FAIR?"},"content":"Nature: A comment regarding FAIR Principles: \n\nWilkinson, M. D. et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Sci. Data 3:160018\n\nEuropean Commission: EC FAIR-Principles: \n\nEuropean Commission. Action Plan for FAIR data recommendations, \n\nThe EC expert group on FAIR data, \n\nEC/H2020 ‚Äì Guidelines on FAIR Data Management in Horizon 2020\n\nGO FAIR: GO FAIR is a bottom-up, stakeholder-driven and self-governed initiative that aims to implement the \n\nFAIR data principles, making data Findable, Accessible, Interoperable and Reusable (FAIR). It offers an open and inclusive ecosystem for individuals, institutions and organisations working together through \n\nImplementation Networks (INs). The INs are active in three activity pillars: \n\nGO CHANGE, \n\nGO TRAIN and \n\nGO BUILD.\n\nTrain-the-Trainer: Training material for ‚ÄúFAIR‚Äù in the train-the-trainer program for Research Data Management: Biernacka, et al. (2020). Train-the-Trainer Concept on Research Data Management (Version 3.0). Zenodo. \n\nBiernacka et al. (2020) (p. 38)\n\nOPENAIRE: A network of Open Access repositories, archives and journals that support Open Access policies. The OpenAIRE Consortium is a \n\nHorizon 2020 (FP8) project, aimed to support the implementation of the \n\nEC and \n\nERC Open Access policies. \n\nhttps://‚Äãwww‚Äã.openaire‚Äã.eu‚Äã/how‚Äã-to‚Äã-make‚Äã-your‚Äã-data‚Äã-fair\n\nCodata: FAIR-Principles and the Committee on Data for Science and Technology (Codata): The \n\nCommittee on Data for Science and Technology (CODATA) is a Paris-based organization with the aim of improving the quality, reliability and accessibility of interesting data from all fields of science and technology. \n\nHodson, S. (2018). Making FAIR data a reality‚Ä¶ and the challenges of interoperability and reusability. Open Science Conference 2018.\n\nFAIR Workflows: Also workflows can be made FAIR. This is a quite new topic. Here‚Äôs the \n\nworkflows communities approach\n\nHistory of FAIR: Information on the \n\nhistory of the FAIR principles","type":"content","url":"/lectures/openscience/openscienceandfair#further-reading-1","position":25},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"References","lvl2":"What is FAIR?"},"type":"lvl3","url":"/lectures/openscience/openscienceandfair#references-1","position":26},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl3":"References","lvl2":"What is FAIR?"},"content":"OpenAIRE (2017). What is Metadata.\n\nF√ºrst, Elena, G√§nsdorfer, Nikos, Kalov√°, Tereza, Macher, Therese, Schranzhofer, Hermann, Stork, Christiane, & Th√∂richt, Heike (2022). Open Educational Resources Research Data Management. DOI: 10.5281/zenodo.6923397\n\nGO FAIR (2023). FAIR Principles.\n\nCreative Commons (2023). About the licenses.","type":"content","url":"/lectures/openscience/openscienceandfair#references-1","position":27},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"Exam"},"type":"lvl2","url":"/lectures/openscience/openscienceandfair#exam","position":28},{"hierarchy":{"lvl1":"Open Science and the FAIR Principles","lvl2":"Exam"},"content":"Let‚Äôs test your theoretical knowledge on open science now. It‚Äôs important that you understand these concepts. We will apply them later on in the course!\n\nWhat do the 4 Rs in the context of Open Science stand for?[( )] Readable, refreshable, recognizable, and receivable\n[(X)] Reliable, reproducable, reusable, and relevant\n[( )] Recitable, renameable, replicatable, and repairable.\n\nWhich statement is correct about Open Science?[( )] Open Science sounds good, but it is unfair because researchers with limited financial means cannot afford access to open research results.\n[(X)] Open Science promotes the transparency of science and the free reuse of existing research results by everyone.\n[( )] Open Science ensures better networking of researchers within the EU. A worldwide exchange is currently not possible.\n\nWhat are the arguments in favour of Open Data?[( )] Generally, open data is not subject to a review process, so this type of data publication is always significantly faster.\n[( )] Lower costs and less time needed to prepare the data than with a closed access publication.\n[(X)] Accessibility of scientific data and metadata, source texts and digital reproductions.\n\nFAIR is an acronym that stands for...[(X)] Findable, Accessible, Interoperable and Reusable\n[( )] Fast Artificial Intelligence Research\n[( )] Fair, Accurate, Inclusive and Respectful Education\n[( )] Free, Available, Implemented and Ready\n\nFindable means...[[X]] (Meta)data are assigned a globally unique and persistent identifier\n[[ ]] (Meta)data are optimized to show up in the highest positions in google searches\n[[X]] (Meta)data are registered or indexed in a searchable resource\n\nAccessible means‚Ä¶[[ ]] (Meta)data must be open.\n[[X]] (Meta)data does not necessarily have to be open.\n[[X]] (Meta)data has access conditions and these are clear to both humans and machines.\n[[ ]] (Meta)data is open for access, but not for reuse.\n\nInteroperable means...[( )] (Meta)data is created and described by at least two researchers from differenct institutes\n[(X)] (Meta)data use standards, formats and vocabularies that allow it to be exchanged and combined across computer systems and between humans\n\nReusable means‚Ä¶[(X)] the data has clear usage licenses and is usable by both people and machines.\n[( )] that all data is usable.\n[( )] the data has clear usage licenses to be used by people.","type":"content","url":"/lectures/openscience/openscienceandfair#exam","position":29},{"hierarchy":{"lvl1":"Open Data and Open Source Software"},"type":"lvl1","url":"/lectures/openscience/opendataopensource","position":0},{"hierarchy":{"lvl1":"Open Data and Open Source Software"},"content":"","type":"content","url":"/lectures/openscience/opendataopensource","position":1},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/openscience/opendataopensource#learning-objectives","position":2},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"Learning Objectives"},"content":"Understand what Open Data is\n\nUnderstand what Open Source Software is","type":"content","url":"/lectures/openscience/opendataopensource#learning-objectives","position":3},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"What is Open Data?"},"type":"lvl2","url":"/lectures/openscience/opendataopensource#what-is-open-data","position":4},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"What is Open Data?"},"content":"","type":"content","url":"/lectures/openscience/opendataopensource#what-is-open-data","position":5},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Data","lvl2":"What is Open Data?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#open-data","position":6},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Data","lvl2":"What is Open Data?"},"content":"Open data is data that anyone can access, use and share. Open data becomes usable when made available in a common, machine-readable format. Open data must be licensed. Its licence must permit people to use the data in any way they want, including transforming, combining and sharing it with others, even commercially. Any restrictions imposed on the use of open data will limit its potential for creating new value.\n\nLimitations: For data to be open, it should have no limitations that prevent it from being used in any particular way. Anyone should be free to use, modify, combine and share the data, even commercially\n\nCost: Open data must be free to use, but this does not mean that it must be free to access. There is often a cost to creating, maintaining and publishing usable data. Ideally, any fee for accessing open data should be no more than the reasonable reproduction cost of the unit of data that is requested. This reproduction cost tends to be negligible for many datasets. Live data and big data can incur ongoing costs related to reliable service provision.\n\nReuse: Once the user has the data, they are free to use, reuse and redistribute it ‚Äì even commercially. Open data is measured by what it can be used for, not by how it is made available. Aspects like format, structure and machine readability all make data more usable, and should all be carefully considered. However, these do not make the data more open.\n\nFAIR vs Open Data: FAIR data is not the same as open data. For example, it is not always possible to grant free access to data for economic and legal reasons. Restrictions on access are compatible with FAIR principles, as long as the conditions and ways of access are evident.\n\n","type":"content","url":"/lectures/openscience/opendataopensource#open-data","position":7},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Data in this course","lvl2":"What is Open Data?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#open-data-in-this-course","position":8},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Data in this course","lvl2":"What is Open Data?"},"content":"The creation of this course would not be possible without Open Data. Here are just a few examples:\n\nThis course itself\n\nMost of the referenced learning resources e.g. \n\nOpen Science: A Practical Guide for Early-Career Researchers that we based the first part of this lesson on.\n\nESA Sentinel Satellite Data","type":"content","url":"/lectures/openscience/opendataopensource#open-data-in-this-course","position":9},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"References","lvl2":"What is Open Data?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#references","position":10},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"References","lvl2":"What is Open Data?"},"content":"European Commission (2022). What is open data?. https://data.europa.eu/elearning/en/module1/#/id/co-01 - \n\nThis work is licensed under a \n\nCreative Commons Attribution-ShareAlike 4.0 International License.\n\nOpen Knowledge Foundation (2022). Open Data Handbook.\n\nF√ºrst, Elena, G√§nsdorfer, Nikos, Kalov√°, Tereza, Macher, Therese, Schranzhofer, Hermann, Stork, Christiane, & Th√∂richt, Heike (2022). Open Educational Resources Research Data Management. DOI: 10.5281/zenodo.6923397\n\nSince we use material from the European Commissions [\n\ndata.europa.eu e-learning programme], which is published under the Creative Commons Attribution-ShareAlike 4.0 International License  we have to publish this section What is Open Data under \n\nThis work is licensed under a \n\nCreative Commons Attribution-ShareAlike 4.0 International License.","type":"content","url":"/lectures/openscience/opendataopensource#references","position":11},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"What is Open Source?"},"type":"lvl2","url":"/lectures/openscience/opendataopensource#what-is-open-source","position":12},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"What is Open Source?"},"content":"","type":"content","url":"/lectures/openscience/opendataopensource#what-is-open-source","position":13},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Source","lvl2":"What is Open Source?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#open-source","position":14},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Source","lvl2":"What is Open Source?"},"content":"Open Source does not simply mean that the source code of a project is available, which is only one element of an Open Source project. The Open Source Initiative (OSI) provides a commonly accepted \n\ndefinition of what constitutes Open Source. To summarize that, in order to be considered Open Source:\n\nOpen Source Software needs a license,\n\na work has to allow free redistribution,\n\nthe source code needs to be made available,\n\nit must be possible to create further works based on it,\n\nthere must be no limitations of who may use the work or for what purpose (so something like ‚Äúno commercial use‚Äù or ‚Äúno military use‚Äù won‚Äôt work with Open Source),\n\nthe work must not require an additional license on top of the one it comes with,\n\nand finally, the license must not depend on a specific distribution format, technology or presence of other works.\n\n","type":"content","url":"/lectures/openscience/opendataopensource#open-source","position":15},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Source Software used in this course","lvl2":"What is Open Source?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#open-source-software-used-in-this-course","position":16},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Open Source Software used in this course","lvl2":"What is Open Source?"},"content":"The creation of this course would not be possible without Open Source Software. Here are just a few examples of Open Source Software used in this course:\n\nPython, used in the coding exercises\n\nWordpress, powering EOCollege‚Äôs content\n\ngit, for versioning the content of this course and collaborating with colleagues\n\nopenEO, used in the coding exercises for standarized interaction with cloud platforms\n\nSTAC Spec, for standardizing metadata, so that we can find the data we need and create\n\nleaflet for the interactive visualization of results\n\nGDAL, powering most geospatial software and is the backbone of many EO cloud platforms","type":"content","url":"/lectures/openscience/opendataopensource#open-source-software-used-in-this-course","position":17},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Further Reading","lvl2":"What is Open Source?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#further-reading","position":18},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"Further Reading","lvl2":"What is Open Source?"},"content":"Help for understanding licenses and choosing the right Open Source license\n\ntl;drLegal (FOSSA) (2023). Software Licencses In Plain English.\n\nGitHub Inc. (2023) Choose an open source license.\n\nAnd plentiful resources on open source projects, how to contribute and incorporate them into your work\n\nopensource.com (2023). Open Source resources.","type":"content","url":"/lectures/openscience/opendataopensource#further-reading","position":19},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"References","lvl2":"What is Open Source?"},"type":"lvl3","url":"/lectures/openscience/opendataopensource#references-1","position":20},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl3":"References","lvl2":"What is Open Source?"},"content":"Open Source Initiative (2007). The Open Source Definition (v1.9).\n\nGina H√§u√üge (2022). A dev‚Äôs guide to open source software licensing. The ReadME Project.","type":"content","url":"/lectures/openscience/opendataopensource#references-1","position":21},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"Exam"},"type":"lvl2","url":"/lectures/openscience/opendataopensource#exam","position":22},{"hierarchy":{"lvl1":"Open Data and Open Source Software","lvl2":"Exam"},"content":"Which statement about Open Data is correct?[(X)] Open Data means information that is freely accessible.\n[( )] With Open Data, only data that is related to a scientific interpretation can be considered.\n[( )] With Open Data, the availability and usability of data on the web is limited.\n\nFAIR data is always open data.[( )] True\n[(X)] False\n\nWhat is true about Open Source Software projects[[X]] The source code is completely available to the public.\n[[ ]] You cannot contribute to Open Source Software Projects.\n[[X]] Open Source Software Projects are community driven.\n[[ ]] You can never use Open Source Software for commercial purposes.\n[[ ]] If software is published under a license, it is not open source.\n\nWhat is GitHub?[( )] GitHub is a cloud storage system specialized in storing big earth observation data sets.\n[(X)] GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere.\n\nFind the following GitHub repositories and copy their link into the text box. Copy the complete link starting with https://\n\nProject\n\nLink\n\nopenEO python client\n\n[[\n\nhttps://‚Äãgithub‚Äã.com‚Äã/Open‚Äã-EO‚Äã/openeo‚Äã-python‚Äã-client]]\n\nSpatio Temporal Asset Catalogue Specification (STAC Spec)\n\n[[\n\nhttps://‚Äãgithub‚Äã.com‚Äã/radiantearth‚Äã/stac‚Äã-spec]]\n\nGeographic Data Abstraction Library (GDAL)\n\n[[\n\nhttps://‚Äãgithub‚Äã.com‚Äã/OSGeo‚Äã/gdal]]","type":"content","url":"/lectures/openscience/opendataopensource#exam","position":23},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms"},"type":"lvl1","url":"/lectures/openscience/openscienceineo","position":0},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms"},"content":"","type":"content","url":"/lectures/openscience/openscienceineo","position":1},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/openscience/openscienceineo#learning-objectives","position":2},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"Learning Objectives"},"content":"Understand what Open Science is\n\nGet to know the FAIR concept\n\nFollow the steps of creating Open Science\n\nUnderstand the role of Open Science in geospatial, EO and EO cloud platforms","type":"content","url":"/lectures/openscience/openscienceineo#learning-objectives","position":3},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"The Open Science Journey"},"type":"lvl2","url":"/lectures/openscience/openscienceineo#the-open-science-journey","position":4},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"The Open Science Journey"},"content":"Finally let‚Äôs see how open science principles are applied in the field of geospatial, earth observation and EO cloud platforms. To begin we will have a look at the open science journey and a research project that has adapted openness and the FAIR principles very well. Then we will have a look at the role open science plays in today‚Äôs geospatial and EO world.\n\nThis drag and drop game asks you to connect the tasks to their respective step within the open science journey. If you hover over the icons, their description will pop up.","type":"content","url":"/lectures/openscience/openscienceineo#the-open-science-journey","position":5},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl3":"Open Science in the ClirSnow Project","lvl2":"The Open Science Journey"},"type":"lvl3","url":"/lectures/openscience/openscienceineo#open-science-in-the-clirsnow-project","position":6},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl3":"Open Science in the ClirSnow Project","lvl2":"The Open Science Journey"},"content":"The ClirSnow Project is a great example of how the concepts of opennes and FAIR are applied to a real world research project.\n\n \n\nVideo content in collaboration with \n\nMichael Matiu (University of Trento). \n‚ÄúIt seems like a lot of work the first time you do it. And it is. But once you know how to do it, you will use it in every research project, because it actually makes research so much easier. And, it will boost your research impact and credibility. It is really worth it.‚Äù","type":"content","url":"/lectures/openscience/openscienceineo#open-science-in-the-clirsnow-project","position":7},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"The Role of Open Source Software in Geospatial - The example of GDAL"},"type":"lvl2","url":"/lectures/openscience/openscienceineo#the-role-of-open-source-software-in-geospatial-the-example-of-gdal","position":8},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"The Role of Open Source Software in Geospatial - The example of GDAL"},"content":"Open Science plays an important role in geospatial. Open source software is a part of that and the Geographic Data Abstraction Library (GDAL) software is a great example of how important open source software is in the geospatial world.\nPaul Ramsey, the co-founder of the PostGIS extension, has described what GDAL is in a metaphoric way in a \n\nmapscaping.com podcast: ‚ÄúGDAL is data plumbing, a bit like an international electrical plug set for traveling ‚Äî it‚Äôs got multiple different shaped plugs. Electricity is ‚Äújust‚Äù electrons moving around. But they can move around as DC, AC, 120 volts or 240 volts. Plus, there are all these different ways you can plug and join electrical things. At the core, electricity is electrons vibrating, but it can be quite complex to get your hair dryer spinning.‚Äù\nHoward Butler, a director of the Open-Source Geospatial Foundation, said about the importance of GDAL:  ‚Äú[‚Ä¶] It‚Äôs open, it provides core functionality, I can‚Äôt understand how anybody gets anything done without it.‚Äú\n\n \n\nVideo content in collaboration with \n\nEven Rouault (Main Developer of GDAL).","type":"content","url":"/lectures/openscience/openscienceineo#the-role-of-open-source-software-in-geospatial-the-example-of-gdal","position":9},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"Open Science in EO Cloud Platforms"},"type":"lvl2","url":"/lectures/openscience/openscienceineo#open-science-in-eo-cloud-platforms","position":10},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"Open Science in EO Cloud Platforms"},"content":"Code: Workflows and Code can easily be shared on EO Cloud Platforms. There are openly available tutorial notebooks. Workflows can be shared as user defined processes and be reused by the community. There are user forums that share solutions and snippets. OpenEO, a standardized processing API for EO in the cloud, allows code to be portable between different cloud platforms. This increases reprodicibility, collaboration and prevents vendor locks.\n\nTo Do: Image Slider: openEO Platform Forum, Tutorial Notebooks Microsoft Planetary Computer, User Defined Processes openEO,\n\nResults: There are multiple ways to share results created in EO cloud platforms. Ideally they can be ingested into the platform and be made available as collections for other users directly upon creation. If the result comes with appropiate metadata (e.g. according to the STAC specification) they can easily be registered in publicly avialable STAC Catalogues. Cloud Native Data Formats (described in more detail in lesson \n\n2.4 Formats and Performance), like cloud optimized geotiff, are accessible via https requests. So instead of sharing a file, only a URL pointing to the file is shared.\n\nTo Do: Image Slider: Collection in a Platform, STAC Catalogue, Link to a COG\n\nPublication: If a publication is built on top of results produced in an EO cloud platform, the results and code can easily be linked to the publication in one of the forms described aboved. For example, you can publish your openEO process graph and link to it, and provide a link to a STAC Catalogue where the results are accessible.\n\nTo Do: Example of a Publication where the code is available on a cloud platform\n\nFAIRness:\n\nFindable: Data is usually presented through a data catalogue (e.g. STAC Catalogues are used in openEO platform and the Microsoft Planetary Computer) that is explicitly made for searching data. In many cases searching data works even without registration on the platform.\n\nAccessible: Data access in cloud platforms is usually granted after registration and authentication. Since cloud computing resources can easily be misused a certain degree of access control is necessary.\n\nInteroperable: Processing standards like openEO aim at making the code interoperable, which means it is transferable between platforms. Standardised metadata attached to the results,the use of cloud optimized formats and reingestion of the results into the platform guarantee easy uptake of the results right away. Different sources of satellite data are made interoperable by the cloud platform through the use of data cubes and processing on the fly - reprojections, regridding and temporal alignment are enabled on the fly.\n\nReusable: To make results reusable for others, they need to be accessible and have an open license. Ideally, a license of choice can be added to the metadata and the results are reingested into the platform as a public collection, available for everyone.\n\nAnalysis Ready Data (ARD): Analysis Ready Data are in the context of EO cloud platforms are usually satellite data that have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets. This means for example that atmospheric correction and cloud masking has already been applied to optical data. Many collections on cloud platforms are analysis ready, so that users can directly start the analysis withouth the tedious and technically demanding preprocessing steps. Since ‚Äòanalysis ready‚Äô can mean different things to different people, CEOS is working on standardizing what analysis ready data are.\n\nTo Do: Image of CEOS ARD","type":"content","url":"/lectures/openscience/openscienceineo#open-science-in-eo-cloud-platforms","position":11},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl3":"References","lvl2":"Open Science in EO Cloud Platforms"},"type":"lvl3","url":"/lectures/openscience/openscienceineo#references","position":12},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl3":"References","lvl2":"Open Science in EO Cloud Platforms"},"content":"ARDC Ltd. (2022). How to Make Your Data FAIR. DOI: 10.5281/zenodo.7426145.\n\nGO FAIR (2022). The FAIRification Process.\n\nMapscaping Podcast with Paul Ramsey (2021). GDAL - Geospatial Data Abstraction Library\n\nCEOS (2022). CEOS Analysis Ready Data.","type":"content","url":"/lectures/openscience/openscienceineo#references","position":13},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"Exam"},"type":"lvl2","url":"/lectures/openscience/openscienceineo#exam","position":14},{"hierarchy":{"lvl1":"The Open Science Journey - Open Science in geospatial, EO and EO cloud platforms","lvl2":"Exam"},"content":"Search a license from the \n\nCreative Commons License Chooser that adheres to the following: proper attribution/citation must be included when used, free for commercial use and Adaptions of the work can be shared, but only under the same or a compatible  license.[( )] CC BY 4.0\n[(X)] CC BY-SA 4.0\n[( )] CC BY-SA-ND 4.0\n\nWas the license you have just chosen a software license or a data license?[( )] Software License\n[(X)] Data License\n\nFind the Open Research Data Set ‚ÄúSnow cover in the European Alps: Station observations of snow depth and depth of snowfall‚Äù on the catalogue \n\nOpenAIRE.\n\nShare the DOI link to the data set version v1.3 in the format https://doi.org/10.5281/zenodo.XXXXXXX. Hint the last numbers are 74[[https://doi.org/10.5281/zenodo.5109574]]\n\nOn which repository is the data set registered?[( )] Integrated Ocean Observing System (https://ioos.noaa.gov)\n[( )] PANGAEA (https://pangaea.de)\n[(X)] ZENODO (https://zenodo.org)\n\nWhich license is used for the data set? Copy the URL to the license here.[(X)] Creative Commons Attribution 4.0 International\n[( )] Creative Commons Attribution-NonCommercial 4.0 International\n[( )] Creative Commons Attribution-ShareAlike 4.0 International\n\nFind the open access publication that is connected to the dataset. The one that has been published in ‚ÄúThe Cryosphere‚Äù. Copy the DOI of the article here in the format https://doi.org/XX.XXXX/tc-XX-XXXX-XXXX. Hint: The DOI ends with 21[[https://doi.org/10.5194/tc-15-1343-2021]]\n\nUnder which license is this course published. You can find this out on the courses GitHub page.[( )] Massachusets Institute of Technology License (MIT License)\n[( )] Creative Commons Attribution-ShareAlike 4.0 International License \n[(X)] Creative Commons Attribution 4.0 International License \n\nHow is data FAIR in a cloud platform? Connect the subjects to the FAIR keywords.[[Findable] [Accessible] [Interoperable] [Reusable]]\n[(X)        ( )          ( )             ( )       ]  STAC Metadata, Metadata Catalogue\n[( )        ( )          (X)             ( )       ]  Usage of abstract data cubes instead of different file formats\n[( )        (X)          ( )             ( )       ]  Authentication, Login, Free Trial Accounts\n[( )        ( )          ( )             (X)       ]  Data licenses attached to collections, provenance of the data is reported","type":"content","url":"/lectures/openscience/openscienceineo#exam","position":15},{"hierarchy":{"lvl1":"Data Discovery"},"type":"lvl1","url":"/lectures/data-discovery/data-discovery","position":0},{"hierarchy":{"lvl1":"Data Discovery"},"content":"","type":"content","url":"/lectures/data-discovery/data-discovery","position":1},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Learning Objectives"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#learning-objectives","position":2},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Learning Objectives"},"content":"Find out what Data Sources we have\n\nGet to know where to get data\n\nDiscover data catalogues\n\nLearn what STAC is","type":"content","url":"/lectures/data-discovery/data-discovery#learning-objectives","position":3},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"What kind of data is available"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#what-kind-of-data-is-available","position":4},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"What kind of data is available"},"content":" \n\nVideo content in collaboration with \n\nAngelos Tzotsos (President OSGeo) and \n\nTom Kralidis (Meteorological Service of Canada).","type":"content","url":"/lectures/data-discovery/data-discovery#what-kind-of-data-is-available","position":5},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Geospatial Data"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#geospatial-data","position":6},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Geospatial Data"},"content":"Knowledge of various geospatial data types is essential for understanding, representing, and analyzing acquired data from various domains. Data carry not only spatial information but also measured values, derived data, or other targeted information. Based on their properties, they can be used for different purposes.\n\nThere are several common geospatial data types, each serving a specific purpose and having distinct characteristics.\n\nFigure: Common data types in EO. Raster, Polygon, Point.","type":"content","url":"/lectures/data-discovery/data-discovery#geospatial-data","position":7},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Raster data","lvl3":"Geospatial Data"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#raster-data","position":8},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Raster data","lvl3":"Geospatial Data"},"content":"Raster data are represented as pixels and grid cells where every pixel has a value associated with it. Data stored in raster format can be discrete (representing distinct categories, eg. land cover types) or continuous (for example temperature or elevation).\n\nExamples of raster data sources:\n\nSatellite data  - Satellite imagery data are a common example of raster data. There are many types of data acquired by satellites and their processing. The temporal aspect of images is very important for continuous analysis.\n\nAerial/drone orthophoto - Digital imagery obtained by aircrafts (e.g. drones) is usually high-resolution raster data usable for detailed analysis.\n\nDEM - Digital Elevation Models are representing the surface of (usually) Earth and are essential for computations on a surface level.\n\nCloud mask - Cloud mask refers to the task of cloud detection in optical satellite data, where the common task is to detect (and remove) clouds to obtain better quality data.","type":"content","url":"/lectures/data-discovery/data-discovery#raster-data","position":9},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Vector data","lvl3":"Geospatial Data"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#vector-data","position":10},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Vector data","lvl3":"Geospatial Data"},"content":"Vector data are a fundamental type of geospatial data. Vector data represents spatial features using 3 basic classes -  points, lines, and polygons and the attributes associated with them. With vector data, it is possible to capture and analyze complex spatial patterns and attributes with great accuracy, providing valuable insights into spatial phenomena and supporting decision-making processes. Vector data are commonly used in addition to raster data to highlight or accompany trends and results.\n\nExamples of vector data sources:\n\nBuilding footprints (Polygons) - Building footprints are usually represented as polygons and capture the perimeter of buildings on the Earth‚Äôs surface. These footprints can vary in complexity from simple rectangles for standard buildings to more intricate shapes that outline the exact contours of complex structures.\n\nStreet networks - Street networks are depicted as lines, representing the pathways through which vehicles and pedestrians can move. Each line segment can indicate a portion of a street, alley, highway, or path, connecting intersections and endpoints to map out the entire transportation infrastructure of an area.\n\nPoints of interest - Points of Interest (POIs) are example of point vector data. They represents specific locations that hold importance or interest within a geographic area or usecase. These can include landmarks, businesses, public facilities, and natural features, among others.","type":"content","url":"/lectures/data-discovery/data-discovery#vector-data","position":11},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"In-situ data","lvl3":"Geospatial Data"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#in-situ-data","position":12},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"In-situ data","lvl3":"Geospatial Data"},"content":"In-situ data are data collected directly at the place of interest and are connected to a special location. There are many reasons why these data are important and one of them is an enhancement of data collected by other means eg. weather measurements from satellites enhanced with local air quality measurements. There are countless examples of what can be measured, two common examples are:\n\nAir temperature\n\nSoil quality","type":"content","url":"/lectures/data-discovery/data-discovery#in-situ-data","position":13},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Datacubes","lvl3":"Geospatial Data"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#datacubes","position":14},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Datacubes","lvl3":"Geospatial Data"},"content":"Datacube refers to a multidimensional representation of data that incorporates spatial and temporal dimensions. It is a concept used to organize and analyze large volumes of geospatial information in a structured and efficient manner. Datacubes are combining the data types discussed above into one data structure. More about datacubes is in \n\nLesson 1.2","type":"content","url":"/lectures/data-discovery/data-discovery#datacubes","position":15},{"hierarchy":{"lvl1":"Data Discovery","lvl2":"EO Catalog protocols"},"type":"lvl2","url":"/lectures/data-discovery/data-discovery#eo-catalog-protocols","position":16},{"hierarchy":{"lvl1":"Data Discovery","lvl2":"EO Catalog protocols"},"content":"For Earth observation data discovery, it is essential to know about various used catalog protocols that define standardized methods and formats for organizing, describing, and accessing Earth observation data catalogs.\n\nSome commonly used protocols shared between diffent platforms include OpenSearch, OGC CSW, OGC API - Features, OGC API - Records, STAC, OData and more.","type":"content","url":"/lectures/data-discovery/data-discovery#eo-catalog-protocols","position":17},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OpenSearch","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#opensearch","position":18},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OpenSearch","lvl2":"EO Catalog protocols"},"content":"The OpenSearch specification was launched in 2005 by \n\nA9.com, an Amazon subsidiary, as a means for sharing search queries and search results in a standardized format.\nThe specification was intended to allow syndication of search results that could then be aggregated by one large index.\nOpenSearch provides a simple to use description of the search interface, which is called an OpenSearch Description document (OSDD).\nA client (e.g., a browser) can use this description to check which response formats are supported and how a query/filter can be formulated.\nThe OpenSearch based REST services are usually offered by existing EO data platforms for compatibility reasons as the protocol itself is stable and not extended anymore.\nThe data models of most catalogs build on top of XML or GeoJSON and allow filtering on a set of simple but not standardized properties. The protocol supports both textual and geospatial search and filtering capabilities, making it suitable for a wide range of applications, including web search engines and geospatial data catalogs.","type":"content","url":"/lectures/data-discovery/data-discovery#opensearch","position":19},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OGC CSW","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#ogc-csw","position":20},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OGC CSW","lvl2":"EO Catalog protocols"},"content":"Catalogue Service for the Web (CSW) standardized by Open Geospatial Consortium (OGC), offers a framework for publishing, discovering, and accessing metadata records, allowing users to effectively search and retrieve geospatial data and related information. The catalogue is made up of records that describe geospatial data, linked geospatial services and related resources.\nCSW enables metadata query using metadata core (mandatory) elements.\nCatalogue services support the use of one of several identified query languages to find and return results using well-known content models (metadata schemas) and encodings.","type":"content","url":"/lectures/data-discovery/data-discovery#ogc-csw","position":21},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OGC API - Features","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#ogc-api-features","position":22},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OGC API - Features","lvl2":"EO Catalog protocols"},"content":"OGC API - Features is a modern and flexible geospatial data access protocol developed by the OGC. It provides a standardized and RESTful approach for querying, retrieving, and manipulating geospatial feature data over the web. By leveraging the power of web technologies such as HTTP, JSON, and GeoJSON, OGC API - Features simplifies the process of accessing and working with geospatial data. It allows users to retrieve specific features based on spatial and attribute filters, perform spatial and attribute queries, and even modify feature data through standard HTTP methods.","type":"content","url":"/lectures/data-discovery/data-discovery#ogc-api-features","position":23},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OGC API - Records","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#ogc-api-records","position":24},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"OGC API - Records","lvl2":"EO Catalog protocols"},"content":"OGC API - Records is a multi-part draft specification (built on top of OGC API - Features) that offers the capability to create, modify, and query metadata on the Web. The draft specification enables the discovery of geospatial resources by standardizing the way collections of descriptive information about the resources (metadata) are exposed. The specification also enables the discovery and sharing of related resources that may be referenced from geospatial resources or their metadata by standardizing the way all kinds of records are exposed and managed.","type":"content","url":"/lectures/data-discovery/data-discovery#ogc-api-records","position":25},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#stac","position":26},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"STAC stands for SpatioTemporal Asset Catalog. It is a community specification that provides a common way for describing and cataloging assets that have a connection to space and time, usually but not necessarily on the Earth. The STAC specification focuses on organizing and sharing geospatial data in a way that is accessible, interoperable, and scalable. The STAC Specification consists of 4 semi-independent specifications (Catalog, Collection, Item and API) which can work independently or be used together. All of them can be and are enriched by a variety of extensions.\nIt is a relatively new specification but increasingly integrated by various data providers and seen as future of EO Data cataloguing and discovery. The data model in the dataspace is still evolving to comply fully with all standardized properties. Because of that, more attention is provided to STAC than other catalogue protocols in this tutorial.","type":"content","url":"/lectures/data-discovery/data-discovery#stac","position":27},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#the-components-of-stac","position":28},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"The STAC specification is divided into three main parts:\n\nSTAC specification for static catalogs, which consists of three parts:\n\nSTAC Items\n\nSTAC Catalogs\n\nSTAC Collections\n\nSTAC API specification for dynamic catalogs.\n\nSTAC extensions (both for static STAC and the STAC API)\n\nAll these components are fairly independent, but all components work together and use links to express the relationship between them so that eventually clients can traverse through them. The links to the actual spatio-temporal data files that the STAC metadata describes are handled specifically and are called STAC Assets. Assets can be made available in Items and Collections.\n\n \n\nImage by Matthias Mohr from \n\nhttps://‚Äãmohr‚Äã.ws‚Äã/foss4g/","type":"content","url":"/lectures/data-discovery/data-discovery#the-components-of-stac","position":29},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Item","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl5","url":"/lectures/data-discovery/data-discovery#stac-item","position":30},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Item","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"A STAC Item is the foundational building block of STAC. It is a GeoJSON feature supplemented with additional metadata that enables clients to traverse through catalogs. Since an item is a GeoJSON, it can be easily read by any modern GIS or geospatial library. One item can describe one or more SpatioTemporal Asset(s). For example, a common practice of using STAC for imagery is that each band in a scene is its own STAC Asset and there is one STAC Item to represent all the bands in a single scene.\n\nThe STAC Item JSON specification uses standard GeoJSON fields as well as a few additional informational fields to describe the asset(s) more thoroughly.\n\nSTAC Item (and other components) have some required fields which must be always filled with information. In the example below, required fields like type, stac_version or id are filled. Properties are also required fields, but here also extended by many STAC extensions, in the format of extension_name:field_name: value. STAC extensions are also listed in the stac_extensions field. Complete STAC Item spec can be found on \n\nGitHub.\n\nExample of a Sentinel 2 L2A STAC Item with one band asset.{\n   \"type\":\"Feature\",\n   \"stac_version\":\"1.0.0\",\n   \"id\":\"S2B_43SCR_20231123_0_L2A\",\n   \"properties\":{\n      \"created\":\"2023-11-23T08:25:34.597Z\",\n      \"platform\":\"sentinel-2b\",\n      \"constellation\":\"sentinel-2\",\n      \"instruments\":[\n         \"msi\"\n      ],\n      \"eo:cloud_cover\":0.414053,\n      \"proj:epsg\":32643,\n      \"mgrs:utm_zone\":43,\n      \"mgrs:latitude_band\":\"S\",\n      \"mgrs:grid_square\":\"CR\",\n      \"grid:code\":\"MGRS-43SCR\",\n      \"view:sun_azimuth\":161.976971280106,\n      \"view:sun_elevation\":35.5947611411469,\n      \"s2:degraded_msi_data_percentage\":0,\n      \"s2:nodata_pixel_percentage\":77.337396,\n      \"s2:saturated_defective_pixel_percentage\":0,\n      \"s2:dark_features_percentage\":0,\n      \"s2:cloud_shadow_percentage\":0.002591,\n      \"s2:vegetation_percentage\":0.010072,\n      \"s2:not_vegetated_percentage\":98.205149,\n      \"s2:water_percentage\":0.9589,\n      \"s2:unclassified_percentage\":0.38101,\n      \"s2:medium_proba_clouds_percentage\":0.410817,\n      \"s2:high_proba_clouds_percentage\":0.003235,\n      \"s2:thin_cirrus_percentage\":0,\n      \"s2:snow_ice_percentage\":0.028226,\n      \"s2:product_type\":\"S2MSI2A\",\n      \"s2:processing_baseline\":\"05.09\",\n      \"s2:product_uri\":\"S2B_MSIL2A_20231123T054139_N0509_R005_T43SCR_20231123T070647.SAFE\",\n      \"s2:generation_time\":\"2023-11-23T07:06:47.000000Z\",\n      \"s2:datatake_id\":\"GS2B_20231123T054139_035066_N05.09\",\n      \"s2:datatake_type\":\"INS-NOBS\",\n      \"s2:datastrip_id\":\"S2B_OPER_MSI_L2A_DS_2BPS_20231123T070647_S20231123T054133_N05.09\",\n      \"s2:granule_id\":\"S2B_OPER_MSI_L2A_TL_2BPS_20231123T070647_A035066_T43SCR_N05.09\",\n      \"s2:reflectance_conversion_factor\":1.02412472897181,\n      \"datetime\":\"2023-11-23T05:50:10.118000Z\",\n      \"s2:sequence\":\"0\",\n      \"earthsearch:s3_path\":\"s3://sentinel-cogs/sentinel-s2-l2a-cogs/43/S/CR/2023/11/S2B_43SCR_20231123_0_L2A\",\n      \"earthsearch:payload_id\":\"roda-sentinel2/workflow-sentinel2-to-stac/ee5536069c6dd3a13ae2dbd530beafeb\",\n      \"earthsearch:boa_offset_applied\":true,\n      \"processing:software\":{\n         \"sentinel2-to-stac\":\"0.1.1\"\n      },\n      \"updated\":\"2023-11-23T08:25:34.597Z\"\n   },\n   \"geometry\":{\n      \"type\":\"Polygon\",\n      \"coordinates\":[\n         [\n            [\n               73.91301155493866,\n               32.53264844646899\n            ],\n            [\n               73.65302362425264,\n               31.539681428386213\n            ],\n            [\n               74.04974127686076,\n               31.543245006457585\n            ],\n            [\n               74.03945448697993,\n               32.53367782512506\n            ],\n            [\n               73.91301155493866,\n               32.53264844646899\n            ]\n         ]\n      ]\n   },\n   \"links\":[\n      {\n         \"rel\":\"self\",\n         \"type\":\"application/geo+json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/items/S2B_43SCR_20231123_0_L2A\"\n      },\n      {\n         \"rel\":\"canonical\",\n         \"href\":\"s3://sentinel-cogs/sentinel-s2-l2a-cogs/43/S/CR/2023/11/S2B_43SCR_20231123_0_L2A/S2B_43SCR_20231123_0_L2A.json\",\n         \"type\":\"application/json\"\n      },\n      {\n         \"rel\":\"license\",\n         \"href\":\"https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice\"\n      },\n      {\n         \"rel\":\"derived_from\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l1c/items/S2B_43SCR_20231123_0_L1C\",\n         \"type\":\"application/geo+json\"\n      },\n      {\n         \"rel\":\"parent\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n      },\n      {\n         \"rel\":\"collection\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n      },\n      {\n         \"rel\":\"root\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1\"\n      },\n      {\n         \"rel\":\"thumbnail\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/items/S2B_43SCR_20231123_0_L2A/thumbnail\"\n      }\n   ],\n   \"assets\":{\n      \"blue\":{\n         \"href\":\"https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/43/S/CR/2023/11/S2B_43SCR_20231123_0_L2A/B02.tif\",\n         \"type\":\"image/tiff; application=geotiff; profile=cloud-optimized\",\n         \"title\":\"Blue (band 2) - 10m\",\n         \"eo:bands\":[\n            {\n               \"name\":\"blue\",\n               \"common_name\":\"blue\",\n               \"description\":\"Blue (band 2)\",\n               \"center_wavelength\":0.49,\n               \"full_width_half_max\":0.098\n            }\n         ],\n         \"gsd\":10,\n         \"proj:shape\":[\n            10980,\n            10980\n         ],\n         \"proj:transform\":[\n            10,\n            0,\n            300000,\n            0,\n            -10,\n            3600000\n         ],\n         \"raster:bands\":[\n            {\n               \"nodata\":0,\n               \"data_type\":\"uint16\",\n               \"bits_per_sample\":15,\n               \"spatial_resolution\":10,\n               \"scale\":0.0001,\n               \"offset\":-0.1\n            }\n         ],\n         \"roles\":[\n            \"data\",\n            \"reflectance\"\n         ]\n      },\n      \"thumbnail\":{\n         \"href\":\"https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/43/S/CR/2023/11/S2B_43SCR_20231123_0_L2A/thumbnail.jpg\",\n         \"type\":\"image/jpeg\",\n         \"title\":\"Thumbnail image\",\n         \"roles\":[\n            \"thumbnail\"\n         ]\n      }\n   },\n   \"bbox\":[\n      73.65302362425264,\n      31.539681428386213,\n      74.04974127686076,\n      32.53367782512506\n   ],\n   \"stac_extensions\":[\n      \"https://stac-extensions.github.io/view/v1.0.0/schema.json\",\n      \"https://stac-extensions.github.io/grid/v1.0.0/schema.json\",\n      \"https://stac-extensions.github.io/mgrs/v1.0.0/schema.json\",\n      \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\",\n      \"https://stac-extensions.github.io/processing/v1.1.0/schema.json\",\n      \"https://stac-extensions.github.io/eo/v1.1.0/schema.json\",\n      \"https://stac-extensions.github.io/projection/v1.1.0/schema.json\"\n   ],\n   \"collection\":\"sentinel-2-l2a\"\n}\n\nSource URL: \n\nhttps://‚Äãearth‚Äã-search‚Äã.aws‚Äã.element84‚Äã.com‚Äã/v1‚Äã/collections‚Äã/sentinel‚Äã-2‚Äã-l2a‚Äã/items‚Äã/S2B‚Äã_43SCR‚Äã_20231123‚Äã_0‚Äã_L2A","type":"content","url":"/lectures/data-discovery/data-discovery#stac-item","position":31},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Catalog","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl5","url":"/lectures/data-discovery/data-discovery#stac-catalog","position":32},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Catalog","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"A STAC Catalog is an entity that logically groups other Catalogs, Collections, and Items. A Catalog contains links to these other entities and can include additional metadata to describe the entities contained therein. A catalog is usually the starting point for navigating a STAC. More specifically, a catalog.json file contains links to some combination of other STAC Catalogs, Collections, and/or Items. We can think of it like a directory on a computer although it doesn‚Äôt necessarily need to mirror the local directory tree.\n\nThere are no restrictions on the way STAC Catalogs are organized. Therefore, the combination of STAC components within a STAC Catalog is quite variable and flexible. Many implementations use a set of ‚Äòsub-catalog(s)‚Äô that group the items in some sensible way, e.g. by years as a first level and months as a second level. It can be easily extended, for example, to include additional metadata to further describe its holdings, as the STAC Collection does.","type":"content","url":"/lectures/data-discovery/data-discovery#stac-catalog","position":33},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Collection","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl5","url":"/lectures/data-discovery/data-discovery#stac-collection","position":34},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Collection","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"A STAC Collection is similar to a STAC Catalog, but includes and partially requires additional metadata about a set of items that exist as part of the collection. It adds additional fields to enable the description of information like the spatial and temporal extent of the data, the license, keywords, providers, etc. Therefore, it can easily be extended with additional collection-level metadata that is common across all children. For example, it could summarize that all Items underneath hold data in either 10m or 30m spatial resolution.\n\nExample of a Sentinel 2 L2A STAC Collection.{\n   \"type\":\"Collection\",\n   \"id\":\"sentinel-2-l2a\",\n   \"stac_version\":\"1.0.0\",\n   \"description\":\"Global Sentinel-2 data from the Multispectral Instrument (MSI) onboard Sentinel-2\",\n   \"links\":[\n      {\n         \"rel\":\"self\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n      },\n      {\n         \"rel\":\"cite-as\",\n         \"href\":\"https://doi.org/10.5270/S2_-742ikth\",\n         \"title\":\"Copernicus Sentinel-2 MSI Level-2A (L2A) Bottom-of-Atmosphere Radiance\"\n      },\n      {\n         \"rel\":\"license\",\n         \"href\":\"https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice\",\n         \"title\":\"proprietary\"\n      },\n      {\n         \"rel\":\"parent\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1\"\n      },\n      {\n         \"rel\":\"root\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1\"\n      },\n      {\n         \"rel\":\"items\",\n         \"type\":\"application/geo+json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/items\"\n      },\n      {\n         \"rel\":\"http://www.opengis.net/def/rel/ogc/1.0/queryables\",\n         \"type\":\"application/schema+json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/queryables\"\n      },\n      {\n         \"rel\":\"aggregate\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/aggregate\",\n         \"method\":\"GET\"\n      },\n      {\n         \"rel\":\"aggregations\",\n         \"type\":\"application/json\",\n         \"href\":\"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a/aggregations\"\n      }\n   ],\n   \"stac_extensions\":[\n      \"https://stac-extensions.github.io/item-assets/v1.0.0/schema.json\",\n      \"https://stac-extensions.github.io/view/v1.0.0/schema.json\",\n      \"https://stac-extensions.github.io/scientific/v1.0.0/schema.json\",\n      \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\",\n      \"https://stac-extensions.github.io/eo/v1.0.0/schema.json\"\n   ],\n   \"title\":\"Sentinel-2 Level 2A\",\n   \"extent\":{\n      \"spatial\":{\n         \"bbox\":[\n            [\n               -180,\n               -90,\n               180,\n               90\n            ]\n         ]\n      },\n      \"temporal\":{\n         \"interval\":[\n            [\n               \"2015-06-27T10:25:31.456000Z\",\n               null\n            ]\n         ]\n      }\n   },\n   \"license\":\"proprietary\",\n   \"keywords\":[\n      \"sentinel\",\n      \"earth observation\",\n      \"esa\"\n   ],\n   \"providers\":[\n      {\n         \"name\":\"ESA\",\n         \"roles\":[\n            \"producer\"\n         ],\n         \"url\":\"https://earth.esa.int/web/guest/home\"\n      },\n      {\n         \"name\":\"Sinergise\",\n         \"roles\":[\n            \"processor\"\n         ],\n         \"url\":\"https://registry.opendata.aws/sentinel-2/\"\n      },\n      {\n         \"name\":\"AWS\",\n         \"roles\":[\n            \"host\"\n         ],\n         \"url\":\"http://sentinel-pds.s3-website.eu-central-1.amazonaws.com/\"\n      },\n      {\n         \"name\":\"Element 84\",\n         \"roles\":[\n            \"processor\"\n         ],\n         \"url\":\"https://element84.com\"\n      }\n   ],\n   \"summaries\":{\n      \"platform\":[\n         \"sentinel-2a\",\n         \"sentinel-2b\"\n      ],\n      \"constellation\":[\n         \"sentinel-2\"\n      ],\n      \"instruments\":[\n         \"msi\"\n      ],\n      \"gsd\":[\n         10,\n         20,\n         60\n      ],\n      \"view:off_nadir\":[\n         0\n      ],\n      \"sci:doi\":[\n         \"10.5270/s2_-znk9xsj\"\n      ],\n      \"eo:bands\":[\n         {\n            \"name\":\"blue\",\n            \"common_name\":\"blue\",\n            \"description\":\"Blue (band 2)\",\n            \"center_wavelength\":0.49,\n            \"full_width_half_max\":0.098\n         }\n      ]\n   }\n}\n\nSource URL: \n\nhttps://‚Äãearth‚Äã-search‚Äã.aws‚Äã.element84‚Äã.com‚Äã/v1‚Äã/collections‚Äã/sentinel‚Äã-2‚Äã-l2a","type":"content","url":"/lectures/data-discovery/data-discovery#stac-collection","position":35},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC API","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl5","url":"/lectures/data-discovery/data-discovery#stac-api","position":36},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC API","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"STAC API is a dynamic version of a static SpatioTemporal Asset Catalog and provides a RESTful endpoint that enables the search of STAC Items and STAC Collections. STAC Catalogs don‚Äôt play a big role in APIs as they are mostly used as an entity for grouping larger static catalogs into smaller chunks, which is usually not needed in the context of a dynamic API.\n\nIf the API implements the Filter or Query extension, additionally the user is allowed to search for specific content based on a set of available metadata fields. Additional extensions may support more interactive elements such as aggregations, or managing the metadata (updating it, creating new entities, or deleting some) through transactions.\n\nA part of the STAC API is built on top of \n\nOGC API - Features.","type":"content","url":"/lectures/data-discovery/data-discovery#stac-api","position":37},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Extension","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"type":"lvl5","url":"/lectures/data-discovery/data-discovery#stac-extension","position":38},{"hierarchy":{"lvl1":"Data Discovery","lvl5":"STAC Extension","lvl4":"The components of STAC","lvl3":"STAC","lvl2":"EO Catalog protocols"},"content":"Extensions to STAC are split into two parts: STAC extensions and STAC API extensions. They are both an important addition to the STAC specifications and can provide either additions to the data model (i.e. additional JSON properties such as eo:cloud_cover) or behavioural changes (e.g. additional types of links or a sorting functionality). Most tend to be about describing a particular domain or type of data.\n\nTo find out which extensions do the STAC API, STAC Catalog, Collection or Item object implement, you can explore \n\nlist of STAC extensions or \n\nlist of STAC API extensions.","type":"content","url":"/lectures/data-discovery/data-discovery#stac-extension","position":39},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"ODAta","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#odata","position":40},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"ODAta","lvl2":"EO Catalog protocols"},"content":"OData (Open Data Protocol) specifies a variety of best practices for creating and using REST APIs that can be handled by a large set of client tools like common web browsers, download-managers. The OData protocol can be used for building URI for performing search queries and product downloads for example on the Copernicus Dataspace.","type":"content","url":"/lectures/data-discovery/data-discovery#odata","position":41},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"References and further readings","lvl3":"ODAta","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#references-and-further-readings","position":42},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"References and further readings","lvl3":"ODAta","lvl2":"EO Catalog protocols"},"content":"OpenSearch specification\n\nOGC OpenSearch Geo, Time and EO extensions\n\nOpenSearch API access examples\n\nOGC CSW specification\n\nOGC API - Features\n\nOGC API - Records (Draft)\n\nMore to read about \n\nSTAC specification\n\nSTAC Tutorials\n\nOData on Copernicus Dataspace","type":"content","url":"/lectures/data-discovery/data-discovery#references-and-further-readings","position":43},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Where to search for data","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#where-to-search-for-data","position":44},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Where to search for data","lvl2":"EO Catalog protocols"},"content":"Earth observation data access is not limited to a single platform or a single entry point. What follows is a non-exhaustive list of some well-known Earth observation data catalogues usually based around the original agency providing the data:\n\nEarthData Search: A comprehensive data discovery and access tool provided by NASA‚Äôs Earth Observing System Data and Information System (EOSDIS). Contains wide range of NASA‚Äôs Earth science data.\n\nCopernicus Data Space: Currently in development platform aiming to provide immediate access to large amounts of open and free Earth observation data and scalable interfaces including both new and historical Sentinel images, commercial datasets, as well as Copernicus Contributing Missions.\n\nUSGS Earth Explorer: EarthExplorer (EE) provides online search, browse display, metadata export, and data download for earth science data from the archives of the U.S. Geological Survey (USGS). Usually of largest demand are data from the Landsat missions.\n\nOpen Science Catalog: A catalog of publicly available geoscience products, datasets and resources developed in the frame of scientific research Projects funded by ESA EO. Queriable by themes, projects, variables and products.\n\nNCEI Catalog: NOAA‚Äôs National Centers for Environmental Information (NCEI) provides access to various environmental data, including satellite imagery, climate data, and other geospatial datasets. In particular it is composed of oceanic, atmospheric, and geophysical data.\n\nSTAC Index: A list of publicly available STAC APIs and Static Catalogs.\n\nThe content above shows the data collections available with the openEO API at the Copernicus Data Space Ecosystem (CDSE). You can scroll and click on the names to discover more details. \n\nOpen content in a new tab!","type":"content","url":"/lectures/data-discovery/data-discovery#where-to-search-for-data","position":45},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#how-to-search-for-data","position":46},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"Large EO data portals usually allow two main different ways of access based on the technical proficiency of the target user group. User can either use graphical user interface of selected portal or use one the available APIs for search. Bellow are examples of way how to search for your data.","type":"content","url":"/lectures/data-discovery/data-discovery#how-to-search-for-data","position":47},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Web Browser Catalogue client","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#web-browser-catalogue-client","position":48},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Web Browser Catalogue client","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"Many large portals provide a easy-to-use search and filtering GUI, which under the hood uses one of the provided catalog APIs. This allows less experienced users to perform queries usually for smaller subset of data with the help of an attached Web map client to orient themselves easily.","type":"content","url":"/lectures/data-discovery/data-discovery#web-browser-catalogue-client","position":49},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"API access","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#api-access","position":50},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"API access","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"For batch operations or script access to the catalogs, it is envisioned that direct approach of the API is performed instead. This removes the need for direct user interaction to the web platform and can be used from user provided scripts or other programs and CLI tools.\n\nFore more information about performing filtering or queries, see \n\nData Properties.","type":"content","url":"/lectures/data-discovery/data-discovery#api-access","position":51},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"STAC Browser","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#stac-browser","position":52},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"STAC Browser","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"In order to access and browse any online STAC catalog or API, a rich web client application STAC Browser can be used on \n\nradiantearth‚Äã.github‚Äã.io‚Äã/stac‚Äã-browser. It does does allow a wide variety of filtering capabilities.","type":"content","url":"/lectures/data-discovery/data-discovery#stac-browser","position":53},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"STAC QGIS plugin","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#stac-qgis-plugin","position":54},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"STAC QGIS plugin","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"QGIS STAC API Browser provides a simple and user-friendly approach in searching and using STAC API resources in QGIS. It offers a comfortable way to filter and browse the STAC API items and ability to add the STAC API assets as map layers into the QGIS.","type":"content","url":"/lectures/data-discovery/data-discovery#stac-qgis-plugin","position":55},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"QGIS MetaSearch Catalog Client","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#qgis-metasearch-catalog-client","position":56},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"QGIS MetaSearch Catalog Client","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"MetaSearch is a QGIS plugin to interact with metadata catalog services, supporting both the OGC API - Records and OGC Catalog Service for the Web (CSW) standards.","type":"content","url":"/lectures/data-discovery/data-discovery#qgis-metasearch-catalog-client","position":57},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"OWSLib","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#owslib","position":58},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"OWSLib","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"OWSLib is a Python package for client programming with Open Geospatial Consortium (OGC) web service (hence OWS) interface standards, and their related content models. The OWSLib client offers support for OGC CSW, OGC API - Records and OpenSearch based catalog services.","type":"content","url":"/lectures/data-discovery/data-discovery#owslib","position":59},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"PySTAC client","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#pystac-client","position":60},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"PySTAC client","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"The \n\nSTAC Python Client (pystac_client) is a Python package for working with STAC Catalogs and APIs that conform to the STAC and STAC API specs in a seamless way. PySTAC Client builds upon PySTAC through higher-level functionality and ability to leverage STAC API search endpoints.\n\n\n\nVideo produced by Qiusheng Wu (\n\nopengeos‚Äã/leafmap‚Äã#347)","type":"content","url":"/lectures/data-discovery/data-discovery#pystac-client","position":61},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Further Reading","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#further-reading","position":62},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Further Reading","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"STAC tutorials\n\nEchoes in space course","type":"content","url":"/lectures/data-discovery/data-discovery#further-reading","position":63},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"References","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#references","position":64},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"References","lvl3":"How to search for data","lvl2":"EO Catalog protocols"},"content":"STAC\n\nOWSLib\n\nSTAC Python Client (pystac_client)\n\nradiantearth‚Äã.github‚Äã.io‚Äã/stac‚Äã-browser\n\nOpenSearch specification","type":"content","url":"/lectures/data-discovery/data-discovery#references","position":65},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Quiz","lvl2":"EO Catalog protocols"},"type":"lvl3","url":"/lectures/data-discovery/data-discovery#quiz","position":66},{"hierarchy":{"lvl1":"Data Discovery","lvl3":"Quiz","lvl2":"EO Catalog protocols"},"content":"Let‚Äôs test your understanding of data types and where to find them. We will be working with the actual data shortly in the next chapters!","type":"content","url":"/lectures/data-discovery/data-discovery#quiz","position":67},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"STAC","lvl3":"Quiz","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#stac-1","position":68},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"STAC","lvl3":"Quiz","lvl2":"EO Catalog protocols"},"content":"What does the STAC stand for:[( )] Spatio and Temporal Asset Certification\n[(x)] SpatioTemporal Asset Catalog\n[( )] SpatioTemporal Asset Collections\n\nIn which line are only mandatory properties of STAC item?[(x)] type, id, assets, properties\n[( )] id, assets, collection, stac_version\n[( )] type, id, stac_version, stac_extensions","type":"content","url":"/lectures/data-discovery/data-discovery#stac-1","position":69},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Data Catalogs and Access Protocols","lvl3":"Quiz","lvl2":"EO Catalog protocols"},"type":"lvl4","url":"/lectures/data-discovery/data-discovery#data-catalogs-and-access-protocols","position":70},{"hierarchy":{"lvl1":"Data Discovery","lvl4":"Data Catalogs and Access Protocols","lvl3":"Quiz","lvl2":"EO Catalog protocols"},"content":"Which one of the listed data access API‚Äôs can you use on Copernicus Dataspace? See \n\nlist of API‚Äôs available[(x)] OData\n[(x)] STAC API\n[(x)] OGC API\n[( )] AstraDataServe\n[( )] OrbitQueryHub\n\nUse an online catalog browser of \n\nCopernicus Dataspace to search for Sentinel 2 L2A products over the island of Sardinia between 1.9.2023 and 30.9.2023 with cloud coverage less than or equal to 20% and get the number of returned products. Use enclosed \n\ngeometry file and upload it to the browser as shown on image below.[( )] 101-200\n[( )] 201-300\n[(x)] 0-100","type":"content","url":"/lectures/data-discovery/data-discovery#data-catalogs-and-access-protocols","position":71},{"hierarchy":{"lvl1":"Data Properties"},"type":"lvl1","url":"/lectures/data-properties/data-properties","position":0},{"hierarchy":{"lvl1":"Data Properties"},"content":"","type":"content","url":"/lectures/data-properties/data-properties","position":1},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Learning objectives"},"type":"lvl2","url":"/lectures/data-properties/data-properties#learning-objectives","position":2},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Learning objectives"},"content":"Learn what are metadata and why they are important\n\nDiscover data properties and how to filter by them","type":"content","url":"/lectures/data-properties/data-properties#learning-objectives","position":3},{"hierarchy":{"lvl1":"Data Properties","lvl2":"What are data properties and metadata"},"type":"lvl2","url":"/lectures/data-properties/data-properties#what-are-data-properties-and-metadata","position":4},{"hierarchy":{"lvl1":"Data Properties","lvl2":"What are data properties and metadata"},"content":"Data properties and metadata are sets of characteristics of geospatial data that serve different purposes. Commonly data properties are seen as attributes describing the image itself whereas, on the other hand, metadata describes the properties of a product.\n\nData Properties: Data properties refer to the characteristics and attributes of the geospatial data itself. These properties describe the actual content, structure, and values within the dataset. For raster data, properties can include information such as pixel values, cell size, spatial resolution, number of bands, and data format. For vector data, properties may include attributes associated with points, lines, or polygons, such as feature identifiers, names, classifications, or numerical values. Data properties are essential for understanding the data at a fundamental level and are used in the analysis, visualization, and processing tasks.\n\nMetadata: Metadata, on the other hand, provides descriptive information about the data product. It serves as documentation that complements the data properties and offers additional context and details about the data. Metadata describes the origin, and characteristics, and helps with usage of the data, allowing users to discover, evaluate, and effectively utilize the data. It typically includes information such as the data source, acquisition parameters, coordinate reference system, temporal coverage, quality measures, and data access policies. Metadata plays a crucial role in data management, data sharing, and data interoperability by providing the necessary documentation and context for understanding and utilizing the data.\n\nBoth together provide a complete description of the data of our interest itself and how to work with them. Standardized ways how to describe metadata and data properties allow geospatial systems to work with data easily.\n\nFigure: STAC standardizing Metadata.","type":"content","url":"/lectures/data-properties/data-properties#what-are-data-properties-and-metadata","position":5},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Properties and metadata used for filtering"},"type":"lvl2","url":"/lectures/data-properties/data-properties#properties-and-metadata-used-for-filtering","position":6},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Properties and metadata used for filtering"},"content":"In \n\nprevious lesson we learned where to find data. Now it is time to look at how we can select only the data we want. Properties and metadata are a great help when we are not interested in the whole collection, but only in certain regions, times or even bands of selected satellite products.\n\nIn (many of) the data catalogs, we can filter by specific values for each satellite. Let‚Äôs talk more about some of them shortly\n\nDataset name/identifier: Filtering directly by name of a product as a unique name or identifier is assigned to the dataset, allowing it to be easily identified and referenced.\n\nTime range: The temporal coverage or specific dates associated with the data acquisition. By this, we can easily select products from the same location with different dates of acquisition. This is particularly relevant for time-series or multi-temporal datasets.\n\nBounding box or other area of interest: Spatial Extent or the geographic coverage of the raster data, typically defined by the bounding coordinates (longitude and latitude) that encompass the dataset. Usually you can select your own area or interest as a rectangle/geometry or use a map window for taking the current extent\n\nMission: You can select by satellite used for data acquisition. In the case of Sentinel missions, you can select only Sentinel-1 as an example\n\nProcessing levels: Typically you can also select by processing level you are interested in for your missions. Typically Level-0 means unprocessed, raw data, and with higher number represents more corrections were applied.\n\nSensors or instrument: Selection by the output of a specific sensor or instrument.\n\nCloud coverage/polarization: Based on a mission, a selection filter can be made by specific parameters. For multi-spectral data such as captured by Sentinel-2, we can typically filter by cloud coverage in percentage. This is used as we are interested in lower cloud coverage percent for analysis. Similarly, specific polarizations can be selected for SAR data such as captured by Sentinel-1.\n\nOrbit number: Typically integer number selects a specific orbit number user is interested in.\n\nOrbit direction: For the majority of data types the selection is either ascending or descending.\n\nAvailability status/Timeliness: Some missions have different time availability for their data allowing them to select Near Real-Time acquisitions (approximately 3 hours after acquisition), raw data with a certain delay, or processed data later. It is also common that infrequently accessed data or older data than a certain threshold (years) can only be accessed ‚Äòon demand‚Äô. Their ‚ÄòAvailability status‚Äô is usually set as ‚ÄòArchived‚Äô or similar and they but must be ‚Äòtasked to be brought online from archive storage‚Äô by the user. This operation usually takes minutes to hours to complete. The user of the platform can later access the catalog again and hopefully the product have been brought online in the meantime.\n\nIn the examples above it also became more clear why a standardized way of expressing the metadata is important. For example, how do you know whether the cloud cover is a percentage expressed in a range from 0 to 1 or from 0 to 100? How would you filter on this property, if both scales would be mixed?\n\n \n\nVideo content in collaboration with \n\nMatthias Mohr (Major STAC contributor). \n‚ÄúHaving metadata in a standardized format makes your data available to others in a simple and unified way. Similarly, you can find and work with the data of others more easily. You can achieve these benefits with STAC.‚Äù","type":"content","url":"/lectures/data-properties/data-properties#properties-and-metadata-used-for-filtering","position":7},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Metadata and properties which are not typically used for filtering","lvl2":"Properties and metadata used for filtering"},"type":"lvl3","url":"/lectures/data-properties/data-properties#metadata-and-properties-which-are-not-typically-used-for-filtering","position":8},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Metadata and properties which are not typically used for filtering","lvl2":"Properties and metadata used for filtering"},"content":"Many metadata are contained within the product but are not used for filtering on the platforms or accessing hubs directly. Among these is for example Author of the dataset or Licence. You should be able to get all information about those in data metadata when you are accessing the data itself or on the general page with information.\n\nUsually it is unfortunatelly not possible to filter by or search by direct data properties, e.g. by values in data.","type":"content","url":"/lectures/data-properties/data-properties#metadata-and-properties-which-are-not-typically-used-for-filtering","position":9},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Dimensions","lvl2":"Properties and metadata used for filtering"},"type":"lvl3","url":"/lectures/data-properties/data-properties#dimensions","position":10},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Dimensions","lvl2":"Properties and metadata used for filtering"},"content":"Dimensions are important description of data and their properties. More information about dimensions of data and datacubes was covered in the lecture about \n\nDatacubes\n\nx, y and sometimes z - Spatial dimension of data\n\ntemporal/time dimension - capturing the time aspect for time series analysis","type":"content","url":"/lectures/data-properties/data-properties#dimensions","position":11},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Value Types (data types)","lvl2":"Properties and metadata used for filtering"},"type":"lvl3","url":"/lectures/data-properties/data-properties#value-types-data-types","position":12},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Value Types (data types)","lvl2":"Properties and metadata used for filtering"},"content":"Once we have selected data products we are interested in, we can look directly into values to select what we are interested in. Common data types representing measured values are these:\n\nbitmask 0/1\n\n8bit 0-255\n\nUInt16 - 0-65k\n\nInt16 - -32k - 32k\n\nFloat32","type":"content","url":"/lectures/data-properties/data-properties#value-types-data-types","position":13},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Exercise"},"type":"lvl2","url":"/lectures/data-properties/data-properties#exercise","position":14},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Exercise"},"content":"Try now to interact with STAC, get to know how to inspect the metadata and use GDAL for a simple data access!\n\nExercise 2.2 Data Properties","type":"content","url":"/lectures/data-properties/data-properties#exercise","position":15},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Further Reading","lvl2":"Exercise"},"type":"lvl3","url":"/lectures/data-properties/data-properties#further-reading","position":16},{"hierarchy":{"lvl1":"Data Properties","lvl3":"Further Reading","lvl2":"Exercise"},"content":"Standartized data management with STAC\n\nSTAC sessions from FOSS4G 2023","type":"content","url":"/lectures/data-properties/data-properties#further-reading","position":17},{"hierarchy":{"lvl1":"Data Properties","lvl3":"References","lvl2":"Exercise"},"type":"lvl3","url":"/lectures/data-properties/data-properties#references","position":18},{"hierarchy":{"lvl1":"Data Properties","lvl3":"References","lvl2":"Exercise"},"content":"","type":"content","url":"/lectures/data-properties/data-properties#references","position":19},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/data-properties/data-properties#quiz","position":20},{"hierarchy":{"lvl1":"Data Properties","lvl2":"Quiz"},"content":"Which of the following is not considered as Data Metadata[( )] Instrument\n[( )] Licence\n[(x)] Values from sensors\n\nWhich of the following is not considered as Data Properties[(x)] Licence\n[( )] Pixel values\n[( )] Number of bands\n\nWhich of these properties and metadata are commonly used for filtering when searching data?[(x)] Bounding Box\n[( )] E-Mail Adress\n[(x)] Time Range\n[(x)] Sensor\n[( )] Telephone Number\n[(x)] Cloud Coverage\n\nWhich statements about the advantage of filtering data by metadata and properties are true?[( )] All images are considered for subsequent analysis.\n[(x)] Only images that are relevant are considered for subsequent analysis.\n[(x)] The amount of data is reduced before starting the actual analysis.\n\nWhat is the EPSG code of the inspected STAC Item? Answer in exercise: 22_data_properties.ipynb[[32632]]\n\nWhat is the NODATA value of the red band read by GDAL? Answer in the gdalinfo output of the exercise: 22_data_properties.ipynb[[0]]","type":"content","url":"/lectures/data-properties/data-properties#quiz","position":21},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties"},"type":"lvl1","url":"/lectures/data-properties/exercises/data-properties","position":0},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties"},"content":"\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties","position":1},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl2":"GDAL, STAC and Data Properties"},"type":"lvl2","url":"/lectures/data-properties/exercises/data-properties#gdal-stac-and-data-properties","position":2},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl2":"GDAL, STAC and Data Properties"},"content":"\n\nIn this exercise we interact with a SpatioTemporal Asset Catalog (STAC) and explore the metadata using GDAL.\n\nStart importing the necessary libraries\n\nfrom osgeo import gdal\nfrom pystac_client import Client\n\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties#gdal-stac-and-data-properties","position":3},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Exploring STAC Collections","lvl2":"GDAL, STAC and Data Properties"},"type":"lvl3","url":"/lectures/data-properties/exercises/data-properties#exploring-stac-collections","position":4},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Exploring STAC Collections","lvl2":"GDAL, STAC and Data Properties"},"content":"\n\nConnect to a STAC API and explore available data collections.\n\nfrom pystac_client import Client\n\n# Connect to a STAC API\ncatalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n\n# List available collections\ncollections = catalog.get_collections()\nprint(\"Available STAC Collections:\")\nfor collection in collections:\n    print(collection.id)\n\n\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties#exploring-stac-collections","position":5},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Querying Sentinel-2 data from a STAC API","lvl2":"GDAL, STAC and Data Properties"},"type":"lvl3","url":"/lectures/data-properties/exercises/data-properties#querying-sentinel-2-data-from-a-stac-api","position":6},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Querying Sentinel-2 data from a STAC API","lvl2":"GDAL, STAC and Data Properties"},"content":"Connect to the same public STAC API and search for Sentinel-2 Level-2A products over a specific area (Sardinia) and time range (September 2023).We will filter the results to include only those items with less than or equal to 20% cloud cover.The code retrieves all matching items and prints the number of results, as well as the unique IDs of the returned items.This demonstrates how to query a geospatial data catalog programmatically using the pystac-client library- \n\nwhich documentation can be found here.\n\n# Connect to a public STAC API (e.g., Sentinel-2)\ncatalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n\n# Search for Sentinel-2 items over a specific area and date range\nitems = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    datetime=\"2023-09-01/2023-09-30\",\n    bbox=[8.0, 40.0, 9.0, 41.0],  # Example bounding box for Sardinia\n    query={\"eo:cloud_cover\": {\"lte\": 20}}\n).item_collection()\n\n# Print the number of items found\nprint(f\"Number of items found: {len(items)}\\n\")\n\n# Display the IDs of the found items\nprint(\"List of Sentinel-2 ids:\")\nfor item in items:\n    print(item.id)\n\n\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties#querying-sentinel-2-data-from-a-stac-api","position":7},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Metadata exploration","lvl2":"GDAL, STAC and Data Properties"},"type":"lvl3","url":"/lectures/data-properties/exercises/data-properties#metadata-exploration","position":8},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Metadata exploration","lvl2":"GDAL, STAC and Data Properties"},"content":"\n\nDisplay some metadata of the first STAC Item returned by our previous query:\n\n# Display metadata for the found items\nprint(f\"ID: {items[0].id}\")\nprint(f\"Date: {items[0].datetime}\")\nprint(f\"Cloud Cover: {items[0].properties['eo:cloud_cover']}%\")\nprint(f\"Geometry: {items[0].geometry}\")\nprint(f\"Projection as EPSG Code: {items[0].properties['proj:epsg']}\")\n\n\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties#metadata-exploration","position":9},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Accessing and inspecting a Sentinel-2 red band raster","lvl2":"GDAL, STAC and Data Properties"},"type":"lvl3","url":"/lectures/data-properties/exercises/data-properties#accessing-and-inspecting-a-sentinel-2-red-band-raster","position":10},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Accessing and inspecting a Sentinel-2 red band raster","lvl2":"GDAL, STAC and Data Properties"},"content":"\n\nAccess the first Sentinel-2 item from a previously retrieved STAC search result and extract the URL for the red band.Using GDAL, we will open the raster file from this URL and inspect its metadata, including the raster size, coordinate reference system (CRS), and geotransform parameters.This demonstrates how to programmatically access specific assets within a STAC item and retrieve relevant geospatial metadata.\n\nInspect the structure of items, available assets and properties\n\nitems[0]\n\n\n\nPrint the metadata using GDAL Python\n\nfirst_item = items[0]\n\nasset_red_href = first_item.assets['red'].href  # Assuming B04 (red band) is available\n\n# Open and inspect the raster file\ndataset = gdal.Open(asset_red_href)\nprint(f\"Raster Size: {dataset.RasterXSize} x {dataset.RasterYSize}\")\nprint(f\"Projection: {dataset.GetProjection()}\")\ngeotransform = dataset.GetGeoTransform()\nprint(f\"GeoTransform: {dataset.GetGeoTransform()}\")\n\n# Get spatial resolution\npixel_width = geotransform[1]\npixel_height = geotransform[5]\nprint(f\"Pixel Size: {pixel_width} x {pixel_height}\")\n\n# Get the number of bands\nbands = dataset.RasterCount\nprint(f\"Number of Bands: {bands}\")\n\n\n\nPrint the metadata using GDAL from command line\n\nimport os\nos.system(f\"gdalinfo {first_item.assets['red'].href}\")\n\n\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties#accessing-and-inspecting-a-sentinel-2-red-band-raster","position":11},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Simple band visualization","lvl2":"GDAL, STAC and Data Properties"},"type":"lvl3","url":"/lectures/data-properties/exercises/data-properties#simple-band-visualization","position":12},{"hierarchy":{"lvl1":"2.2 Data Discovery & Properties","lvl3":"Simple band visualization","lvl2":"GDAL, STAC and Data Properties"},"content":"\n\nRead the red band data as a NumPy array:\n\nred_band = dataset.GetRasterBand(1).ReadAsArray()\n\n\n\nFinally, visualize the content\n\nplt.imshow(red_band/1800,vmin=0,vmax=1)\n\n","type":"content","url":"/lectures/data-properties/exercises/data-properties#simple-band-visualization","position":13},{"hierarchy":{"lvl1":"Access EO Data from the Cloud"},"type":"lvl1","url":"/lectures/data-access/data-access-1","position":0},{"hierarchy":{"lvl1":"Access EO Data from the Cloud"},"content":"Using a cloud provider for accessing data, and in this specific scenario Earth Observation data, could improve your productivity a lot. To get the most out of it, we will provide you some important insights.","type":"content","url":"/lectures/data-access/data-access-1","position":1},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Learning objectives"},"type":"lvl3","url":"/lectures/data-access/data-access-1#learning-objectives","position":2},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Learning objectives"},"content":"In this lecture you will learn the usage and peculiarities of the main data operators commonly available on cloud platforms like:\n\nData loading\n\nFilter\n\nApply\n\nReduce\n\nResample\n\nAggregate\n\nFinally, you will be able to create a simple workflow with openEO or, alternatively with Pangeo.\n\nThe openEO exercises will use the openEO Python Client Side Processing functionality, which allows to experiment using openEO without a connection to an openEO back-end.\nAdditionally, we incorporated Pangeo exercises, which will allow you to experiment using the Pangeo ecosystem for scalable and interactive data analysis.","type":"content","url":"/lectures/data-access/data-access-1#learning-objectives","position":3},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Raster Data Loading"},"type":"lvl3","url":"/lectures/data-access/data-access-1#raster-data-loading","position":4},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Raster Data Loading"},"content":"","type":"content","url":"/lectures/data-access/data-access-1#raster-data-loading","position":5},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"type":"lvl4","url":"/lectures/data-access/data-access-1#raster-data-loading-with-openeo","position":6},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"content":"In openEO, your process graph will always start with defining the data you want to load, which can be done mainly using the following processes:\n\nload_collection: loads a collection from the current back-end by its id and returns it as a processable data cube. The data that is added to the data cube can be restricted with the parameters spatial_extent, temporal_extent, bands and properties.\n\nload_stac: loads data from a static STAC catalog or a STAC API Collection and returns the data as a processable data cube. A batch job result can be loaded by providing a reference to it. If supported by the underlying metadata and file format, the data that is added to the data cube can be restricted with the parameters spatial_extent, temporal_extent and bands.\n\nExercise 2.3 Data Access Lazy Loading with openEO","type":"content","url":"/lectures/data-access/data-access-1#raster-data-loading-with-openeo","position":7},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"type":"lvl5","url":"/lectures/data-access/data-access-1#references","position":8},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"content":"openEO Python Client Processing documentation\n\nload_collection process definition\n\nload_stac process definition","type":"content","url":"/lectures/data-access/data-access-1#references","position":9},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"type":"lvl4","url":"/lectures/data-access/data-access-1#raster-data-loading-with-pangeo","position":10},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"content":"With the Pangeo ecosystem, the STAC Catalogue Python Library pystac_client is recommended for lazily loading of data from a STAC Collection. To restrict the search, spatial extent can be passed as a parameter. Then for accessing the data, we use stackstac to create a data cube (an xarray in Python) of all the data.\n\nExercise 2.3 Data Access Lazy Loading with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#raster-data-loading-with-pangeo","position":11},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"type":"lvl5","url":"/lectures/data-access/data-access-1#references-1","position":12},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"content":"pystac_client\n\nstackstac\n\nxarray","type":"content","url":"/lectures/data-access/data-access-1#references-1","position":13},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Processes on Datacubes"},"type":"lvl3","url":"/lectures/data-access/data-access-1#processes-on-datacubes","position":14},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Processes on Datacubes"},"content":"In the following part, the basic processes for manipulating datacubes are introduced.","type":"content","url":"/lectures/data-access/data-access-1#processes-on-datacubes","position":15},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Filter","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#filter","position":16},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Filter","lvl3":"Processes on Datacubes"},"content":"When filtering data, only the data that satisfies a condition is returned. For example, this condition could be a timestamp or interval, (a set of) coordinates, or specific bands. By applying filtering the datacube becomes smaller, according to the selected data.\n\nWith openEO, you can use \n\nfilter_spatial, \n\nfilter_temporal, \n\nfilter_bands.\n\nAlternatively, using Pangeo, you can perform similar operations with tools like xarray and use Python‚Äôs [] syntax or sel or create masks with where for spatial, temporal, and band filtering, leveraging its powerful data processing capabilities of xarray Python package.\n\n:warning: Simplified\n\n\nfilter([üåΩ, ü•î, üê∑], isVegetarian) => [üåΩ, ü•î]\n\nIn the image, the example datacube can be seen at the top with labeled dimensions. The filtering techniques are displayed separately below. On the left, the datacube is filtered temporally with the interval [\"2020-10-15\", \"2020-10-27\"]. The result is a cube with only the rasters for the timestep that lies within that interval (\"2020-10-25\") and unchanged bands and spatial dimensions. Likewise, the original cube is filtered for a specific band [\"nir\"] in the middle and a specific spatial region [Polygon(...)] on the right.\n\nFigure: Datacube filtering: From the datacube 4 by 3 grid, arrows depict what happens if the grid is filtered. Temporal filtering results in data for one timestep with all four bands, filtering bands results in data with one band with all three timesteps, and spatial filtering results in all timesteps and bands being preserved, but all with a smaller area. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nExercise 2.3 on Data Access Filter can be completed using both openEO and the Pangeo ecosystem:\n\nExercise 2.3 Data Access Filter with openEO\n\nExercise 2.3 Data Access Filter with Pangeo.\n\nThis additional documentation is helpful to understand the previous exercises: \n\nXarray - Indexing and selecting data.","type":"content","url":"/lectures/data-access/data-access-1#filter","position":17},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Apply","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#apply","position":18},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Apply","lvl3":"Processes on Datacubes"},"content":"The apply* functions employ a process on the datacube that calculates new pixel values for each pixel, based on n other pixels.\n\nWith openEO, the following apply* functions can be used: \n\napply, \n\napply_neighborhood, \n\napply_dimension, \n\napply_kernel.\n\nSimilarly, with Pangeo, the apply and apply_ufunc functions in xarray allow operations on multidimensional arrays, where new values are calculated for each element based on surrounding data, similar to the apply functions in openEO.\n\nPlease note that several programming languages use the name map instead of apply, but they describe the same type of function.\n\n:warning: Simplified\n\n\napply([üåΩ, ü•î, üê∑], cook) => [üçø, üçü, üçñ]\n\nFor the case n = 1 this is called a unary function and means that only the pixel itself is considered when calculating the new pixel value. A prominent example is the absolute() function, calculating the absolute value of the input pixel value.\n\nFigure: Datacube apply unary: 3 example tiles hold values below and above 0. after applying the process ‚Äòabsolute‚Äô, all values in the three example tiles have changed to their absolute values above 0. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nIf n is larger than 1, the function is called n-ary. In practice, this means that the pixel neighbourhood is taken into account to calculate the new pixel value. Such neighbourhoods can be of spatial and/or temporal nature. A spatial function works on a kernel that weights the surrounding pixels (e.g. smoothing values with nearby observations), a temporal function works on a time series at a certain pixel location (e.g. smoothing values over time). Combinations of types to n-dimensional neighbourhoods are also possible.\n\nIn the example below, an example weighted kernel with the openEO ecosystem (shown in the middle) is applied to the cube (via openEO \n\napply_kernel). To avoid edge effects (affecting pixels on the edge of the image with less neighbours), a padding has been added in the background.\n\nFigure: Datacube apply spatial kernel: Three example tiles hold some values with a lot of variance. A spatial kernel (a cell plus it‚Äôs 4 direct neighbours) is applied to all pixels, and the result appears to be spatially smoothed, with less variance. Reference: \n\nopeneo.org (2022). Processes on Datacubes.\n\nOf course this also works for temporal neighbourhoods (timeseries), considering neighbours before and after a pixel. To be able to show the effect, two timesteps were added in this example figure. A moving average of window size 3 is then applied, meaning that for each pixel the average is calculated out of the previous, the next, and the timestep in question (tn-1, tn and tn+1). No padding was added which is why we observe edge effects (NA values are returned for t1 and t5, because their temporal neighbourhood is missing input timesteps).\n\nFigure: Datacube apply temporal moving average: Smoothing is applied to 5 example tiles by calculating the mean of 3 timesteps of every single pixel. The resulting tiles for the timestamps look much more alike. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nAlternatively, a process can also be applied along a dimension of the datacube, meaning the input is no longer a neighbourhood of some sort but all pixels along that dimension (n equals the complete dimension). If a process is applied along the time dimension (e.g. a breakpoint detection), the complete pixel timeseries are the input. If a process is applied along the spatial dimensions (e.g. a mean), all pixels of an image are the input. The process is then applied to all pixels along that dimension and the dimension continues to exist. This is in contrast to \n\nreduce, which drops the specified dimension of the data cube. In the image below, a mean is applied to the time dimension. An example pixel timeseries is highlighted by a green line and processed step-by-step.\n\nFigure: Datacube apply dimension time: The mean of all 5 timesteps is calculated for every single pixel. The resulting 5 tiles look exaclty the same, as they have been averaged. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nTry it out with the openEO and Pangeo ecosystems:\n\nExercise 2.3 Data Access Apply with openEO\n\nExercise 2.3 Data Access Apply with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#apply","position":19},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Reduce","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#reduce","position":20},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Reduce","lvl3":"Processes on Datacubes"},"content":"The \n\nreduce_dimension process collapses a whole dimension of the datacube. It does so by using some sort of reducer, which is a function that calculates a single result from an amount of values, as e.g. mean(), min() and max() are. For example we can reduce the time dimension (t) of a timeseries by calculating the mean value of all timesteps for each pixel. We are left with a cube that has no time dimension, because all values of that dimension are compressed into a single mean value. The same goes for e.g. the spatial dimensions: If we calculate the mean along the x and y dimensions, we are left without any spatial dimensions, but a mean value for each instance that previously was a raster is returned. In the image below, the dimensions that are reduced are crossed out in the result.\n\n:warning: Simplified\n\n\nreduce([ü•¨, ü•í, üçÖ, üßÖ], prepare) => ü•ó\n\nThink of it as a waste press that does math instead of using brute force. Given a representation of our example datacube, let‚Äôs see how it is affected.\n\nFigure: Datacube reduce: Three arrows depict what happens to the 12 example tiles, if they are reduced: Reducing timesteps leads to four tiles (one for each band), and the time dimension is deleted. Reducing bands lead to one tile per timestep, and the bands dimension is deleted. Reducing spatially leads to the original 4 by 3 bands by time layout, but the result has no spatial dimension and thus, the tiles have been turned into single values, per tile. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nAs mentioned earlier, reduction operations can be performed with both openEO and Pangeo:\n\nExercise 2.3 Data Access Reduce with openEO\n\nExercise 2.3 Data Access Reduce with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#reduce","position":21},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Resample","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#resample","position":22},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Resample","lvl3":"Processes on Datacubes"},"content":"In a resampling processes, the layout of a certain dimension is changed into another layout, most likely also changing the resolution of that dimension. This is done by mapping values of the source (old) datacube to the new layout of the target (new) datacube. During that process, resolutions can be upscaled or downscaled (also called upsampling and downsampling), depending on whether they have a finer or a coarser spacing afterwards. A function is then needed to translate the existing data into the new resolution. A prominent example is to reproject a datacube into the coordinate reference system of another datacube, for example in order to merge the two cubes.\n\nWith the openEO ecosystem, you can use \n\nresample_cube_spatial, \n\nresample_cube_temporal).\nWith the Pangeo ecosystem and xarray, you can use \n\nresample or \n\ncoarsen.\n\n:warning: Simplified\n\n\nresample(üñºÔ∏è, downscale) => üü¶\n\nresample(üåç, reproject) => üó∫Ô∏è\n\n\n\nThe first figure gives an overview of temporal resampling. How exactly the input timesteps are rescaled to the output timesteps depends on the resampling function.\n\nFigure: Datacube temporal resampling (up and down): Downsampling: To a timeline-representation of the example tiles, another timeline with only 2 steps at different dates is applied. The result has tiles only at those new timesteps. In Upsampling, the existing 3 timesteps are sampled into 5 result timesteps. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nThe second figure displays spatial resampling. Observe how in the upsampling process, the output datacube has not gained in information value. The resulting grid still carries the same pixel information, but in higher spatial resolution. Other upsampling methods may yield smoother results, e.g. by using interpolation.\n\nFigure: Datacube spatial resampling (up and down): Downsampling: The resulting tiles have a lower spatial resolution than the input tiles. Upsampling: The resulting tiles have a higher spatial resolution than the input tiles, but contain the same image than before (no information added). Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nTry it out with exercises using both the openEO and Pangeo ecosystems:\n\nExercise 2.3 Data Access Resample with openEO\n\nExercise 2.3 Data Access Resample with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#resample","position":23},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Aggregate","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#aggregate","position":24},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Aggregate","lvl3":"Processes on Datacubes"},"content":"An aggregation of a datacube can be thought of as a grouped reduce. That means it consists of two steps:\n\nGrouping via a grouping variable, i.e. spatial geometries or temporal intervals\n\nReducing these groups along the grouped dimension with a certain reducer function, e.g. calculating the mean pixel value per polygon or the maximum pixel values per month\n\nWhile the layout of the reduced dimension is changed, other dimensions keep their resolution and geometry. But in contrast to pure reduce, the dimensions along which the reducer function is applied still exist after the operation.\n\n:warning: Simplified\n\n\naggregate(üë™ üë©‚Äçüë¶ üë®‚Äçüë©‚Äçüë¶‚Äçüë¶, countFamilyMembers) => [3Ô∏è‚É£, 2Ô∏è‚É£, 4Ô∏è‚É£]\n\nA temporal aggregation (e.g. with openEO \n\naggregate_temporal or with Pangeo \n\nGroupBy is similar to the downsampling process, as it can be seen in the according image above. Intervals for grouping can either be set manually, or periods can be chosen: monthly, yearly, etc. All timesteps in an interval are then collapsed via a reducer function (mean, max, etc.) and assigned to the given new labels.\n\nA spatial aggregation (e.g. with openEO \n\naggregate_spatial or with Pangeo \n\nGroupBy) works in a similar manner. Polygons, lines and points can be selected for grouping. Their spatial dimension is then reduced by a given process and thus, a vector cube is returned. The vector cube then has dimensions containing features, attributes and time. In the graphic below, the grouping is only shown for the first timestep.\n\nFigure: Datacube spatial aggregation: A line and a polygon are selected from the original example tiles. The pixels covered by these geometries are aggregated and the result consists no longer of imagery tiles but of an array with values for 2 geometries by 4 bands by 3 timesteps. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nTry it out with openEO and Pangeo:\n\nExercise 2.3 Data Access Aggregate with openEO\n\nExercise 2.3 Data Access Aggregate with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#aggregate","position":25},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Recap Processes","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#recap-processes","position":26},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Recap Processes","lvl3":"Processes on Datacubes"},"content":"Here are some exercises to recap the different processes that can be used on a data cube.","type":"content","url":"/lectures/data-access/data-access-1#recap-processes","position":27},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#references-2","position":28},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Processes on Datacubes"},"content":"openeo.org (2022). Processes on Datacubes.\n\npangeo.io (2022).\n\nxarray (2025): \n\nDocumentation; Reference: Hoyer, S., Roos, M., Joseph, H., Magin, J., Cherian, D., Fitzgerald, C., Hauser, M., Fujii, K., Maussion, F., Imperiale, G., Clark, S., Kleeman, A., Nicholas, T., Kluyver, T., Westling, J., Munroe, J., Amici, A., Barghini, A., Banihirwe, A., ‚Ä¶ Littlejohns, O. (2025). xarray (v2025.01.1). Zenodo. \n\nDOI: 10.5281/zenodo.14623571","type":"content","url":"/lectures/data-access/data-access-1#references-2","position":29},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl3","url":"/lectures/data-access/data-access-1#openeo-a-standardized-api-for-eo-cloud-processing","position":30},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"","type":"content","url":"/lectures/data-access/data-access-1#openeo-a-standardized-api-for-eo-cloud-processing","position":31},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for a standarized API in EO cloud processing","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#the-need-for-a-standarized-api-in-eo-cloud-processing","position":32},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for a standarized API in EO cloud processing","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"Earth Observation data are becoming too large to be downloaded locally for analysis. Also, the way they are organised (as tiles, or granules: files containing the imagery for a small part of the Earth and a single observation date) makes it unnecessary complicated to analyse them. The solution to this is to store these data in the cloud, on compute back-ends, process them there, and browse the results or download resulting figures or numbers. But how do we do that? openEO develops an open application programming interface (API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way.\nWith such an API,\n\neach client can work with every back-end, and\n\nit becomes possible to compare back-ends in terms of capacity, cost, and results (validation, reproducibility)","type":"content","url":"/lectures/data-access/data-access-1#the-need-for-a-standarized-api-in-eo-cloud-processing","position":33},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does openEO stand for?","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#what-does-openeo-stand-for","position":34},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does openEO stand for?","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"The acronym openEO contracts two concepts:\n\nopen: used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0)\nEO: Earth observation\nJointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts:\n\nSimplicity: nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows\n\nUnification: current EO cloud back-ends all have a different API, making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems.","type":"content","url":"/lectures/data-access/data-access-1#what-does-openeo-stand-for","position":35},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Why an API?","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#why-an-api","position":36},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Why an API?","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"An API is an application programming interface. It defines a language that two computers (a client and a server) use to communicate.\nThe following figure shows how many interfaces are needed to be able to compare back-ends from different clients, without an openEO API. And if you use the slider you will see how the situation becomes much clearer with a standardized API.\n\n \n\nVideo content in collaboration with \n\nEdzer Pebesma (University of M√ºnster). \n‚ÄúFor analysing Earth Observation data, don‚Äôt use a platform that locks you in.‚Äù","type":"content","url":"/lectures/data-access/data-access-1#why-an-api","position":37},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#references-3","position":38},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"openeo.org - About OpenEO\n\nEdzer Pebesma, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian Briese, Markus Neteler (2016). OpenEO: a GDAL for Earth Observation Analytics. https://r-spatial.org/2016/11/29/openeo.html\n\nopeneo.org - Project Deliverable 22","type":"content","url":"/lectures/data-access/data-access-1#references-3","position":39},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl3","url":"/lectures/data-access/data-access-1#pangeo-an-open-source-ecosystem-for-cloud-based-data-analysis","position":40},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"","type":"content","url":"/lectures/data-access/data-access-1#pangeo-an-open-source-ecosystem-for-cloud-based-data-analysis","position":41},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for an open source-source ecosystem in EO cloud-based data analysis","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#the-need-for-an-open-source-source-ecosystem-in-eo-cloud-based-data-analysis","position":42},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for an open source-source ecosystem in EO cloud-based data analysis","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"The need for an open-source ecosystem in cloud-based data analysis\nEarth Observation data are growing rapidly in size, making it increasingly difficult to download and process them locally. Additionally, the way these data are structured‚Äîoften as tiles or granules (files containing imagery for a specific part of the Earth and a single observation date)‚Äîcan complicate analysis. The solution to this challenge is to store and process these data in the cloud, utilizing scalable compute resources, and to browse or download the processed results. But how do we achieve this? Pangeo is an open-source ecosystem that enables scalable analysis of large geospatial and environmental data in the cloud. It leverages Python and tools such as xarray, dask, and zarr to provide an accessible, flexible, and high-performance framework for data processing and analysis.\n\nWith this ecosystem:\n\nUsers can seamlessly process and analyze large datasets in the cloud using powerful, distributed computing tools.\n\nIt becomes easier to work with diverse data sources and compare different cloud platforms for performance, cost, and reproducibility of results.","type":"content","url":"/lectures/data-access/data-access-1#the-need-for-an-open-source-source-ecosystem-in-eo-cloud-based-data-analysis","position":43},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does Pangeo stand for?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#what-does-pangeo-stand-for","position":44},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does Pangeo stand for?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"The acronym Pangeo combines two key concepts:\n\nPan: Derived from the Greek word for ‚Äúall‚Äù or ‚Äúevery,‚Äù reflecting the goal of making tools and data accessible to everyone, with a focus on enabling large-scale data analysis and open collaboration.\n\nGeo: Referring to geospatial and Earth system data, Pangeo focuses on the analysis and processing of large environmental datasets, particularly in the context of Earth Observation (EO), climate science, and related fields.\n\nTogether, Pangeo provides an open-source ecosystem for scalable analysis of geospatial and Earth system data in the cloud. The main objectives of the Pangeo project include:\n\nSimplicity: Pangeo leverages widely-used Python libraries like xarray, dask, and zarr to provide easy-to-use tools for processing and analyzing large, multidimensional datasets. The aim is to make working with large volumes of geospatial data as simple as possible, while integrating seamlessly into existing workflows.\n\nUnification: Different data sources and computing environments often require specific configurations and APIs, which can make it difficult to combine and compare datasets across platforms. Pangeo unifies these efforts by offering a set of open-source tools and a scalable framework that works across multiple cloud back-ends, promoting reproducibility, cost efficiency, and data interoperability.","type":"content","url":"/lectures/data-access/data-access-1#what-does-pangeo-stand-for","position":45},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"How is the API for Pangeo?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#how-is-the-api-for-pangeo","position":46},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"How is the API for Pangeo?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"The Pangeo ecosystem doesn‚Äôt have a single API, but instead relies on a set of Python tools for scalable data processing. Key components include:\n\nxarray: For working with multi-dimensional arrays, such as geospatial data.\n\ndask: For distributed computing, enabling parallel processing of large datasets.\n\nzarr: A storage format for efficient handling of large, chunked datasets.\n\nThese tools work together to provide a flexible, cloud-based framework for Earth system data analysis.","type":"content","url":"/lectures/data-access/data-access-1#how-is-the-api-for-pangeo","position":47},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#references-4","position":48},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"Pangeo.io - About the Pangeo ecosystem\n\nxarray (2025)","type":"content","url":"/lectures/data-access/data-access-1#references-4","position":49},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/data-access/data-access-1#quiz","position":50},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl2":"Quiz"},"content":"Lazy data loading with openEO: What is the difference between using an openEO remote back-end and openEO Python Client-Side-Processing? Tick what is correct. Answer in exercise: 23_openeo_data_access_lazy_loading.ipynb[[ ]] You need an account to authenticate in both cases\n[[x]] For openEO, you don't need an account with Client-Side Processing\n[[x]] For openEO, the client-side processing does not interact with an openEO back-end\n[[x]] data processing does not interact with a openEO back-end\n\nLazy data loading with Pangeo: How does data access differ between working with remote cloud-based data and processing data locally in the Pangeo ecosystem?\nAnswer in exercise: 23_pangeo_data_access_lazy_loading.ipynb[[ ]] You need a cloud account to access both remote and local data\n[[x]] Pangeo allows direct access to cloud-based datasets without needing to download them first\n[[x]] Processing data locally in Pangeo does not require cloud access\n[[ ]] Pangeo local processing requires an internet connection for all data access\n[[x]] Remote datasets in Pangeo can be accessed lazily, meaning they are only loaded when needed\n\nLazy data loading: What are the dimension names of the loaded datacube? Answer in exercises: 23_openeo_data_access_lazy_loading.ipynb and 23_pangeo_data_access_lazy_loading.pynb‚Äô[( )] northing, easting, time, spectral\n[( )] lat, lon, t, bands\n[(x)] time, band, y, x\n\nFiltering: How many time steps are left after filtering temporally 2023-05-10, 2023-06-30? Answer in exercises: 23_openeo_data_access_filter.ipynb[( )] 11\n[(x)] 7\n[( )] 32\n\nFiltering: How many time steps are left after filtering temporally 2024-05-10, 2024-06-30? Answer in exercises: 23_pangeo_data_access_filter.ipynb[(x)] 3\n[( )] 9\n[( )] 32\n\nFiltering with openEO: How many pixels are left along y after using filter_bbox with spatial_extent = {\"west\": 11.259613, \"east\": 11.406212, \"south\": 46.461019, \"north\": 46.522237}? Answer in exercise: 23_openeo_data_access_filter.ipynb and 23_pangeo_data_access_filter.ipynb[( )] 1006\n[( )] 1145\n[(x)] 489\n\nReducing Dimension: What would be a use case for reducing the time dimension? Tick what is correct.[[ ]] Get a full time series graph\n[[x]] Getting a cloudfree image for a certain time range\n[[x]] Get information about a whole season\n[[ ]] Filling gaps in a time series\n\nReducing Dimension: How many pixels are left in the datacube after we use reduce_dimension over band? Answer in exercises: 23_openeo_data_access_reduce.ipynb[[21226010]]\n\nReducing Dimension: How many pixels are left in the datacube after we reduce the temporal dimension using Xarray? Answer in exercises: 23_pangeo_data_access_reduce.ipynb[[1636488]]\n\nReducing Dimension: What are the dimension names after running reduce_spatial with openEO? Answer in exercise: 23_openeo_data_access_reduce.ipynb[( )] x, y, band\n[( )] time, x, y\n[(x)] time, band\n\nReducing Dimension: How many NDVI values in time remain after computing the NDVI time series with Pangeo? Answer in exercise: 23_pangeo_data_access_reduce.ipynb[( )] less than 10\n[( )] less than 100\n[(x)] more than 100\n\nApply Operator: What would be a use case for using the apply operator? Tick what is correct. Answer in exercises: 23_openeo_data_access_apply.ipynb and 23_pangeo_data_access_apply.ipynb[( )] Get a full time series graph\n[(x)] Rescale the values to create a visible image\n[( )] Get information about a whole season\n[(x)] Create a binary mask applying a threshold\n\nWhich of the following statements are correct regarding how Sentinel-2 data is resampled to match Landsat? Answer in exercises: 23_openeo_data_access_resample.ipynb and 23_pangeo_data_access_resample.ipynb[[x]] OpenEO uses resample_cube_spatial() to align spatial resolution.\n[[x]] Pangeo uses stackstac.stack() with resolution=30.0\n[[ ]] OpenEO automatically resamples Sentinel-2 when loading it.\n[[ ]] Pangeo requires a separate reprojecting step after stacking.\n\nSelecting, Filtering, and Resampling Data: How does Pangeo‚Äôs approach compare to OpenEO when working with large geospatial datasets?[[x]] Both Pangeo and OpenEO support efficient selection, filtering, and resampling of large datasets using parallelized computation.\n[[x]] Pangeo leverages libraries like xarray and dask for flexible local or cloud processing, while OpenEO provides an API-based approach with local and backend execution.\n[[ ]] OpenEO only allows remote back-end processing and does not support local execution.\n[[ ]] Pangeo requires users to manually handle all resampling operations without built-in functions.\n[[x]] OpenEO is optimized for cloud-based operations, while Pangeo provides a more customizable workflow with Python-based tools.","type":"content","url":"/lectures/data-access/data-access-1#quiz","position":51},{"hierarchy":{"lvl1":"Access EO Data from the Cloud"},"type":"lvl1","url":"/lectures/data-access/data-access-1","position":0},{"hierarchy":{"lvl1":"Access EO Data from the Cloud"},"content":"Using a cloud provider for accessing data, and in this specific scenario Earth Observation data, could improve your productivity a lot. To get the most out of it, we will provide you some important insights.","type":"content","url":"/lectures/data-access/data-access-1","position":1},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Learning objectives"},"type":"lvl3","url":"/lectures/data-access/data-access-1#learning-objectives","position":2},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Learning objectives"},"content":"In this lecture you will learn the usage and peculiarities of the main data operators commonly available on cloud platforms like:\n\nData loading\n\nFilter\n\nApply\n\nReduce\n\nResample\n\nAggregate\n\nFinally, you will be able to create a simple workflow with openEO or, alternatively with Pangeo.\n\nThe openEO exercises will use the openEO Python Client Side Processing functionality, which allows to experiment using openEO without a connection to an openEO back-end.\nAdditionally, we incorporated Pangeo exercises, which will allow you to experiment using the Pangeo ecosystem for scalable and interactive data analysis.","type":"content","url":"/lectures/data-access/data-access-1#learning-objectives","position":3},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Raster Data Loading"},"type":"lvl3","url":"/lectures/data-access/data-access-1#raster-data-loading","position":4},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Raster Data Loading"},"content":"","type":"content","url":"/lectures/data-access/data-access-1#raster-data-loading","position":5},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"type":"lvl4","url":"/lectures/data-access/data-access-1#raster-data-loading-with-openeo","position":6},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"content":"In openEO, your process graph will always start with defining the data you want to load, which can be done mainly using the following processes:\n\nload_collection: loads a collection from the current back-end by its id and returns it as a processable data cube. The data that is added to the data cube can be restricted with the parameters spatial_extent, temporal_extent, bands and properties.\n\nload_stac: loads data from a static STAC catalog or a STAC API Collection and returns the data as a processable data cube. A batch job result can be loaded by providing a reference to it. If supported by the underlying metadata and file format, the data that is added to the data cube can be restricted with the parameters spatial_extent, temporal_extent and bands.\n\nExercise 2.3 Data Access Lazy Loading with openEO","type":"content","url":"/lectures/data-access/data-access-1#raster-data-loading-with-openeo","position":7},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"type":"lvl5","url":"/lectures/data-access/data-access-1#references","position":8},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with openEO","lvl3":"Raster Data Loading"},"content":"openEO Python Client Processing documentation\n\nload_collection process definition\n\nload_stac process definition","type":"content","url":"/lectures/data-access/data-access-1#references","position":9},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"type":"lvl4","url":"/lectures/data-access/data-access-1#raster-data-loading-with-pangeo","position":10},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"content":"With the Pangeo ecosystem, the STAC Catalogue Python Library pystac_client is recommended for lazily loading of data from a STAC Collection. To restrict the search, spatial extent can be passed as a parameter. Then for accessing the data, we use stackstac to create a data cube (an xarray in Python) of all the data.\n\nExercise 2.3 Data Access Lazy Loading with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#raster-data-loading-with-pangeo","position":11},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"type":"lvl5","url":"/lectures/data-access/data-access-1#references-1","position":12},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl5":"References","lvl4":"Raster Data Loading with Pangeo","lvl3":"Raster Data Loading"},"content":"pystac_client\n\nstackstac\n\nxarray","type":"content","url":"/lectures/data-access/data-access-1#references-1","position":13},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Processes on Datacubes"},"type":"lvl3","url":"/lectures/data-access/data-access-1#processes-on-datacubes","position":14},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Processes on Datacubes"},"content":"In the following part, the basic processes for manipulating datacubes are introduced.","type":"content","url":"/lectures/data-access/data-access-1#processes-on-datacubes","position":15},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Filter","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#filter","position":16},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Filter","lvl3":"Processes on Datacubes"},"content":"When filtering data, only the data that satisfies a condition is returned. For example, this condition could be a timestamp or interval, (a set of) coordinates, or specific bands. By applying filtering the datacube becomes smaller, according to the selected data.\n\nWith openEO, you can use \n\nfilter_spatial, \n\nfilter_temporal, \n\nfilter_bands.\n\nAlternatively, using Pangeo, you can perform similar operations with tools like xarray and use Python‚Äôs [] syntax or sel or create masks with where for spatial, temporal, and band filtering, leveraging its powerful data processing capabilities of xarray Python package.\n\n:warning: Simplified\n\n\nfilter([üåΩ, ü•î, üê∑], isVegetarian) => [üåΩ, ü•î]\n\nIn the image, the example datacube can be seen at the top with labeled dimensions. The filtering techniques are displayed separately below. On the left, the datacube is filtered temporally with the interval [\"2020-10-15\", \"2020-10-27\"]. The result is a cube with only the rasters for the timestep that lies within that interval (\"2020-10-25\") and unchanged bands and spatial dimensions. Likewise, the original cube is filtered for a specific band [\"nir\"] in the middle and a specific spatial region [Polygon(...)] on the right.\n\nFigure: Datacube filtering: From the datacube 4 by 3 grid, arrows depict what happens if the grid is filtered. Temporal filtering results in data for one timestep with all four bands, filtering bands results in data with one band with all three timesteps, and spatial filtering results in all timesteps and bands being preserved, but all with a smaller area. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nExercise 2.3 on Data Access Filter can be completed using both openEO and the Pangeo ecosystem:\n\nExercise 2.3 Data Access Filter with openEO\n\nExercise 2.3 Data Access Filter with Pangeo.\n\nThis additional documentation is helpful to understand the previous exercises: \n\nXarray - Indexing and selecting data.","type":"content","url":"/lectures/data-access/data-access-1#filter","position":17},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Apply","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#apply","position":18},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Apply","lvl3":"Processes on Datacubes"},"content":"The apply* functions employ a process on the datacube that calculates new pixel values for each pixel, based on n other pixels.\n\nWith openEO, the following apply* functions can be used: \n\napply, \n\napply_neighborhood, \n\napply_dimension, \n\napply_kernel.\n\nSimilarly, with Pangeo, the apply and apply_ufunc functions in xarray allow operations on multidimensional arrays, where new values are calculated for each element based on surrounding data, similar to the apply functions in openEO.\n\nPlease note that several programming languages use the name map instead of apply, but they describe the same type of function.\n\n:warning: Simplified\n\n\napply([üåΩ, ü•î, üê∑], cook) => [üçø, üçü, üçñ]\n\nFor the case n = 1 this is called a unary function and means that only the pixel itself is considered when calculating the new pixel value. A prominent example is the absolute() function, calculating the absolute value of the input pixel value.\n\nFigure: Datacube apply unary: 3 example tiles hold values below and above 0. after applying the process ‚Äòabsolute‚Äô, all values in the three example tiles have changed to their absolute values above 0. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nIf n is larger than 1, the function is called n-ary. In practice, this means that the pixel neighbourhood is taken into account to calculate the new pixel value. Such neighbourhoods can be of spatial and/or temporal nature. A spatial function works on a kernel that weights the surrounding pixels (e.g. smoothing values with nearby observations), a temporal function works on a time series at a certain pixel location (e.g. smoothing values over time). Combinations of types to n-dimensional neighbourhoods are also possible.\n\nIn the example below, an example weighted kernel with the openEO ecosystem (shown in the middle) is applied to the cube (via openEO \n\napply_kernel). To avoid edge effects (affecting pixels on the edge of the image with less neighbours), a padding has been added in the background.\n\nFigure: Datacube apply spatial kernel: Three example tiles hold some values with a lot of variance. A spatial kernel (a cell plus it‚Äôs 4 direct neighbours) is applied to all pixels, and the result appears to be spatially smoothed, with less variance. Reference: \n\nopeneo.org (2022). Processes on Datacubes.\n\nOf course this also works for temporal neighbourhoods (timeseries), considering neighbours before and after a pixel. To be able to show the effect, two timesteps were added in this example figure. A moving average of window size 3 is then applied, meaning that for each pixel the average is calculated out of the previous, the next, and the timestep in question (tn-1, tn and tn+1). No padding was added which is why we observe edge effects (NA values are returned for t1 and t5, because their temporal neighbourhood is missing input timesteps).\n\nFigure: Datacube apply temporal moving average: Smoothing is applied to 5 example tiles by calculating the mean of 3 timesteps of every single pixel. The resulting tiles for the timestamps look much more alike. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nAlternatively, a process can also be applied along a dimension of the datacube, meaning the input is no longer a neighbourhood of some sort but all pixels along that dimension (n equals the complete dimension). If a process is applied along the time dimension (e.g. a breakpoint detection), the complete pixel timeseries are the input. If a process is applied along the spatial dimensions (e.g. a mean), all pixels of an image are the input. The process is then applied to all pixels along that dimension and the dimension continues to exist. This is in contrast to \n\nreduce, which drops the specified dimension of the data cube. In the image below, a mean is applied to the time dimension. An example pixel timeseries is highlighted by a green line and processed step-by-step.\n\nFigure: Datacube apply dimension time: The mean of all 5 timesteps is calculated for every single pixel. The resulting 5 tiles look exaclty the same, as they have been averaged. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nTry it out with the openEO and Pangeo ecosystems:\n\nExercise 2.3 Data Access Apply with openEO\n\nExercise 2.3 Data Access Apply with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#apply","position":19},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Reduce","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#reduce","position":20},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Reduce","lvl3":"Processes on Datacubes"},"content":"The \n\nreduce_dimension process collapses a whole dimension of the datacube. It does so by using some sort of reducer, which is a function that calculates a single result from an amount of values, as e.g. mean(), min() and max() are. For example we can reduce the time dimension (t) of a timeseries by calculating the mean value of all timesteps for each pixel. We are left with a cube that has no time dimension, because all values of that dimension are compressed into a single mean value. The same goes for e.g. the spatial dimensions: If we calculate the mean along the x and y dimensions, we are left without any spatial dimensions, but a mean value for each instance that previously was a raster is returned. In the image below, the dimensions that are reduced are crossed out in the result.\n\n:warning: Simplified\n\n\nreduce([ü•¨, ü•í, üçÖ, üßÖ], prepare) => ü•ó\n\nThink of it as a waste press that does math instead of using brute force. Given a representation of our example datacube, let‚Äôs see how it is affected.\n\nFigure: Datacube reduce: Three arrows depict what happens to the 12 example tiles, if they are reduced: Reducing timesteps leads to four tiles (one for each band), and the time dimension is deleted. Reducing bands lead to one tile per timestep, and the bands dimension is deleted. Reducing spatially leads to the original 4 by 3 bands by time layout, but the result has no spatial dimension and thus, the tiles have been turned into single values, per tile. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nAs mentioned earlier, reduction operations can be performed with both openEO and Pangeo:\n\nExercise 2.3 Data Access Reduce with openEO\n\nExercise 2.3 Data Access Reduce with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#reduce","position":21},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Resample","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#resample","position":22},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Resample","lvl3":"Processes on Datacubes"},"content":"In a resampling processes, the layout of a certain dimension is changed into another layout, most likely also changing the resolution of that dimension. This is done by mapping values of the source (old) datacube to the new layout of the target (new) datacube. During that process, resolutions can be upscaled or downscaled (also called upsampling and downsampling), depending on whether they have a finer or a coarser spacing afterwards. A function is then needed to translate the existing data into the new resolution. A prominent example is to reproject a datacube into the coordinate reference system of another datacube, for example in order to merge the two cubes.\n\nWith the openEO ecosystem, you can use \n\nresample_cube_spatial, \n\nresample_cube_temporal).\nWith the Pangeo ecosystem and xarray, you can use \n\nresample or \n\ncoarsen.\n\n:warning: Simplified\n\n\nresample(üñºÔ∏è, downscale) => üü¶\n\nresample(üåç, reproject) => üó∫Ô∏è\n\n\n\nThe first figure gives an overview of temporal resampling. How exactly the input timesteps are rescaled to the output timesteps depends on the resampling function.\n\nFigure: Datacube temporal resampling (up and down): Downsampling: To a timeline-representation of the example tiles, another timeline with only 2 steps at different dates is applied. The result has tiles only at those new timesteps. In Upsampling, the existing 3 timesteps are sampled into 5 result timesteps. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nThe second figure displays spatial resampling. Observe how in the upsampling process, the output datacube has not gained in information value. The resulting grid still carries the same pixel information, but in higher spatial resolution. Other upsampling methods may yield smoother results, e.g. by using interpolation.\n\nFigure: Datacube spatial resampling (up and down): Downsampling: The resulting tiles have a lower spatial resolution than the input tiles. Upsampling: The resulting tiles have a higher spatial resolution than the input tiles, but contain the same image than before (no information added). Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nTry it out with exercises using both the openEO and Pangeo ecosystems:\n\nExercise 2.3 Data Access Resample with openEO\n\nExercise 2.3 Data Access Resample with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#resample","position":23},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Aggregate","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#aggregate","position":24},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Aggregate","lvl3":"Processes on Datacubes"},"content":"An aggregation of a datacube can be thought of as a grouped reduce. That means it consists of two steps:\n\nGrouping via a grouping variable, i.e. spatial geometries or temporal intervals\n\nReducing these groups along the grouped dimension with a certain reducer function, e.g. calculating the mean pixel value per polygon or the maximum pixel values per month\n\nWhile the layout of the reduced dimension is changed, other dimensions keep their resolution and geometry. But in contrast to pure reduce, the dimensions along which the reducer function is applied still exist after the operation.\n\n:warning: Simplified\n\n\naggregate(üë™ üë©‚Äçüë¶ üë®‚Äçüë©‚Äçüë¶‚Äçüë¶, countFamilyMembers) => [3Ô∏è‚É£, 2Ô∏è‚É£, 4Ô∏è‚É£]\n\nA temporal aggregation (e.g. with openEO \n\naggregate_temporal or with Pangeo \n\nGroupBy is similar to the downsampling process, as it can be seen in the according image above. Intervals for grouping can either be set manually, or periods can be chosen: monthly, yearly, etc. All timesteps in an interval are then collapsed via a reducer function (mean, max, etc.) and assigned to the given new labels.\n\nA spatial aggregation (e.g. with openEO \n\naggregate_spatial or with Pangeo \n\nGroupBy) works in a similar manner. Polygons, lines and points can be selected for grouping. Their spatial dimension is then reduced by a given process and thus, a vector cube is returned. The vector cube then has dimensions containing features, attributes and time. In the graphic below, the grouping is only shown for the first timestep.\n\nFigure: Datacube spatial aggregation: A line and a polygon are selected from the original example tiles. The pixels covered by these geometries are aggregated and the result consists no longer of imagery tiles but of an array with values for 2 geometries by 4 bands by 3 timesteps. Reference: \n\nopeneo.org (2022). Processes on Datacubes..\n\nTry it out with openEO and Pangeo:\n\nExercise 2.3 Data Access Aggregate with openEO\n\nExercise 2.3 Data Access Aggregate with Pangeo","type":"content","url":"/lectures/data-access/data-access-1#aggregate","position":25},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Recap Processes","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#recap-processes","position":26},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Recap Processes","lvl3":"Processes on Datacubes"},"content":"Here are some exercises to recap the different processes that can be used on a data cube.","type":"content","url":"/lectures/data-access/data-access-1#recap-processes","position":27},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Processes on Datacubes"},"type":"lvl4","url":"/lectures/data-access/data-access-1#references-2","position":28},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Processes on Datacubes"},"content":"openeo.org (2022). Processes on Datacubes.\n\npangeo.io (2022).\n\nxarray (2025): \n\nDocumentation; Reference: Hoyer, S., Roos, M., Joseph, H., Magin, J., Cherian, D., Fitzgerald, C., Hauser, M., Fujii, K., Maussion, F., Imperiale, G., Clark, S., Kleeman, A., Nicholas, T., Kluyver, T., Westling, J., Munroe, J., Amici, A., Barghini, A., Banihirwe, A., ‚Ä¶ Littlejohns, O. (2025). xarray (v2025.01.1). Zenodo. \n\nDOI: 10.5281/zenodo.14623571","type":"content","url":"/lectures/data-access/data-access-1#references-2","position":29},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl3","url":"/lectures/data-access/data-access-1#openeo-a-standardized-api-for-eo-cloud-processing","position":30},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"","type":"content","url":"/lectures/data-access/data-access-1#openeo-a-standardized-api-for-eo-cloud-processing","position":31},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for a standarized API in EO cloud processing","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#the-need-for-a-standarized-api-in-eo-cloud-processing","position":32},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for a standarized API in EO cloud processing","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"Earth Observation data are becoming too large to be downloaded locally for analysis. Also, the way they are organised (as tiles, or granules: files containing the imagery for a small part of the Earth and a single observation date) makes it unnecessary complicated to analyse them. The solution to this is to store these data in the cloud, on compute back-ends, process them there, and browse the results or download resulting figures or numbers. But how do we do that? openEO develops an open application programming interface (API) that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way.\nWith such an API,\n\neach client can work with every back-end, and\n\nit becomes possible to compare back-ends in terms of capacity, cost, and results (validation, reproducibility)","type":"content","url":"/lectures/data-access/data-access-1#the-need-for-a-standarized-api-in-eo-cloud-processing","position":33},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does openEO stand for?","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#what-does-openeo-stand-for","position":34},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does openEO stand for?","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"The acronym openEO contracts two concepts:\n\nopen: used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0)\nEO: Earth observation\nJointly, the openEO targets the processing and analysis of Earth observation data. The main objectives of the project are the following concepts:\n\nSimplicity: nowadays, many end-users use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows\n\nUnification: current EO cloud back-ends all have a different API, making EO data analysis hard to validate and reproduce and back-ends difficult to compare in terms of capability and costs, or to combine them in a joint analysis across back-ends. A unified API can resolve many of these problems.","type":"content","url":"/lectures/data-access/data-access-1#what-does-openeo-stand-for","position":35},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Why an API?","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#why-an-api","position":36},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"Why an API?","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"An API is an application programming interface. It defines a language that two computers (a client and a server) use to communicate.\nThe following figure shows how many interfaces are needed to be able to compare back-ends from different clients, without an openEO API. And if you use the slider you will see how the situation becomes much clearer with a standardized API.\n\n \n\nVideo content in collaboration with \n\nEdzer Pebesma (University of M√ºnster). \n‚ÄúFor analysing Earth Observation data, don‚Äôt use a platform that locks you in.‚Äù","type":"content","url":"/lectures/data-access/data-access-1#why-an-api","position":37},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"openEO - A standardized API for EO cloud processing"},"type":"lvl4","url":"/lectures/data-access/data-access-1#references-3","position":38},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"openEO - A standardized API for EO cloud processing"},"content":"openeo.org - About OpenEO\n\nEdzer Pebesma, Wolfgang Wagner, Jan Verbesselt, Erwin Goor, Christian Briese, Markus Neteler (2016). OpenEO: a GDAL for Earth Observation Analytics. https://r-spatial.org/2016/11/29/openeo.html\n\nopeneo.org - Project Deliverable 22","type":"content","url":"/lectures/data-access/data-access-1#references-3","position":39},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl3","url":"/lectures/data-access/data-access-1#pangeo-an-open-source-ecosystem-for-cloud-based-data-analysis","position":40},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"","type":"content","url":"/lectures/data-access/data-access-1#pangeo-an-open-source-ecosystem-for-cloud-based-data-analysis","position":41},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for an open source-source ecosystem in EO cloud-based data analysis","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#the-need-for-an-open-source-source-ecosystem-in-eo-cloud-based-data-analysis","position":42},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"The need for an open source-source ecosystem in EO cloud-based data analysis","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"The need for an open-source ecosystem in cloud-based data analysis\nEarth Observation data are growing rapidly in size, making it increasingly difficult to download and process them locally. Additionally, the way these data are structured‚Äîoften as tiles or granules (files containing imagery for a specific part of the Earth and a single observation date)‚Äîcan complicate analysis. The solution to this challenge is to store and process these data in the cloud, utilizing scalable compute resources, and to browse or download the processed results. But how do we achieve this? Pangeo is an open-source ecosystem that enables scalable analysis of large geospatial and environmental data in the cloud. It leverages Python and tools such as xarray, dask, and zarr to provide an accessible, flexible, and high-performance framework for data processing and analysis.\n\nWith this ecosystem:\n\nUsers can seamlessly process and analyze large datasets in the cloud using powerful, distributed computing tools.\n\nIt becomes easier to work with diverse data sources and compare different cloud platforms for performance, cost, and reproducibility of results.","type":"content","url":"/lectures/data-access/data-access-1#the-need-for-an-open-source-source-ecosystem-in-eo-cloud-based-data-analysis","position":43},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does Pangeo stand for?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#what-does-pangeo-stand-for","position":44},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"What does Pangeo stand for?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"The acronym Pangeo combines two key concepts:\n\nPan: Derived from the Greek word for ‚Äúall‚Äù or ‚Äúevery,‚Äù reflecting the goal of making tools and data accessible to everyone, with a focus on enabling large-scale data analysis and open collaboration.\n\nGeo: Referring to geospatial and Earth system data, Pangeo focuses on the analysis and processing of large environmental datasets, particularly in the context of Earth Observation (EO), climate science, and related fields.\n\nTogether, Pangeo provides an open-source ecosystem for scalable analysis of geospatial and Earth system data in the cloud. The main objectives of the Pangeo project include:\n\nSimplicity: Pangeo leverages widely-used Python libraries like xarray, dask, and zarr to provide easy-to-use tools for processing and analyzing large, multidimensional datasets. The aim is to make working with large volumes of geospatial data as simple as possible, while integrating seamlessly into existing workflows.\n\nUnification: Different data sources and computing environments often require specific configurations and APIs, which can make it difficult to combine and compare datasets across platforms. Pangeo unifies these efforts by offering a set of open-source tools and a scalable framework that works across multiple cloud back-ends, promoting reproducibility, cost efficiency, and data interoperability.","type":"content","url":"/lectures/data-access/data-access-1#what-does-pangeo-stand-for","position":45},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"How is the API for Pangeo?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#how-is-the-api-for-pangeo","position":46},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"How is the API for Pangeo?","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"The Pangeo ecosystem doesn‚Äôt have a single API, but instead relies on a set of Python tools for scalable data processing. Key components include:\n\nxarray: For working with multi-dimensional arrays, such as geospatial data.\n\ndask: For distributed computing, enabling parallel processing of large datasets.\n\nzarr: A storage format for efficient handling of large, chunked datasets.\n\nThese tools work together to provide a flexible, cloud-based framework for Earth system data analysis.","type":"content","url":"/lectures/data-access/data-access-1#how-is-the-api-for-pangeo","position":47},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"type":"lvl4","url":"/lectures/data-access/data-access-1#references-4","position":48},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl4":"References","lvl3":"Pangeo - An Open-Source Ecosystem for Cloud-Based Data Analysis"},"content":"Pangeo.io - About the Pangeo ecosystem\n\nxarray (2025)","type":"content","url":"/lectures/data-access/data-access-1#references-4","position":49},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/data-access/data-access-1#quiz","position":50},{"hierarchy":{"lvl1":"Access EO Data from the Cloud","lvl2":"Quiz"},"content":"Lazy data loading with openEO: What is the difference between using an openEO remote back-end and openEO Python Client-Side-Processing? Tick what is correct. Answer in exercise: 23_openeo_data_access_lazy_loading.ipynb[[ ]] You need an account to authenticate in both cases\n[[x]] For openEO, you don't need an account with Client-Side Processing\n[[x]] For openEO, the client-side processing does not interact with an openEO back-end\n[[x]] data processing does not interact with a openEO back-end\n\nLazy data loading with Pangeo: How does data access differ between working with remote cloud-based data and processing data locally in the Pangeo ecosystem?\nAnswer in exercise: 23_pangeo_data_access_lazy_loading.ipynb[[ ]] You need a cloud account to access both remote and local data\n[[x]] Pangeo allows direct access to cloud-based datasets without needing to download them first\n[[x]] Processing data locally in Pangeo does not require cloud access\n[[ ]] Pangeo local processing requires an internet connection for all data access\n[[x]] Remote datasets in Pangeo can be accessed lazily, meaning they are only loaded when needed\n\nLazy data loading: What are the dimension names of the loaded datacube? Answer in exercises: 23_openeo_data_access_lazy_loading.ipynb and 23_pangeo_data_access_lazy_loading.pynb‚Äô[( )] northing, easting, time, spectral\n[( )] lat, lon, t, bands\n[(x)] time, band, y, x\n\nFiltering: How many time steps are left after filtering temporally 2023-05-10, 2023-06-30? Answer in exercises: 23_openeo_data_access_filter.ipynb[( )] 11\n[(x)] 7\n[( )] 32\n\nFiltering: How many time steps are left after filtering temporally 2024-05-10, 2024-06-30? Answer in exercises: 23_pangeo_data_access_filter.ipynb[(x)] 3\n[( )] 9\n[( )] 32\n\nFiltering with openEO: How many pixels are left along y after using filter_bbox with spatial_extent = {\"west\": 11.259613, \"east\": 11.406212, \"south\": 46.461019, \"north\": 46.522237}? Answer in exercise: 23_openeo_data_access_filter.ipynb and 23_pangeo_data_access_filter.ipynb[( )] 1006\n[( )] 1145\n[(x)] 489\n\nReducing Dimension: What would be a use case for reducing the time dimension? Tick what is correct.[[ ]] Get a full time series graph\n[[x]] Getting a cloudfree image for a certain time range\n[[x]] Get information about a whole season\n[[ ]] Filling gaps in a time series\n\nReducing Dimension: How many pixels are left in the datacube after we use reduce_dimension over band? Answer in exercises: 23_openeo_data_access_reduce.ipynb[[21226010]]\n\nReducing Dimension: How many pixels are left in the datacube after we reduce the temporal dimension using Xarray? Answer in exercises: 23_pangeo_data_access_reduce.ipynb[[1636488]]\n\nReducing Dimension: What are the dimension names after running reduce_spatial with openEO? Answer in exercise: 23_openeo_data_access_reduce.ipynb[( )] x, y, band\n[( )] time, x, y\n[(x)] time, band\n\nReducing Dimension: How many NDVI values in time remain after computing the NDVI time series with Pangeo? Answer in exercise: 23_pangeo_data_access_reduce.ipynb[( )] less than 10\n[( )] less than 100\n[(x)] more than 100\n\nApply Operator: What would be a use case for using the apply operator? Tick what is correct. Answer in exercises: 23_openeo_data_access_apply.ipynb and 23_pangeo_data_access_apply.ipynb[( )] Get a full time series graph\n[(x)] Rescale the values to create a visible image\n[( )] Get information about a whole season\n[(x)] Create a binary mask applying a threshold\n\nWhich of the following statements are correct regarding how Sentinel-2 data is resampled to match Landsat? Answer in exercises: 23_openeo_data_access_resample.ipynb and 23_pangeo_data_access_resample.ipynb[[x]] OpenEO uses resample_cube_spatial() to align spatial resolution.\n[[x]] Pangeo uses stackstac.stack() with resolution=30.0\n[[ ]] OpenEO automatically resamples Sentinel-2 when loading it.\n[[ ]] Pangeo requires a separate reprojecting step after stacking.\n\nSelecting, Filtering, and Resampling Data: How does Pangeo‚Äôs approach compare to OpenEO when working with large geospatial datasets?[[x]] Both Pangeo and OpenEO support efficient selection, filtering, and resampling of large datasets using parallelized computation.\n[[x]] Pangeo leverages libraries like xarray and dask for flexible local or cloud processing, while OpenEO provides an API-based approach with local and backend execution.\n[[ ]] OpenEO only allows remote back-end processing and does not support local execution.\n[[ ]] Pangeo requires users to manually handle all resampling operations without built-in functions.\n[[x]] OpenEO is optimized for cloud-based operations, while Pangeo provides a more customizable workflow with Python-based tools.","type":"content","url":"/lectures/data-access/data-access-1#quiz","position":51},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate"},"type":"lvl1","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate","lvl2":"Aggregate Operators with openEO"},"type":"lvl2","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate#aggregate-operators-with-openeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate","lvl2":"Aggregate Operators with openEO"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate#aggregate-operators-with-openeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate","lvl3":"aggregate_temporal_period: temporal aggregation with predefined intervals","lvl2":"Aggregate Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate#aggregate-temporal-period-temporal-aggregation-with-predefined-intervals","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate","lvl3":"aggregate_temporal_period: temporal aggregation with predefined intervals","lvl2":"Aggregate Operators with openEO"},"content":"\n\nStart importing the necessary libraries and initialize a local connection for Client-Side Processing.\n\nimport openeo\nfrom openeo.local import LocalConnection\nlocal_conn = LocalConnection('')\n\n\n\nCreate the starting Sentinel-2 datacube:\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n\nspatial_extent = {\"west\": 11.4, \"east\": 11.42, \"south\": 45.5, \"north\": 45.52}\ntemporal_extent = [\"2020-01-01\", \"2020-12-31\"]\nbands = [\"red\",\"green\",\"blue\"]\nproperties = {\"eo:cloud_cover\": dict(lt=50)}\n\ns2_cube = local_conn.load_stac(url=url,\n   spatial_extent=spatial_extent,\n   temporal_extent=temporal_extent,\n   bands=bands,\n   properties=properties\n)\ns2_cube.execute()\n\n\n\nWe might be interested in aggregating our data over periods like week, month, year etc., defining what operation to use to combine the data available in the chosen period.\n\nUsing aggregate_temporal_period we can achieve this easily:\n\ns2_monthly_min = s2_cube.aggregate_temporal_period(period=\"month\",reducer=\"min\")\ns2_monthly_min\n\n\n\nCheck what happens to the datacube inspecting the resulting xArray object. Now the time dimension has 12 labels, one for each month.\n\ns2_monthly_min.execute()\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate#aggregate-temporal-period-temporal-aggregation-with-predefined-intervals","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate","lvl3":"aggregate_spatial: spatial aggregation with geometries","lvl2":"Aggregate Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate#aggregate-spatial-spatial-aggregation-with-geometries","position":6},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Aggregate","lvl3":"aggregate_spatial: spatial aggregation with geometries","lvl2":"Aggregate Operators with openEO"},"content":"\n\nBefore performing the operation on the satellite data with openEO, we need to get a vector data to be used in our workflow.\n\nWe will use the city boundaries of Bolzano (Italy) to perform an aggregation of the pixels within the specified geometry.\n\nThe geometry will be loaded from a public geoJSON file containing the boundaries of all the italian municipalities.\n\nimport geopandas as gpd\n\nitalian_municipalities = gpd.read_file(\"https://raw.githubusercontent.com/openpolis/geojson-italy/refs/heads/master/comuni.geojson\")\nitalian_municipalities.head()\n\n\n\nWe select the row corresponding to Bolzano and plot it with a base layer:\n\nimport contextily as cx\n\nbz_geom = italian_municipalities.loc[italian_municipalities[\"name\"] == \"Bolzano/Bozen\"]\n\nbz_wm = bz_geom.to_crs(epsg=3857)\nax = bz_wm.plot(figsize=(10, 10), alpha=0.5, edgecolor=\"k\")\ncx.add_basemap(ax)\n\n\n\nWe can now build our workflow with openEO, starting to load some Sentinel-2 data from STAC as before:\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n\nspatial_extent = {\"west\": 11.25, \"east\": 11.44, \"south\": 46.4, \"north\": 46.6}\ntemporal_extent = [\"2020-01-01\", \"2020-12-31\"]\nbands = [\"red\"]\nproperties = {\"eo:cloud_cover\": dict(lt=15),\n             \"earthsearch:boa_offset_applied\": dict(eq=True)}\n\ns2_cube = local_conn.load_stac(url=url,\n   spatial_extent=spatial_extent,\n   temporal_extent=temporal_extent,\n   bands=bands,\n   properties=properties\n)\n\n\n\nNow we call the aggregate_spatial process and compute the average over the Sentinel-2 pixels inside the city.\n\nRunning this cell may take up to 2 minutes\n\nfrom joblib import parallel_backend\n\nwith parallel_backend(\"threading\"):\n    bolzano_mean_xr = s2_cube.aggregate_spatial(\n        geometries=bz_geom.geometry.values[0].geoms[0],\n        reducer=\"mean\"\n    ).execute()\n\n\n\n\nFinally, we can plot the resulting time series of values for a sample band:\n\nbolzano_mean_xr[:,:,0].plot()\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-aggregate#aggregate-spatial-spatial-aggregation-with-geometries","position":7},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Apply"},"type":"lvl1","url":"/lectures/data-access/exercises/openeo/openeo-data-access-apply","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Apply"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-apply","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Apply","lvl2":"Apply Operator with openEO"},"type":"lvl2","url":"/lectures/data-access/exercises/openeo/openeo-data-access-apply#apply-operator-with-openeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Apply","lvl2":"Apply Operator with openEO"},"content":"The apply operator applies a process to each value in the data cube (i.e. a local operation).\n\nLet‚Äôs start again with the same sample data from the Sentinel-2 STAC Collection, applying the filters directly in the load_stac call, to reduce the amount of data.\n\nimport openeo\nfrom openeo.local import LocalConnection\nlocal_conn = LocalConnection('')\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\nspatial_extent = {\"west\": 11.259613, \"east\": 11.406212, \"south\": 46.461019, \"north\": 46.522237}\ntemporal_extent = ['2022-07-10T00:00:00Z','2022-07-13T00:00:00Z']\nbands = [\"red\",\"green\",\"blue\"]\nproperties = {\"eo:cloud_cover\": dict(lt=50)}\ndatacube = local_conn.load_stac(url=url,\n                    spatial_extent=spatial_extent,\n                    temporal_extent = temporal_extent,\n                    bands=bands,\n                    properties=properties\n)\n\ndatacube.execute()\n\n\n\nVisualize the RGB bands of our sample dataset:\n\ndata = datacube.execute()\ndata[0].plot.imshow()\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-apply#apply-operator-with-openeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Apply","lvl3":"Apply an arithmetic formula","lvl2":"Apply Operator with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-apply#apply-an-arithmetic-formula","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Apply","lvl3":"Apply an arithmetic formula","lvl2":"Apply Operator with openEO"},"content":"\n\nWe would like to improve the previous visualization, rescaling all the pixels between 0 and 1.\n\nWe can use apply in combination with the linear_scale_range processe.\n\nfrom openeo.processes import linear_scale_range\ninput_min = -0.1\ninput_max = 0.2\noutput_min = 0\noutput_max = 1\n\ndef rescale(x):\n    return linear_scale_range(x,input_min,input_max,output_min,output_max)\n\nscaled_data = datacube.apply(rescale)\nscaled_data\n\n\n\nVisualize the result and see how apply scaled the data resulting in a more meaningful visualization:\n\nscaled_data_xr = scaled_data.execute()\nscaled_data_xr[0].plot.imshow()\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-apply#apply-an-arithmetic-formula","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter"},"type":"lvl1","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl2":"Filter Operators with openEO"},"type":"lvl2","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#filter-operators-with-openeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl2":"Filter Operators with openEO"},"content":"When interacting with large data collections, it is necessary to keep in mind that it‚Äôs not possible to load everything!\n\nTherefore, we always have to define our requirements in advance and apply them to the data using filter operators.\n\nLet‚Äôs start again with the same sample data from the Sentinel-2 STAC Collection with an additional filter.\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#filter-operators-with-openeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Properties Filter","lvl2":"Filter Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#properties-filter","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Properties Filter","lvl2":"Filter Operators with openEO"},"content":"When working with optical data like Sentinel-2, most of the times we would like to discard cloudy acquisitions as soon as possible.\n\nWe can do it using a property filter: in this case we want to keep only the acquisitions with less than 50% cloud coverage.\n\nproperties = {\"eo:cloud_cover\": dict(lt=50)}\n\n\n\nimport openeo\nfrom openeo.local import LocalConnection\nlocal_conn = LocalConnection('')\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\nspatial_extent = {\"west\": 11.1, \"east\": 11.5, \"south\": 46.1, \"north\": 46.5}\ntemporal_extent = [\"2023-04-01T00:00:00Z\",\"2023-11-30T00:00:00Z\"]\nbands = [\"red\",\"green\",\"blue\",\"nir\"]\n\ndatacube = local_conn.load_stac(url=url,\n                    spatial_extent=spatial_extent,\n                    temporal_extent=temporal_extent,\n                    bands=bands,\n                    properties=properties)\ndatacube.execute()\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#properties-filter","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Temporal Filter","lvl2":"Filter Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#temporal-filter","position":6},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Temporal Filter","lvl2":"Filter Operators with openEO"},"content":"\n\nTo filter along time the data collection with openEO, we can use the filter_temporal process.\n\ntemporal_extent = [\"2023-05-10T00:00:00Z\",\"2023-06-30T00:00:00Z\"]\ntemporal_slice = datacube.filter_temporal(temporal_extent)\ntemporal_slice.execute()\n\n\n\nAfter running the previous cell, it is visible that the result has less elements (or labels) in the temporal dimension time.\n\nAdditionally, the size of the selected data reduced a lot.\n\nQuiz hint: look carefully at the dimensions of the resulting datacube!\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#temporal-filter","position":7},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Spatial Filter","lvl2":"Filter Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#spatial-filter","position":8},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Spatial Filter","lvl2":"Filter Operators with openEO"},"content":"\n\nTo slice along the spatial dimensions the data collection with openEO, we can use filter_bbox or filter_spatial processes.\n\nThe filter_bbox process is used with a set of coordinates:\n\nspatial_extent = {\"west\": 11.259613, \"east\": 11.406212, \"south\": 46.461019, \"north\": 46.522237}\nspatial_slice = datacube.filter_bbox(spatial_extent)\nspatial_slice.execute()\n\n\n\nQuiz hint: look carefully at the dimensions of the loaded datacube!\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#spatial-filter","position":9},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Bands Filter","lvl2":"Filter Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#bands-filter","position":10},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Filter","lvl3":"Bands Filter","lvl2":"Filter Operators with openEO"},"content":"\n\nTo slice along the bands dimension, keeping only the necessary bands, we can use the filter_bands process.\n\nbands = [\"red\",\"green\",\"blue\"]\nbands_slice = datacube.filter_bands(bands)\nbands_slice.execute()\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-filter#bands-filter","position":11},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Lazy Loading"},"type":"lvl1","url":"/lectures/data-access/exercises/openeo/openeo-data-access-lazy-loading","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Lazy Loading"},"content":"\n\n\n\nThe exercise will use the openEO Python Client Side Processing functionality, which allows to experiment using openEO without a connection to an openEO back-end.\n\nQuiz hint: remeber this information for the final quiz!\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-lazy-loading","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Lazy Loading","lvl2":"Lazy data loading with openEO"},"type":"lvl2","url":"/lectures/data-access/exercises/openeo/openeo-data-access-lazy-loading#lazy-data-loading-with-openeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Lazy Loading","lvl2":"Lazy data loading with openEO"},"content":"When accessing data using an API, most of the time the data is lazily loaded.\n\nIt means that only the metadata is loaded, so that it is possible to know about the data dimensions and their extents (spatial and temporal), the available bands and other additional information.\n\nLet‚Äôs start with a call to the openEO process load_stac for lazily loading some Sentinel-2 data from a public STAC Collection. Please note that not every STAC Collection or Item is currently supported.\n\nWe need to specify an Area Of Interest (AOI) to get only part of the Collection, otherwise our code would try to load the metadata of all Sentinel-2 tiles available in the world!\n\nRunning this cell may take up to 2 minutes\n\nfrom openeo.local import LocalConnection\nlocal_conn = LocalConnection('')\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\nspatial_extent = {\"west\": 11.1, \"east\": 11.5, \"south\": 46.1, \"north\": 46.5}\ntemporal_extent = [\"2015-01-01\",\"2022-01-01\"]\ndatacube = local_conn.load_stac(\n    url=url,\n    spatial_extent=spatial_extent,\n    temporal_extent=temporal_extent)\ndatacube\n\n\n\nCalling the .execute() method, the data will be lazily loaded and an xArray.DataArray object returned.\n\nRunning the next cell will show the selected data content with the dimension names and their extent.\n\nRunning this cell may take up to 2 minutes\n\ndatacube.execute()\n\n\n\nFrom the output of the previous cell you can notice something really interesting: the size of the selected data is more than 3 TB!\n\nBut you should have noticed that it was too quick to download this huge amount of data.\n\nThis is what lazy loading allows: getting all the information about the data in a quick manner without having to access and download all the available files.\n\nQuiz hint: look carefully at the dimensions of the loaded datacube!","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-lazy-loading#lazy-data-loading-with-openeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Reduce"},"type":"lvl1","url":"/lectures/data-access/exercises/openeo/openeo-data-access-reduce","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Reduce"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-reduce","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Reduce","lvl2":"Reduce Operators with openEO"},"type":"lvl2","url":"/lectures/data-access/exercises/openeo/openeo-data-access-reduce#reduce-operators-with-openeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Reduce","lvl2":"Reduce Operators with openEO"},"content":"When computing statistics over time or indices based on multiple bands, it is possible to use reduce operators.\n\nIn openEO we can use the \n\nreduce_dimension process, which applies a reducer to a data cube dimension by collapsing all the values along the specified dimension into an output value computed by the reducer.\n\nReduce the temporal dimension to a single value, the mean for instance:\n\nimport openeo\nfrom openeo.processes import clip\nfrom openeo.local import LocalConnection\nlocal_conn = LocalConnection('')\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\nspatial_extent = {\"west\": 11.259613, \"east\": 11.406212, \"south\": 46.461019, \"north\": 46.522237}\ntemporal_extent = [\"2021-05-28T00:00:00Z\",\"2021-06-30T00:00:00Z\"]\nbands = [\"red\",\"nir\"]\ndatacube = local_conn.load_stac(url=url,\n                                spatial_extent=spatial_extent,\n                                temporal_extent=temporal_extent,\n                                bands=bands)\ndatacube = datacube.apply(lambda x: clip(x,0,10000)) # Get rid of possible negative values\n\ndatacube_mean_time = datacube.reduce_dimension(dimension=\"time\",reducer=\"mean\")\ndatacube_mean_time\n\n\n\nCheck what happens to the datacube inspecting the resulting xArray object:\n\ndatacube_mean_time.execute()\n\n\n\nIt is possible to reduce in the same way all the available dimensions of the datacube.\n\nWe can, for instance, reduce the band dimension similarly as we did for the temporal dimension:\n\ndatacube_mean_band = datacube.reduce_dimension(dimension=\"band\",reducer=\"mean\")\n\n\n\nThe result will now contain values resulting from the average of the bands:\n\ndatacube_mean_band.execute()\n\n\n\nQuiz hint: look carefully at number of pixels of the loaded datacube!\n\nThe reducer could be again a single process, but when computing spectral indices like NDVI, NDSI etc. an arithmentical formula is used instead.\n\nFor instance, the \n\nNDVI formula can be expressed using a reduce_dimension process over the bands dimension:NDVI = {{NIR - RED} \\over {NIR + RED}}\n\ndef NDVI(data):\n    red = data.array_element(index=0)\n    nir = data.array_element(index=1)\n    ndvi = (nir - red)/(nir + red)\n    return ndvi\n\nndvi = datacube.reduce_dimension(reducer=NDVI,dimension=\"band\")\nndvi_xr = ndvi.execute()\nndvi_xr\n\n\n\nVisualize a sample NDVI result:\n\n%%time\nndvi_xr[0].plot.imshow(vmin=-1,vmax=1,cmap=\"Greens\")\n\n\n\nAdditionally, it is possible to reduce both spatial dimensions of the datacube at the same time.\n\nTo do this, we need the reduce_spatial process.\n\nThis time we select a smaller area of interest, to reduce the amount of downloaded data:\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\nspatial_extent = {\"west\": 11.31369, \"east\": 11.31906, \"south\": 46.52167, \"north\": 46.52425}\ntemporal_extent = [\"2021-01-01T00:00:00Z\",\"2021-12-30T00:00:00Z\"]\nbands = [\"red\",\"nir\"]\nproperties = {\"eo:cloud_cover\": dict(lt=15)}\n\ndatacube = local_conn.load_stac(url=url,\n                                spatial_extent=spatial_extent,\n                                temporal_extent=temporal_extent,\n                                bands=bands,\n                                properties=properties)\ndatacube = datacube.apply(lambda x: clip(x,0,10000)) # Get rid of possible negative values\n\n\n\ndatacube_spatial_median = datacube.reduce_spatial(reducer=\"median\")\ndatacube_spatial_median\n\n\n\nVerify that the spatial dimensions were collapsed:\n\ndatacube_spatial_median.execute()\n\n\n\nQuiz hint: look carefully at the dimensions of the resulting datacube!\n\nWe can combine this spatial reducer with the previous over bands to compute a time series of NDVI values:\n\nndvi_spatial = datacube_spatial_median.reduce_dimension(reducer=NDVI,dimension=\"band\")\n\n\n\nndvi_spatial_xr = ndvi_spatial.execute()\nndvi_spatial_xr = ndvi_spatial_xr.compute()\nndvi_spatial_xr\n\n\n\nRemember that calling .compute() on an xarray + dask based object will load into memory the data.\nIn this case it will trigger the download of the data from the STAC Catalog and the processing defined as openEO process graph, computing the NDVI time series.\n\nVisualize the NDVI time series:\n\nndvi_spatial_xr.where(ndvi_spatial_xr<1).plot.scatter()\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-reduce#reduce-operators-with-openeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Resample"},"type":"lvl1","url":"/lectures/data-access/exercises/openeo/openeo-data-access-resample","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Resample"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-resample","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Resample","lvl2":"Resample Operators with openEO"},"type":"lvl2","url":"/lectures/data-access/exercises/openeo/openeo-data-access-resample#resample-operators-with-openeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Resample","lvl2":"Resample Operators with openEO"},"content":"Sometimes we need to align the spatial or temporal dimension of two datacubes, so that they can be merged together.\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-resample#resample-operators-with-openeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Resample","lvl3":"resample_cube_spatial: spatial resampling Sentinel-2 to match Landsat","lvl2":"Resample Operators with openEO"},"type":"lvl3","url":"/lectures/data-access/exercises/openeo/openeo-data-access-resample#resample-cube-spatial-spatial-resampling-sentinel-2-to-match-landsat","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing OpenEO Resample","lvl3":"resample_cube_spatial: spatial resampling Sentinel-2 to match Landsat","lvl2":"Resample Operators with openEO"},"content":"\n\nStart importing the necessary libraries and initialize a local connection for Client-Side Processing.\n\nimport openeo\nfrom openeo.local import LocalConnection\nlocal_conn = LocalConnection('')\n\n\n\nCreate two datacubes, one for Sentinel-2 and one for Landsat\n\nspatial_extent = {\"west\": 11.4, \"east\": 11.42, \"south\": 45.5, \"north\": 45.52}\ntemporal_extent = [\"2023-06-01\", \"2023-06-30\"]\n\n\n\nDatacube for Sentinel-2\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\nbands = [\"red\",\"nir\"]\ns2_cube = local_conn.load_stac(url=url,\n   spatial_extent=spatial_extent,\n   temporal_extent=temporal_extent,\n   bands=bands\n)\ns2_cube.execute()\n\n\n\nDatacube for Landsat\n\nurl = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/landsat-c2-l2\"\nbands = [\"red\",\"nir08\"]\nl8_cube = local_conn.load_stac(url=url,\n                    spatial_extent=spatial_extent,\n                    temporal_extent=temporal_extent,\n                    bands=bands)\nl8_cube.execute()\n\n\n\nFrom the previous outputs, notice the shape difference in the spatial dimensions x and y.\n\nThis is due to the different resolution of the two collections: 10m for Sentinel-2, 30m for Landsat.\n\nNow we use the resample_cube_spatial process to resample the Sentinel-2 data to match Landsat.\n\ns2_cube_30m = s2_cube.resample_cube_spatial(target=l8_cube,method=\"average\")\ns2_cube_30m\n\n\n\nCheck what happens to the datacube inspecting the resulting xArray object. Now the x and y shape is the same as Landsat:\n\ns2_cube_30m.execute()\n\n","type":"content","url":"/lectures/data-access/exercises/openeo/openeo-data-access-resample#resample-cube-spatial-spatial-resampling-sentinel-2-to-match-landsat","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate"},"type":"lvl1","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate","lvl2":"Aggregate Operators with Pangeo"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate#aggregate-operators-with-pangeo","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate","lvl2":"Aggregate Operators with Pangeo"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate#aggregate-operators-with-pangeo","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate","lvl3":"resample: temporal aggregation with predefined intervals","lvl2":"Aggregate Operators with Pangeo"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate#resample-temporal-aggregation-with-predefined-intervals","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate","lvl3":"resample: temporal aggregation with predefined intervals","lvl2":"Aggregate Operators with Pangeo"},"content":"\n\nWe start by creating the shared folders and data files needed to complete the exercise using the following shell commands\n\nStart importing the necessary libraries.\n\nimport pystac_client\nimport stackstac\nimport rioxarray\n\n\n\nDefine the necessary parameters\n\nspatial_extent = [11.4, 45.5, 11.42, 45.52]\ntemporal_extent = [\"2020-01-01\", \"2020-12-31\"]\nbands = [\"red\",\"green\",\"blue\"]\nproperties = {\"eo:cloud_cover\": dict(lt=15)}\n\n\n\nQuery the STAC Catalog to get the corresponding STAC Items for Sentinel-2\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    collections=[\"sentinel-2-l2a\"],\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    query=properties\n).item_collection()\n\n\n\nCreate the starting Sentinel-2 datacube:\n\ns2_cube = stackstac.stack(items,\n                     bounds_latlon=spatial_extent,\n                     assets=bands\n)\ns2_cube\n\n\n\nWe might be interested in aggregating our data over periods like week, month, year etc., defining what operation to use to combine the data available in the chosen period.\n\nUsing resample with a sampling frequency (e.g. ‚Äò1MS‚Äô ) to specify how to resample the data, we can achieve this easily:\n\ns2_monthly_min = s2_cube.resample(time=\"1MS\").min(dim=\"time\")\n\n\n\nCheck what happens to the datacube inspecting the resulting Xarray object. Now the time dimension has 12 labels, one for each month.\n\ns2_monthly_min\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate#resample-temporal-aggregation-with-predefined-intervals","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate","lvl3":"Spatial aggregation over an Area of Interest","lvl2":"Aggregate Operators with Pangeo"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate#spatial-aggregation-over-an-area-of-interest","position":6},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Aggregate","lvl3":"Spatial aggregation over an Area of Interest","lvl2":"Aggregate Operators with Pangeo"},"content":"One of the basic concepts in GIS is to clip data using a vector geometry. Xarray is not directly capable of dealing with vectors but thanks to Rioxarray it can be easily achieved. Rioxarray extends Xarray with most of the features that Rasterio (GDAL) brings.\n\nLet‚Äôs first define the area of interest. It is defined in a geojson file which we can read with geopandas.\n\nimport geopandas as gpd\n\n\n\nAOI = gpd.read_file('region.geojson')\n\n\n\nAOI.geometry\n\n\n\nAOI.plot()\n\n\n\nepsg = s2_cube[\"proj:epsg\"].values\n\n\n\nWe reproject our AOI to the same coordinate Reference System than our Sentinel-2 datacube and we clip the data with the polygon that has been obtained through geopandas at the beginning of the notebook.\n\ns2_clipped = s2_cube.rio.clip(AOI.to_crs(epsg=epsg).geometry, crs=epsg)\ns2_clipped\n\n\n\ns2_clipped.isel(time=0).isel(band=0).plot()\n\n\n\nWe finally perform the spatial aggregation, taking the average over the remaining pixels:\n\nregion_mean_xr = s2_clipped.mean((\"x\", \"y\"))\nregion_mean_xr\n\n\n\nWe can compute the result and plot the resulting time series of values for a sample band:\n\nregion_mean_xr = region_mean_xr.loc[dict(band=\"red\")].compute()\nregion_mean_xr.plot()\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-aggregate#spatial-aggregation-over-an-area-of-interest","position":7},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply"},"type":"lvl1","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply","lvl2":"Apply Operator with Pangeo ecosystem"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply#apply-operator-with-pangeo-ecosystem","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply","lvl2":"Apply Operator with Pangeo ecosystem"},"content":"The apply operator applies a process to each value in the data cube (i.e. a local operation).\n\nLet‚Äôs start again with the same sample data from the Sentinel-2 STAC Collection, applying the filters directly in the stackstac.stack call, to reduce the amount of data.\n\nimport pystac_client\nimport stackstac\nimport xarray as xr\n\n\n\n#                  West,     South,     East,      North\nspatial_extent = [11.259613, 46.461019, 11.406212, 46.522237]\ntemporal_extent = ['2022-07-10T00:00:00Z','2022-07-13T00:00:00Z']\nbands = [\"red\",\"green\",\"blue\"]\n\n\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\ns2_items = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    query=[\"eo:cloud_cover<50\"],\n    collections=[\"sentinel-2-l2a\"]\n).item_collection()\n\ns2_cube = stackstac.stack(s2_items,\n                     bounds_latlon=spatial_extent,\n                     assets=bands\n)\ns2_cube\n\n\n\nVisualize the RGB bands of our sample dataset:\n\ns2_cube.isel(time=0).plot.imshow()\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply#apply-operator-with-pangeo-ecosystem","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply","lvl3":"Apply an arithmetic formula","lvl2":"Apply Operator with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply#apply-an-arithmetic-formula","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply","lvl3":"Apply an arithmetic formula","lvl2":"Apply Operator with Pangeo ecosystem"},"content":"\n\nWe would like to improve the previous visualization, rescaling all the pixels between 0 and 1.\n\nWe can use apply with an ad-hoc rescale function.\n\ninput_min = -0.1\ninput_max = 0.2\noutput_min = 0\noutput_max = 1\n\n\ndef rescale(arr):\n    norm_arr = arr.clip(min=input_min, max=input_max)\n    norm_arr = ((norm_arr - input_min) / (input_max - input_min)) * (output_max - output_min) + output_min\n    return norm_arr\n\nscaled_data = s2_cube.to_dataset().apply(rescale)\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply#apply-an-arithmetic-formula","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply","lvl2":"Visualise the scaled image"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply#visualise-the-scaled-image","position":6},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Apply","lvl2":"Visualise the scaled image"},"content":"\n\nVisualize the result and see how apply scaled the data resulting in a more meaningful visualization:\n\nscaled_data.to_dataarray().squeeze().plot.imshow()\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-apply#visualise-the-scaled-image","position":7},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter"},"type":"lvl1","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#filter-operators-with-pangeo-ecosystem","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"content":"When interacting with large data collections, it is necessary to keep in mind that it‚Äôs not possible to load everything!\n\nTherefore, we always have to define our requirements in advance and apply them to the data using filter operators.\n\nLet‚Äôs start again with the same sample data from the Sentinel-2 STAC Collection with an additional filter.\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#filter-operators-with-pangeo-ecosystem","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Properties Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#properties-filter","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Properties Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"content":"When working with optical data like Sentinel-2, most of the times we would like to discard cloudy acquisitions as soon as possible.\n\nWe can do it using a property filter: in this case we want to keep only the acquisitions with less than 50% cloud coverage.\n\nimport pystac_client\nimport stackstac\nimport numpy as np\nfrom pyproj import Proj\n\n\n\nproperties = {\"eo:cloud_cover\": dict(lt=50)}\n\n\n\nspatial_extent = [11.1, 46.1, 11.5, 46.5]\ntemporal_extent = [\"2024-01-01T00:00:00Z\",\"2024-10-30T00:00:00Z\"]\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    collections=[\"sentinel-2-l2a\"],\n    query=properties\n).item_collection()\n\ndatacube = stackstac.stack(items,\n                     bounds_latlon=spatial_extent,\n)\ndatacube\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#properties-filter","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Temporal Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#temporal-filter","position":6},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Temporal Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"content":"\n\nTo filter along time the data collection with Pangeo Xarray, we create a binary mask combining two conditions, passing it to the where method from Xarray. Remember to set drop=True to discard the dates outside the selected period.\n\nNote: We cannot use the sel method to select a slice to filter on the time dimension since the times are not equally spaced.\n\ntemporal_slice = datacube.where((datacube.time>=np.datetime64(\"2024-05-10T00:00:00\")) &\n                                (datacube.time<=np.datetime64(\"2024-06-30T00:00:00\")),\n                                drop=True)\ntemporal_slice\n\n\n\nAfter running the previous cell, it is visible that the result has less elements (or labels) in the temporal dimension time.\n\nAdditionally, the size of the selected data reduced a lot.\n\nQuiz hint: look carefully at the dimensions of the resulting datacube!\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#temporal-filter","position":7},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Spatial Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#spatial-filter","position":8},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Spatial Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"content":"\n\nTo slice along the spatial dimensions the data collection with Pangeo, we can use sel method with a slice.\n\nLet‚Äôs get the spatial extent expressed in the Coordinate Reference System of the datacube. We need to project latitudes, longitudes to the data cube CRS.\n\nwest = 11.259613; east = 11.406212\nsouth = 46.461019; north = 46.522237\nprojection = Proj(temporal_slice.attrs[\"crs\"])\nx_ws, y_ws = projection(west, south)\nx_wn, y_wn = projection(west, north)\nx_es, y_es = projection(east, south)\nx_en, y_en = projection(east, north)\nxmax = max(x_ws, x_wn, x_es, x_en)\nxmin = min(x_ws, x_wn, x_es, x_en)\nymax = max(y_ws, y_wn, y_es, y_en)\nymin = min(y_ws, y_wn, y_es, y_en)\nprint(xmin, xmax, ymin, ymax)\n\n\n\nThe sel method with slice is used with a set of coordinates.\n\nNote: The order of the interval in the slice needs to be expressed in the same order as the corresponding coordinate.\n\nspatial_slice = temporal_slice.sel(x=slice(xmin,xmax), y = slice(ymax,ymin))\nspatial_slice\n\n\n\nQuiz hint: look carefully at the dimensions of the loaded datacube!\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#spatial-filter","position":9},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Bands Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#bands-filter","position":10},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Filter","lvl3":"Bands Filter","lvl2":"Filter Operators with Pangeo ecosystem"},"content":"\n\nTo slice along the bands dimension, keeping only the necessary bands, we can use the sel method and isin.\n\nbands = [\"red\", \"green\", \"blue\"]\nbands_slice = spatial_slice.sel(band=spatial_slice.band.isin(bands))\nbands_slice\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-filter#bands-filter","position":11},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Lazy Loading"},"type":"lvl1","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-lazy-loading","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Lazy Loading"},"content":"\n\n\n\nThe exercise will use the Pangeo ecosystem to access and process data.\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-lazy-loading","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Lazy Loading","lvl2":"Lazy data loading with Pangeo ecosystem"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-lazy-loading#lazy-data-loading-with-pangeo-ecosystem","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Lazy Loading","lvl2":"Lazy data loading with Pangeo ecosystem"},"content":"When accessing data using an API, most of the time the data is lazily loaded.\n\nIt means that only the metadata is loaded, so that it is possible to know about the data dimensions and their extents (spatial and temporal), the available bands and other additional information.\n\nLet‚Äôs start with a call to the STAC Catalogue Python Libraries pystac_client for lazily loading some Sentinel-2 data from a public STAC Collection.\n\nWe need to specify an Area Of Interest (AOI) to get only part of the Collection, otherwise our code would try to load the metadata of all Sentinel-2 tiles available in the world!\n\nimport pystac_client\nimport stackstac\n\n\n\n#                West, South, East, North\nspatial_extent = [11.1, 46.1, 11.5, 46.5]\ntemporal_extent = [\"2015-01-01\",\"2022-01-01\"]\n\n\n\nRunning this cell may take up to 2 minutes\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    collections=[\"sentinel-2-l2a\"]\n).item_collection()\n\n\n\nCalling  stackstac.stack() method for the items, the data will be lazily loaded and an xArray.DataArray object returned.\n\nRunning the next cell will show the selected data content with the dimension names and their extent.\n\ndatacube = stackstac.stack(items, bounds_latlon=spatial_extent)\ndatacube\n\n\n\nFrom the output of the previous cell you can notice something really interesting: the size of the selected data is more than 3 TB!\n\nBut you should have noticed that it was too quick to download this huge amount of data.\n\nThis is what lazy loading allows: getting all the information about the data in a quick manner without having to access and download all the available files.\n\nQuiz hint: look carefully at the dimensions of the loaded datacube!","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-lazy-loading#lazy-data-loading-with-pangeo-ecosystem","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce"},"type":"lvl1","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl2":"Reduce Operators with Pangeo ecosystem"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#reduce-operators-with-pangeo-ecosystem","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl2":"Reduce Operators with Pangeo ecosystem"},"content":"When computing statistics over time or indices based on multiple bands, it is possible to use reduce operators.\n\nIn Pangeo and Xarray we can use different methods for reducing the dimensions, such as median, mean or groupby, which applies a reducer to a data cube dimension by collapsing all the values along the specified dimension into an output value computed by the reducer.\n\nReduce the temporal dimension to a single value, the mean for instance:\n\nimport pystac_client\nimport stackstac\nimport xarray as xr\nimport numpy as np\n\n\n\n#                  West,     South,     East,      North\nspatial_extent = [11.259613, 46.461019, 11.406212, 46.522237]\ntemporal_extent = [\"2021-05-28T00:00:00Z\",\"2021-06-30T00:00:00Z\"]\n\n\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\ns2_items = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    collections=[\"sentinel-2-l2a\"]\n).item_collection()\n\ns2_cube = stackstac.stack(s2_items,\n                     bounds_latlon=spatial_extent,\n                     assets=[\"red\",\"nir\"]\n)\ns2_cube\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#reduce-operators-with-pangeo-ecosystem","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl3":"Get rid of possible negative values","lvl2":"Reduce Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#get-rid-of-possible-negative-values","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl3":"Get rid of possible negative values","lvl2":"Reduce Operators with Pangeo ecosystem"},"content":"\n\ns2_cube = xr.where(s2_cube>=0, s2_cube, 0)\ns2_cube\n\n\n\nReduce the time dimension by averaging along the time dimension.\n\nWe can use the Xarray \n\nreduce method, passing the reducer function and the dimension to be reduced:\n\ndatacube_mean_time = s2_cube.reduce(np.mean,dim=\"time\")\n\n\n\nAlternatively, Xarray define convenience methods, allowing to call the reduce operation over time easily with the following syntax, producing the same result. Check what happens to the datacube inspecting the resulting Xarray object.\n\ndatacube_mean_time = s2_cube.mean(\"time\")\ndatacube_mean_time\n\n\n\nIt is possible to reduce in the same way all the available dimensions of the datacube.\n\nWe can, for instance, reduce the band dimension similarly as we did for the temporal dimension:\n\nThe result will now contain values resulting from the average of the bands:\n\ndatacube_mean_band = s2_cube.mean(\"band\")\ndatacube_mean_band\n\n\n\nQuiz hint: look carefully at number of pixels of the loaded datacube!\n\nThe reducer could be again a single process, but when computing spectral indices like NDVI, NDSI etc. an arithmentical formula is used instead.\n\nFor instance, the \n\nNDVI formula can be expressed using a selection (sel) method over the band dimension:NDVI = {{NIR - RED} \\over {NIR + RED}}\n\ndef NDVI(data):\n    red = data.sel(band=\"red\")\n    nir = data.sel(band=\"nir\")\n    ndvi = (nir - red)/(nir + red)\n    return ndvi\n\nndvi_xr = NDVI(s2_cube)\nndvi_xr\n\n\n\nVisualize a sample NDVI result:\n\n%%time\nndvi_xr.isel(time=0).plot.imshow(vmin=-1,vmax=1,cmap=\"Greens\")\n\n\n\nAdditionally, it is possible to reduce both spatial dimensions of the datacube at the same time.\n\nTo do this, we need to reduce the spatial dimension using for instance median.\n\nThis time we select a smaller area of interest, to reduce the amount of downloaded data:\n\n#                  West,     South,     East,      North\nspatial_extent = [11.31369, 46.52167, 11.31906, 46.52425]\ntemporal_extent = [\"2021-01-01T00:00:00Z\",\"2021-12-30T00:00:00Z\"]\nbands = [\"red\",\"nir\"]\n\n\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\ns2_items = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    collections=[\"sentinel-2-l2a\"]\n).item_collection()\n\ns2_cube = stackstac.stack(s2_items,\n                     bounds_latlon=spatial_extent,\n                     assets=bands\n)\ns2_cube = s2_cube[s2_cube[\"eo:cloud_cover\"] < 35]\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#get-rid-of-possible-negative-values","position":5},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl3":"Get rid of possible negative values","lvl2":"Reduce Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#get-rid-of-possible-negative-values-1","position":6},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl3":"Get rid of possible negative values","lvl2":"Reduce Operators with Pangeo ecosystem"},"content":"\n\ns2_cube = xr.where(s2_cube>=0, s2_cube, 0)\n\n\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#get-rid-of-possible-negative-values-1","position":7},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl3":"Reduce dimension x and y with median values","lvl2":"Reduce Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#reduce-dimension-x-and-y-with-median-values","position":8},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Reduce","lvl3":"Reduce dimension x and y with median values","lvl2":"Reduce Operators with Pangeo ecosystem"},"content":"\n\ndatacube_spatial_median = s2_cube.median(dim=[\"x\", \"y\"])\n\n\n\nVerify that the spatial dimensions were collapsed:\n\ndatacube_spatial_median\n\n\n\nWe can combine this spatial reducer with the previous over bands to compute a time series of NDVI values:\n\nndvi_spatial_xr = NDVI(datacube_spatial_median)\nndvi_spatial_xr\n\n\n\nndvi_spatial_xr = ndvi_spatial_xr.compute()\nndvi_spatial_xr\n\n\n\nQuiz hint: look carefully at the dimensions of the resulting datacube!\n\ncompute() executes the Dask computation and blocks until the result is available. It then collects and returns the final result to the local process.\nIn this case it will trigger the download of the data from the STAC Catalog and the computing the NDVI time series.\n\nVisualize the NDVI time series:\n\nndvi_spatial_xr.where(ndvi_spatial_xr<1).plot.scatter()\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-reduce#reduce-dimension-x-and-y-with-median-values","position":9},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Resample"},"type":"lvl1","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-resample","position":0},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Resample"},"content":"\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-resample","position":1},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Resample","lvl2":"Resample Operators with Pangeo ecosystem"},"type":"lvl2","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-resample#resample-operators-with-pangeo-ecosystem","position":2},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Resample","lvl2":"Resample Operators with Pangeo ecosystem"},"content":"Sometimes we need to align the spatial or temporal dimension of two datacubes, so that they can be merged together.\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-resample#resample-operators-with-pangeo-ecosystem","position":3},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Resample","lvl3":"Spatial resampling Sentinel-2 to match Landsat","lvl2":"Resample Operators with Pangeo ecosystem"},"type":"lvl3","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-resample#spatial-resampling-sentinel-2-to-match-landsat","position":4},{"hierarchy":{"lvl1":"2.3 Data Access and Basic Processing Pangeo Resample","lvl3":"Spatial resampling Sentinel-2 to match Landsat","lvl2":"Resample Operators with Pangeo ecosystem"},"content":"\n\nStart importing the necessary libraries\n\nimport pystac_client\nimport stackstac\nfrom rasterio.enums import Resampling\n\n\n\nCreate two datacubes, one for Sentinel-2 and one for Landsat\n\nspatial_extent = [11.4, 45.5, 11.42, 45.52]\ntemporal_extent = [\"2023-06-01\", \"2023-06-30\"]\n\n\n\nDatacube for Sentinel-2\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\ns2_items = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    collections=[\"sentinel-2-l2a\"]\n).item_collection()\n\ns2_cube = stackstac.stack(s2_items,\n                     bounds_latlon=spatial_extent,\n                     assets=[\"red\",\"nir\"]\n)\ns2_cube\n\n\n\nDatacube for Landsat\n\nurl = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\ncatalog = pystac_client.Client.open(URL)\nl8_items = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    collections=[\"landsat-c2-l2\"]\n).item_collection()\n\nl8_cube = stackstac.stack(l8_items,\n                     bounds_latlon=spatial_extent,\n                     assets=[\"red\",\"nir08\"]\n)\nl8_cube\n\n\n\nFrom the previous outputs, notice the shape difference in the spatial dimensions x and y.\n\nThis is due to the different resolution of the two collections: 10m for Sentinel-2, 30m for Landsat.\n\nNow we use the stackstac.stack to load and resample the Sentinel-2 data to match Landsat.\n\ns2_cube_coarse = stackstac.stack(s2_items,\n                                 bounds_latlon=spatial_extent,\n                                 assets=[\"red\",\"nir\"],\n                                 resolution=30.0, resampling=Resampling.average, epsg=32632)\ns2_cube_coarse\n\n","type":"content","url":"/lectures/data-access/exercises/pangeo/pangeo-data-access-resample#spatial-resampling-sentinel-2-to-match-landsat","position":5},{"hierarchy":{"lvl1":"Data Formats and Performance"},"type":"lvl1","url":"/lectures/formats-and-performance/formats-and-performance","position":0},{"hierarchy":{"lvl1":"Data Formats and Performance"},"content":"","type":"content","url":"/lectures/formats-and-performance/formats-and-performance","position":1},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/formats-and-performance/formats-and-performance#learning-objectives","position":2},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Learning Objectives"},"content":"Understand what cloud native and cloud optimized data formats are\n\nUnderstand how the cloud does computing more efficiently\n\nUnderstand chunking\n\nUnderstand how chunking impact performance","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#learning-objectives","position":3},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Outline"},"type":"lvl2","url":"/lectures/formats-and-performance/formats-and-performance#outline","position":4},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Outline"},"content":"Cloud Native and Cloud Optimized Data Formats\n\nWhy do we need them\n\nWhat is their advantage over classical file formats\n\nNot one format to rule them all.\n\nExamples\n\nPerformance in the cloud\n\nTiling\n\nChunking\n\nCost of Scalability\n\nDistributed Computing","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#outline","position":5},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl2","url":"/lectures/formats-and-performance/formats-and-performance#cloud-native-and-cloud-optimized-data-formats","position":6},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#cloud-native-and-cloud-optimized-data-formats","position":7},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Why do we need them","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#why-do-we-need-them","position":8},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Why do we need them","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"Cloud native and cloud optimized formats have been specifically designed to optimize the storage, access and processing in cloud computing environments. The main difference between cloud optimized and cloud native comes from their origin: the first are an optimized version of an existing file format whereas the latter have been designed to be efficient for a cloud usage from the beginning. These formats are tailored to leverage the scalability, flexibility, and parallel processing capabilities of a cloud infrastructure, enabling efficient handling of large-scale datasets.\n\n \n\nVideo content in cooperation with \n\nAimee Barciauskas (DevelopmentSeed) and \n\nRyan Avery (DevelopmentSeed). \n‚ÄúCloud-optimized means organizing so subsets of data can be read. Ideally, the data is also compressed. Both of these factors minimize the amount of data that has to be transferred across a network.‚Äù","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#why-do-we-need-them","position":9},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#characteristics","position":10},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"Cloud native and cloud optimized means mainly optimized read access, and more specifically partial and parallel reads capabilities. The main common characteristics are listed below.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#characteristics","position":11},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Data Chunking","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl4","url":"/lectures/formats-and-performance/formats-and-performance#data-chunking","position":12},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Data Chunking","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"When working with large data files or collections, it‚Äôs often impossible to load all the data into a single computer‚Äôs memory at once. In such cases, a data chunking approach can be highly effective. By dividing the dataset into smaller chunks, the data can be processed piece by piece without exceeding the computer‚Äôs memory capacity. This approach is particularly useful for managing large datasets on a single machine and can also scale to distributed computing environments, such as cloud platforms or high-performance computing systems.\n\nThe chunk is the smallest atomic unit of a larger dataset that can be processed independently, enabling efficient data handling by dividing the dataset into manageable pieces enabling parallel processing and efficient retrieval of specific portions of the data, reducing the need to access the entire dataset.\n\nThe figure below visually explains the concept of data chunking: on the left, a three-dimensional data cube (x, y, and time) is shown without chunks, while on the right, the same data cube is displayed with chunks highlighted.\n\nData Cube without chunking\n\nData Cube with chunking\n\n\n\n\n\nFigure: on the left, a 3D data cube without any chunking strategy applied. On the right, a 3D data cube with box chunking.\n\nThere are different ways to chunk data, depending on the nature of the dataset and the analysis requirements.\n\nSpatial chunking divides data based on geographical or spatial dimensions (e.g., longitude, latitude), which is ideal for geospatial datasets where the data is naturally distributed across space.\n\nTime-based chunking focuses on temporal dimensions (e.g., by day, month, or year), which is suitable for time-series data.\n\nBox chunking divides the data into fixed-size blocks (e.g., cubes or boxes), providing a balance between spatial and time-based chunking.\n\nThe choice of chunking strategy can significantly impact the efficiency of data access. Spatial chunking is optimal for spatial queries, while time-based chunking improves access to time-series data. Using the right chunking strategy can reduce the computational overhead and improve the overall performance of data processing tasks.\n\nThe image below illustrates the two most current chunking strategies:\n\nSpatial chunking strategy\n\nBox chunking strategy\n\n\n\n\n\nFigure: on the left, a 3D data cube with spatial chunking. On the right, a 3D data cube with box chunking.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#data-chunking","position":13},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Data Tiling","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl4","url":"/lectures/formats-and-performance/formats-and-performance#data-tiling","position":14},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Data Tiling","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"Tiling strategies are used to divide the data into smaller, manageable tiles that can be independently accessed and processed.\nData tiling is similar to chunking, but specifically suited for Raster images, web maps, and tiled datasets (e.g., Web Map Tiles, Cloud-Optimized GeoTIFFs). It divides spatial data into smaller, regularly shaped regions (usually squares or rectangles) to optimize visualization and retrieval. It allows efficient loading and rendering of specific map sections without loading the entire dataset and allows access by geographic location (e.g., XYZ tiles in a web map).","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#data-tiling","position":15},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Internal Indexing","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl4","url":"/lectures/formats-and-performance/formats-and-performance#internal-indexing","position":16},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Internal Indexing","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"These formats incorporate internal indexing structures that facilitate fast spatial and attribute queries. This enables efficient data access and retrieval operations without the need for extensive scanning or processing of the entire dataset.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#internal-indexing","position":17},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Metadata Optimization","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl4","url":"/lectures/formats-and-performance/formats-and-performance#metadata-optimization","position":18},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Metadata Optimization","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"The metadata is optimized for storage and indexing, allowing for efficient access and retrieval of metadata associated with the data at once. This supports faster discovery and interpretation of data properties and characteristics.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#metadata-optimization","position":19},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Compression","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl4","url":"/lectures/formats-and-performance/formats-and-performance#compression","position":20},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl4":"Compression","lvl3":"Characteristics","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"Advanced compression techniques are often applied to reduce storage requirements while maintaining data quality.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#compression","position":21},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Examples of cloud native and cloud optimized raster and multi-dimensional array formats","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#examples-of-cloud-native-and-cloud-optimized-raster-and-multi-dimensional-array-formats","position":22},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Examples of cloud native and cloud optimized raster and multi-dimensional array formats","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"Zarr is a format specifically designed for storing and accessing multidimensional arrays. It supports chunking, compression, and parallel processing, making it suitable for large-scale geospatial datasets, for example, weather data. Metadata is stored externally in data files itself. Tt can be considered the cloud evolution of netCDF, which is instead optimized for local/HPC storage.\n\nCloud-Optimized GeoTIFF (COG) is an optimized version of the GeoTIFF format. It organizes raster data into chunks, utilizes internal tiling and compression, and uses HTTP range requests for efficient data access in the cloud. HTTP Range Request allows clients to request only a specific portion or range of data instead of a complete dataset.\n\nTileDB is a multi-dimensional, cloud-first format that supports array-based geospatial data.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#examples-of-cloud-native-and-cloud-optimized-raster-and-multi-dimensional-array-formats","position":23},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Examples of cloud native and cloud optimized vector formats","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#examples-of-cloud-native-and-cloud-optimized-vector-formats","position":24},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Examples of cloud native and cloud optimized vector formats","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"FlatGeoBuf is a cloud optimized vector data format. It is a binary encoding format for geodata and holds a collection of Simple Features.\n\nGeoParquet is a columnar, optimized tabular geospatial format leveraging Parquet, allowing efficient querying in cloud-based analytics.\n\nThese two formats can be used replace the Shapefile (SHP) format, which is a widely used but outdated vector data format that stores geospatial features in multiple files (.shp, .dbf, .shx). While still supported in many GIS applications, it has limitations such as file size restrictions (4GB limit), lack of support for modern cloud storage, and inefficiency in querying large datasets.\n\nConnect the cloud native data format and it‚Äôs predecessor to it‚Äôs spatial data type:","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#examples-of-cloud-native-and-cloud-optimized-vector-formats","position":25},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Available Material","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#available-material","position":26},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Available Material","lvl2":"Cloud Native and Cloud Optimized Data Formats"},"content":"Ryan Avery, Aimee Barciauskas, Development Seed, United States (2023). Technologies used to Create, Store and Access Geospatial Data in the Cloud. \n\nhttps://‚Äã2023‚Äã.ieeeigarss‚Äã.org‚Äã/view‚Äã_paper‚Äã.php‚Äã?PaperNum‚Äã=‚Äã5306\n\nESIP Talk on Cloud Native Formats: \n\nhttps://‚Äãwww‚Äã.youtube‚Äã.com‚Äã/watch‚Äã?v‚Äã=‚Äãac‚Äã_UKunUrNM\n\nFOSS4G Talk On Cloud Native Formats (Matthew Hanson)\n\nhttps://‚Äãtalks‚Äã.osgeo‚Äã.org‚Äã/foss4g‚Äã-2023‚Äã/talk‚Äã/XBHYF9/\n\nhttps://‚Äãyoutu‚Äã.be‚Äã/g7iMO1KmyJE‚Äã?si‚Äã=‚ÄãerNwG1VzwUIQR3t5\n\nOGC White Paper on Cloud Native Formats (Chris Holmes, Scott Simmons):\n\nhttps://‚Äãwww‚Äã.ogc‚Äã.org‚Äã/blog‚Äã-article‚Äã/towards‚Äã-a‚Äã-cloud‚Äã-native‚Äã-ogc/\n\nhttps://‚Äãwww‚Äã.ogc‚Äã.org‚Äã/blog‚Äã-article‚Äã/towards‚Äã-a‚Äã-cloud‚Äã-native‚Äã-geospatial‚Äã-standards‚Äã-baseline/\n\nhttps://‚Äãwww‚Äã.ogc‚Äã.org‚Äã/blog‚Äã-article‚Äã/the‚Äã-latest‚Äã-on‚Äã-cloud‚Äã-native‚Äã-geospatial‚Äã-standards‚Äã-in‚Äã-ogc/\n\nCloud-Native Geospatial Foundation initiative of Radiant Earth:\n\nhttps://‚Äãcloudnativegeo‚Äã.org","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#available-material","position":27},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Scaling"},"type":"lvl2","url":"/lectures/formats-and-performance/formats-and-performance#scaling","position":28},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Scaling"},"content":"Scaling refers to the process of increasing or decreasing the capacity or size of a system to handle a larger or smaller workload or data volume. Scaling does not necessarily means only in the direction of larger and bigger but also saving unnecessary costs in times when there is no traffic. In our context, scaling involves managing the growth of data, traffic, or processing requirements to ensure optimal performance and availability.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#scaling","position":29},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Horizontal vs vertical scaling","lvl2":"Scaling"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#horizontal-vs-vertical-scaling","position":30},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Horizontal vs vertical scaling","lvl2":"Scaling"},"content":"Two classical approaches to scaling are horizontal and vertical:\n\nHorizontal scaling: Also known as scaling out, horizontal scaling involves adding more machines or nodes to distribute the workload across a larger number of resources. This could mean adding more servers or instances to handle increased traffic or data processing demands. Horizontal scaling offers the advantage of improved fault tolerance and increased capacity, as the workload is spread across multiple resources.\n\nVertical scaling: Also known as scaling up, vertical scaling involves increasing the capacity of an individual machine or resource. This can be achieved by upgrading the hardware, such as adding more powerful processors, memory, or storage, to handle the growing demands of the geospatial application. Vertical scaling is often suitable for applications with single-node architectures or when the workload cannot be easily distributed across multiple machines.\n\nBoth horizontal and vertical scaling have their advantages and considerations. Horizontal scaling provides better scalability and fault tolerance, as it can handle increased traffic and processing by adding more resources. However, it may require additional effort to distribute and synchronize data across multiple nodes. Vertical scaling, on the other hand, simplifies data management as all resources are contained within a single node, but it may have limitations in terms of the maximum capacity a single machine can handle.\n\nIn common workflows, a combination of both approaches is used to ensure optimal speed and resource utilization while being able to keep the simplicity of a workflow.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#horizontal-vs-vertical-scaling","position":31},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"How to scale","lvl2":"Scaling"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#how-to-scale","position":32},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"How to scale","lvl2":"Scaling"},"content":"There are many approaches how to handle scaling properly and we will use two excercises to experiment Vertical scaling and Horizontal scaling using the Dask library, together with chunking strategies.\n\nExercise 2.4 chunking\n\nExercise 2.4 dask","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#how-to-scale","position":33},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Cost of scalability","lvl2":"Scaling"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#cost-of-scalability","position":34},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Cost of scalability","lvl2":"Scaling"},"content":"Executing code, whether on your own computer or a cloud platform, comes with costs‚Äîboth financial and environmental. Every computation step consumes resources and energy. While simple computations like sorting a small list are quick and resource-light, more complex workflows, such as training a machine learning model or processing large datasets, can be resource-intensive.\n\nThese resource-intensive workflows not only require significant energy but also take considerable time. Saving just a few seconds in a small workflow might seem insignificant, but in large-scale computations, those seconds can translate into hours or even days saved.\n\nIt‚Äôs important to measure and understand the resource consumption of your workflows to minimize their environmental impact. Tools like Codecarbon can estimate the energy use of your code, providing a starting point for optimization. You can read more about the tool \n\nonline or on \n\nGitHub.\nNo tool is perfect, so interpreting results critically and understanding their limitations is essential. However, using such tools can help raise awareness and guide improvements.\n\nOn cloud platforms, optimizing resource usage doesn‚Äôt just reduce your carbon footprint‚Äîit can also save you money.\n\nAs you design and execute workflows, think about how you can reduce complexity, reuse existing resources, and streamline processes. By doing so, you‚Äôre contributing to a more sustainable approach to computing.\n\nTry now to measure the energy footprint of your workflow in the following exercise!\n\nExercise 2.4 Formats and Performance","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#cost-of-scalability","position":35},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Subscription vs. On-Demand usage","lvl2":"Scaling"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#subscription-vs-on-demand-usage","position":36},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"Subscription vs. On-Demand usage","lvl2":"Scaling"},"content":"Subscription: A subscription model involves a fixed, recurring payment made by the user to access and utilize the cloud platform‚Äôs services over a specific period, typically monthly or annually. Under a subscription model, users typically commit to a predetermined level of resources and pay a regular fee regardless of the actual usage during that period. This model often provides cost predictability and may offer discounts or benefits for long-term commitments. Users can usually choose from various options and combinations of resources (eg. GPU, CPU, disk storage combinations).\n\nAdvantages of the Subscription Model:\n\nCost Predictability: Users have a clear understanding of the ongoing costs as they pay a fixed fee.\n\nPotential Cost Savings: Subscriptions may offer discounts or cost benefits for longer-term commitments.\n\nContinuous Service Access: Users have continuous access to the subscribed services without the need for frequent renewal or payment management.\n\nOn-Demand Usage: In an on-demand model, users pay for the cloud platform‚Äôs services based on actual usage and consumption. Users are charged on a pay-as-you-go basis, where they pay for the resources or services they utilize in a given period. There are no long-term commitments or fixed fees. This model offers flexibility and scalability, allowing users to scale resources up or down as per their needs.\n\nAdvantages of On-Demand Usage:\n\nFlexibility: Users have the flexibility to adjust resources based on their varying demands, scaling up or down as required.\n\nCost Efficiency: Users only pay for what they use, making it suitable for workloads with unpredictable or fluctuating resource needs.\n\nNo Long-Term Commitments: On-demand models do not require users to commit to a specific period or predefined resource levels, allowing for agility and quick adjustments.\n\nChoosing between subscription and on-demand models depends on various factors, including the nature of your workloads, budget considerations, and usage patterns. Based on this (and data availability) users can choose a platform that suits their needs best. Reviewing the pricing details is an important part before selecting a working environment.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#subscription-vs-on-demand-usage","position":37},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"References","lvl2":"Scaling"},"type":"lvl3","url":"/lectures/formats-and-performance/formats-and-performance#references","position":38},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl3":"References","lvl2":"Scaling"},"content":"codecarbon (2024); Reference: Courty, B., Schmidt, V., Goyal-Kamal, MarionCoutarel, Feld, B., Lecourt, J., LiamConnell, SabAmine, Inimaz, Supatomic, L√©val, M., Blanche, L., Cruveiller, A., Ouminasara, Zhao, F., Joshi, A., Bogroff, A., Saboni, A., De Lavoreille, H., . . . MinervaBooks. (2024). mlco2/codecarbon: v2.4.1. Zenodo. \n\nDOI: https://doi.org/10.5281/zenodo.11171501\n\nDask Development Team (2016). Dask: Library for dynamic task scheduling\nURL \n\nhttp://‚Äãdask‚Äã.pydata‚Äã.org","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#references","position":39},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"What to avoid and what are the limitations"},"type":"lvl2","url":"/lectures/formats-and-performance/formats-and-performance#what-to-avoid-and-what-are-the-limitations","position":40},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"What to avoid and what are the limitations"},"content":"While scaling is providing many options and is essential for achieving results on a larger scale, there are some limitations to keep in mind and activities to even avoid.\n\nCosts: One of the main characteristics to consider are costs of computing. Scaling resources dynamically can lead to increased costs, especially if not properly managed. It is essential to monitor resource usage and set appropriate maximum scaling policies to ensure cost optimization. Failure to do so may result in unnecessary provisioning of resources, leading to higher expenses. The purchase of many computational resources can be easy, but very costly. Code optimization is important to ensure there are no memory leaks, unnecessary data storage, and other expensive operations.\n\nData Access: In geospatial cloud workflows, one of the big challenges lies in data access and optimal data storage. The easy trap is in loading large portions of unnecessary data without applying correct filters ahead. Such data volumes can lead to more requirements on RAM or disk space resulting in higher costs of processing or longer times (and more computational time) just to load the data.\n\nAccessing data as files: Geospatial data are stored in many formats as discussed in this lesson and some are more appropriate to access in the cloud. The opportunity of first evaluating metadata before loading the whole dataset is great for saving time and money.\n\nLatency and Data Transfer: In distributed and scaled-out architectures, managing data transfer and minimizing latency can be crucial. Moving data between services or instances across different locations can introduce network overhead and impact application performance. Efficient data caching, or data partitioning strategies can help mitigate these challenges.\n\nScaling Limits in the platform: While cloud platforms offer high scalability, there are still practical limits to consider. Every service or resource has its scalability limits, such as maximum instance count, storage capacity, or network throughput. It is important to understand these limitations and design your programs and applications accordingly.\n\nTo mitigate these challenges and limitations, it‚Äôs advisable to thoroughly plan and architect your application for scalability, leverage cloud-native tools and services, monitor resource usage and costs, and regularly test and optimize your scaling strategies. Additionally, staying updated with the latest advancements in cloud technologies and best practices will help you navigate the complexities of cloud-native scaling more effectively.","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#what-to-avoid-and-what-are-the-limitations","position":41},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/formats-and-performance/formats-and-performance#quiz","position":42},{"hierarchy":{"lvl1":"Data Formats and Performance","lvl2":"Quiz"},"content":"What are cloud native data formats in EO and GIS[(X)] Cloud native data formats in EO should be compatible to cloud services (APIs, http requests, cloud storage), enable fast viewing and access to spatial sub regions.\n[( )] Cloud native data formats in EO are exclusively designed to be compressed as much as possible, so that the least amount of storage space is necessary.\n[( )] Cloud native data formats in EO have to be human readable when you open them in a text editor.\n\nLazy-loading is essential when working with large data collections, why?[(X)] Lazy-loading improves performance by loading data only when needed, reducing memory usage and speeding up application response times.\n[( )] Lazy-loading automatically compresses all data in large collections, leading to faster data retrieval.\n[( )] Lazy-loading duplicates data across multiple processes, ensuring data availability at all times.\n[(X)] Lazy-loading allows to process data which is larger than memory with appropriate chunking strategies.\n\nWhich of the following is correct when considering time-based chunking?[( )] It divides data into fixed-size cubes or boxes for balanced processing.\n[(X)] It organizes data based on temporal dimensions such as days, months, or years.\n[(X)] It is particularly useful for analyzing trends and patterns over time.\n\nConsider a cloud provider, that should decide if compressing the data on its storage would let it spare money.\n\nIf the compression process of a COG takes 0.05 CPU hour every 1 GB of data, the total amount of COGs on the storage takes 1200 TB and one CPU hour costs 0.05‚Ç¨ for the first 10000 CPU hour and then 0.03‚Ç¨ for the rest, how much would the compression process cost?[( )] 3000‚Ç¨\n[( )] 1200‚Ç¨\n[(X)] 2000‚Ç¨\n[( )] 2043‚Ç¨\n\nSolution:\n\n1 TB = 1000 GB following the international standard, see \n\nGigabyte.\n\nThe amount of data on the storage is 1200 TB * 1000 GB/TB = 1200000 GB\n\nIf to compress 1 GB of data takes 0.05 CPU hour, to compress 1200000 GB it will take: 1200000 GB * 0.05 CPU hour / GB = 60000 CPU hour\n\nThe first 10000 CPU hour cost 10000 CPU hour * 0.05 ‚Ç¨/CPU hour = 500‚Ç¨\n\nThe rest is 50000 CPU hour and it will cost 50000 CPU hour * 0.03 ‚Ç¨/CPU hour = 1500 ‚Ç¨\n\nThe total is 500 ‚Ç¨ + 1500 ‚Ç¨ = 2000 ‚Ç¨\n\nIn the same scenario, now we also consider the storage maintanance cost. If each TB of storage has a maintenance cost of 0.5 ‚Ç¨ every month, how much time would it take before the compression would let the cloud provider spare money? Consider a compression rate of 70%.[( )] ~ 9 months\n[( )] ~ 10 months\n[(X)] ~ 11 months\n[( )] ~ 12 months\n\nSolution:\n\nWithout compression, every month the storage would cost: 1200 TB * 0.5 ‚Ç¨/TB = 600 ‚Ç¨\n\nWith a compression rate of 70%, the data would use 1200 TB * 0.7 = 840 TB of disk space\n\nWith compression, every month the storage would cost: 840 TB * 0.5 ‚Ç¨/TB = 420 ‚Ç¨\n\nWith compression, each month it‚Äôs possible to spare 180 ‚Ç¨.\n\nSince the compression process costs 2000 ‚Ç¨, and each month costs 180 ‚Ç¨ less with it, it would take 2000 ‚Ç¨ / 180 ‚Ç¨/month ~= 11 months to start to spare money.\n\nWhat does CodeCarbon measure in Python code? Answer in exercise: 24_energy_consumption.ipynb[( )] Code execution speed\n[(X)] Energy consumption of code execution\n[( )] Memory usage of variables\n[( )] The number of lines in the code\n\nApproximately how many meters of driving a Mars rover would consume the same amount of energy as our sample NDVI workflow? Answer in exercise: 24_energy_consumption.ipynb[(X)] 0-2 meter\n[( )] 2-4 meters\n[( )] 4-6 meters\n[( )] 6-8 meters","type":"content","url":"/lectures/formats-and-performance/formats-and-performance#quiz","position":43},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo"},"type":"lvl1","url":"/lectures/formats-and-performance/exercises/chunking","position":0},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo"},"content":"\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking","position":1},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Data chunking"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#data-chunking","position":2},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Data chunking"},"content":"\n\nOverview Questions\n\nWhat is chunking and why does it matter?\n\nHow can we utilize chunking to make our processing more efficient?Objectives\n\nExplore chunking of data\n\nLearn about the Zarr file format\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#data-chunking","position":3},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Context"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#context","position":4},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Context"},"content":"As explained in Section 2.4 - Formats and Performance, when working with large data files or collections, it is often impractical to load the entire dataset into the memory of a single computer at once. This is where the Pangeo ecosystem is particularly useful. In Section 2.3 - Data Access, we discussed the concept of lazy loading. Xarray enables lazy processing of data in chunks, meaning the dataset is divided into manageable pieces. By reading and processing the data in these chunks, we can efficiently handle large datasets on a single computer or scale the processing to a distributed computing cluster using Dask (e.g., on the cloud or high-performance computing environments).\n\nHow we process these chunks in a parallel environment to scale our computation vertically is discussed in \n\n2.4 dask. In this notebook, you will explore the concept of chunks through various exercises.\n\nProcessing data piece by piece is more efficient when both our input and output data are also stored in chunks. As introduced in Section 2.4 - Formats and Performance, \n\nZarr is a cloud-native data format and serves as the reference library in the Pangeo ecosystem for storing Xarray multi-dimensional datasets in chunks.","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#context","position":5},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Data"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#data","position":6},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Data"},"content":"We‚Äôll begin with the same sample data retrieval method from the Sentinel-2 STAC collection, as described in Exercise 2.3 - Data Access: Lazy Loading with Pangeo.\n\nThe analysis will be similar to our previous exercises, but this time, we‚Äôll use a larger spatial extent to demonstrate the scalability.\n\nWe start by copying the data files needed to complete the exercise using the following shell commands\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#data","position":7},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Load Libraries"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#load-libraries","position":8},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Load Libraries"},"content":"\n\nimport pystac_client\nimport geopandas as gpd\nfrom shapely.geometry import mapping\nimport stackstac\nimport warnings\nimport xarray as xr\nimport numpy as np\nimport rioxarray as rio\nwarnings.filterwarnings(\"ignore\")\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#load-libraries","position":9},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Load Sentinel-2 data"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#load-sentinel-2-data","position":10},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Load Sentinel-2 data"},"content":"\n\naoi = gpd.read_file('./assets/catchment_outline.geojson', crs=\"EPGS:4326\")\naoi_geojson = mapping(aoi.iloc[0].geometry)\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    intersects=aoi_geojson,\n    collections=[\"sentinel-2-l2a\"],\n    datetime=\"2019-02-01/2019-04-28\",\n    query= {\"proj:epsg\": dict(eq=32632)}\n).item_collection()\nsentinel2_l2a = stackstac.stack(items,assets=[\"red\",\"nir\"])\nsentinel2_l2a\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#load-sentinel-2-data","position":11},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"What is a Chunk?"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#what-is-a-chunk","position":12},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"What is a Chunk?"},"content":"If you closely examine the sentinel2_l2a dataset, you‚Äôll notice that the xarray.DataArray is backed by a dask.array with a chunk size of (1, 1, 1024, 1024). The full dataset consists of arrays with dimensions (102, 2, 20982, 10980), totaling 47,124 chunks, which amounts to 350.16 GiB of data potentially loaded into the computer‚Äôs RAM.\n\nYou can view the dask.array information by clicking the blue-circled icon in the image below.\n\nBy clicking the red-circled triangle icon, you‚Äôll see detailed information about the xarray.DataArray, including its Coordinates, Indexes, and Attributes.\n\nWhen creating an Xarray object using stackstac, we can easily convert a STAC collection into a lazily-loaded, chunked xarray.DataArray backed by Dask.\n\nThe size and shape of the chunks determine the level of parallelization performed by Dask. Therefore, selecting an appropriate chunk size can significantly impact the performance of your computation.\n\nThis is where understanding and effectively using chunking becomes crucial.\n\nIn our case, for the moment, we used stackstac without specifying the ‚Äòchunk‚Äô explicitly. The dataset is composed of 8 MiB chunks, each containing 1 time step, 1 band, and a resolution of 1024 x 1024 in the x and y directions.\n\nIf the chunk size is too small, our workflow will be divided into too many tiny pieces, which can lead to excessive communication and increased distribution overhead.\n\nOn the other hand, if the chunk size is too large, there may not be enough memory available to handle the workload, causing the workflow to fail.\n\nThe optimal chunk size depends on both your computation and the machine you‚Äôre using.\n\nFor example, 8 MiB is relatively small compared to the typical RAM size available. Dask‚Äôs default array chunk size, for instance, is 128 MiB.\n\nimport dask\ndask.config.get('array.chunk-size')\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#what-is-a-chunk","position":13},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Modifying chunks"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#modifying-chunks","position":14},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Modifying chunks"},"content":"Let‚Äôs try to modify our chunk size.\n\nTo modify chunks on your existing xarray.DataArray we can use the chunk method.\nWe know that we only need 2 bands to compute the Normalized Difference Vegetation Index (NDVI) example, so we select only red and nir to simplify our example.\n\nWe would like to have each time series separated in each chunk, then keep all band information on one chunk, and let dask to compute x and y coordinate‚Äôs chunk size.\n\ndef NDVI(data):\n    red = data.sel(band=\"red\")\n    nir = data.sel(band=\"nir\")\n    ndvi = (nir - red)/(nir + red)\n    return ndvi\n\nndvi_xr = NDVI(sentinel2_l2a)\nndvi_xr\n\n\n\nsentinel2_l2a = sentinel2_l2a.sel(\n    band=['red','nir']).chunk(\n    chunks={'time': 1, 'band':2, 'x':'auto','y':'auto'})\nsentinel2_l2a\n\n\n\nIf you look into the details of any variable in the representation above, you‚Äôll see that each x and y coordinate‚Äôs chunk is bigger, and we have less chunks than the example before.\n\nNote here from the chunk size, the auto option computed the optimal chunk size for y and x if we want to keep the chunk size of time and band as 1 and 2 respectively.\n\nGo Further You can try to apply different ways for specifying chunk.\n\nchunks = -1 -> the entire array will be used as a single chunk\n\nchunks = {'x':-1, 'y': 1000} -> chunks of entire _x_ dimension, but splitted every 1000 values on _y_ dimension\n\nchunks = {'x':-1, 'y': 'auto'} -> Xarray relies on Dask to use an ideal size according to the preferred chunk sizes for _y_ dimension\n\nchunks = { 'x':-1 ,'y':\"500MiB\" } -> Xarray seeks the size according to a specific memory target expressed in MiB\n\nchunks = ( 1, 3, 12048,2048) -> Specifying chunk size in the order of dimension.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#modifying-chunks","position":15},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Defining the chunk size at the creation with Xarray"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#defining-the-chunk-size-at-the-creation-with-xarray","position":16},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Defining the chunk size at the creation with Xarray"},"content":"We can define the chunk size when we create the object.This is usually done with Xarray using the chunks kwarg when opening a file with xr.open_dataset or with xr.open_mfdataset, if you create Xarray from your local file.\n\nIn our NDVI example, we create Xarray from stackstac. As stackstac‚Äôs default ‚Äòchunksize‚Äô definition is 1024 for the x and y dimensions, we had that chunk size. We can pass the chunksize option to stackstac and make it larger.\n\n%%time\nsentinel2_l2a = stackstac.stack(items,\n                                assets=['red','nir'],\n                                chunksize=( 1, 2, 2048,2048)\n)\nsentinel2_l2a\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#defining-the-chunk-size-at-the-creation-with-xarray","position":17},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"So, why chunks?"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#so-why-chunks","position":18},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"So, why chunks?"},"content":"As explained in Section 2.4 - Formats and Performance, chunks are mandatory for accessing files or datasets that are larger than a single computer‚Äôs memory. If all the data has to be accessed, it can be done sequentially (i.e., chunks are processed one after the other).\n\nMoreover, chunks allow for distributed processing and increased speed for your data analysis, as seen in the next section.","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#so-why-chunks","position":19},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl3":"Chunks and files","lvl2":"So, why chunks?"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/chunking#chunks-and-files","position":20},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl3":"Chunks and files","lvl2":"So, why chunks?"},"content":"Xarray chunking capabilities also depend on the underlying input or output file format used. Most modern file formats allow datasets or single files to be stored using chunks. For example, NetCDF4 uses chunks when storing a file on the disk via HDF5. Any read of data from a NetCDF4 file will load at least one chunk of that file. So when reading one of its chunks as defined in the open_dataset call, Xarray will take advantage of native file chunking and won‚Äôt need to read the entire file.\n\nHowever, it is important to note that Xarray chunks and file chunks are not necessarily the same. It is a good practice to configure Xarray chunks so that they align well with the input file format chunks (ideally, Xarray chunks should contain one or several input file chunks).\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#chunks-and-files","position":21},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Zarr storage format"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#zarr-storage-format","position":22},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Zarr storage format"},"content":"This brings us to our next subject: \n\nZarr.\n\nIf we can have our original dataset already ‚Äòchunked‚Äô and accessed in an optimized way according to its actual byte storage on disk, we won‚Äôt need to load the entire dataset every time. This can greatly optimize our data analysis, even when working with the entire dataset.\n\nLet‚Äôs convert our intermediate data into Zarr format so that we can learn what it is. We can keep the data as a DataArray or convert it into a Dataset before storing it.\n\nWe start again by loading data using stackstac, but this time, we proceed to the next step: clipping the data and computing the NDVI. Then, let‚Äôs try to save those intermediate results in a Zarr file.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#zarr-storage-format","position":23},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Data Loading"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#data-loading","position":24},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Data Loading"},"content":"Load data using stackstac (with specific chunk size)\n\naoi = gpd.read_file(\"./assets/catchment_outline.geojson\", crs=\"EPGS:4326\")\naoi_geojson = mapping(aoi.iloc[0].geometry)\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    intersects=aoi_geojson,\n    collections=[\"sentinel-2-l2a\"],\n    datetime=\"2019-02-01/2019-04-28\",\n    query= {\"proj:epsg\": dict(eq=32632)}\n).item_collection()\nds = stackstac.stack(items,\n                     assets=['red','nir'],\n                     chunksize=( 1, 2, 1024,1024)\n)\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#data-loading","position":25},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"NDVI computation"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#ndvi-computation","position":26},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"NDVI computation"},"content":"Compute the NDVI as in 2.3 Data Access - Reduce\n\ndef NDVI(data):\n    red = data.sel(band=\"red\")\n    nir = data.sel(band=\"nir\")\n    ndvi = (nir - red)/(nir + red)\n    return ndvi\n\nndvi_xr = NDVI(ds)\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#ndvi-computation","position":27},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Spatial clipping"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#spatial-clipping","position":28},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Spatial clipping"},"content":"Restrict the data to the area of interest from the loaded polygon\n\naoi_utm32 = aoi.to_crs(epsg=32632)\ngeom_utm32 = aoi_utm32.iloc[0]['geometry']\nndvi_xr.rio.write_crs(\"EPSG:32632\", inplace=True)\nndvi_xr.rio.set_nodata(np.nan, inplace=True)\nndvi_xr = ndvi_xr.rio.clip([geom_utm32])\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#spatial-clipping","position":29},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Save to Zarr"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#save-to-zarr","position":30},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Save to Zarr"},"content":"Select just a few days, to reduce the amount of data for this example\n\nndvi_small = ndvi_xr.isel(time=slice(0,3))\nndvi_small\n\n\n\nBefore saving, we can modify the chunk shape\n\nndvi_small = ndvi_small.chunk(chunks = {'x':'auto', 'y': 'auto'}).to_dataset(name='data')\nndvi_small\n\n\n\nThen clean attributes that might create issues while writing and save to Zarr\n\n%%time\n\ndef remove_attrs(obj, to_remove):\n    new = obj.copy()\n    new.attrs = {k: v for k, v in obj.attrs.items() if k not in to_remove}\n    return new\n\ndef encode(obj):\n    object_coords = [name for name, coord in obj.coords.items() if coord.dtype.kind == \"O\"]\n    return obj.drop_vars(object_coords).pipe(remove_attrs, [\"spec\", \"transform\"])\n\nndvi_small.pipe(encode).to_zarr('test.zarr',mode='w')\n\n\n\nExercise\n\nWhat about saving the data in Netcdf format? `ls -la test.zarr` and `ls -la test.zarr/nobs `\n\nYou can try to explore the zarr file you just created using `ls -la test.zarr` and `ls -la test.zarr/nobs `\n\nYou can explore zarr metadata file by `cat test.zarr/.zmetadata`\n\nDid you find the __chunks__ we defined previously in your zarr file?\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#save-to-zarr","position":31},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Compare Zarr and netCDF"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#compare-zarr-and-netcdf","position":32},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Compare Zarr and netCDF"},"content":"Lets compare how the zarr and NetCDF files are stored.We read our sample dataset from Zarr and store it as netCDF:\n\nxr.open_zarr('test.zarr').to_netcdf('test.nc')\n\n\n\nCompare the disk space used by the two formats:\n\n!du -sh test.zarr/ test.nc\n\n\n\nList the content of the Zarr directory:\n\n!ls  -la test.zarr/\n\n\n\nList the content of the Zarr data directory:\n\n!ls  -la test.zarr/data\n\n\n\nPrint the content of the Zarr metadata file\n\n!cat test.zarr/.zmetadata | head -n 30\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#compare-zarr-and-netcdf","position":33},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl3":"Zarr format main characteristics","lvl2":"Compare Zarr and netCDF"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/chunking#zarr-format-main-characteristics","position":34},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl3":"Zarr format main characteristics","lvl2":"Compare Zarr and netCDF"},"content":"\n\nEvery chunk of a Zarr dataset is stored as a single file (see x.y files in ls -al test.zarr/data)\n\nEach Data array in a Zarr dataset has a two unique files containing metadata:\n\n.zattrs for dataset or dataarray general metadatas\n\n.zarray indicating how the dataarray is chunked, and where to find them on disk or other storage.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#zarr-format-main-characteristics","position":35},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Conclusion"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/chunking#conclusion","position":36},{"hierarchy":{"lvl1":"2.4 Data Chunking with Pangeo","lvl2":"Conclusion"},"content":"Understanding chunking is key to optimize your data analysis when dealing with large datasets. In this exercise, we learned how to optimize data access time and memory resources by using native file chunks loaded by stackstac and instructing Xarray to modify the chunk. Computing on large datasets can be very slow on a single machine, and to optimize your time we may need to parallelize your computations. This is what you will learn in the next exercise with Dask.","type":"content","url":"/lectures/formats-and-performance/exercises/chunking#conclusion","position":37},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo"},"type":"lvl1","url":"/lectures/formats-and-performance/exercises/dask","position":0},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo"},"content":"\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask","position":1},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Parallel computing with Dask"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/dask#parallel-computing-with-dask","position":2},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Parallel computing with Dask"},"content":"\n\nOverview Questions\n\nWhat is Dask?\n\nHow can I parallelize my data analysis with Dask?Objectives\n\nLearn about Dask\n\nLearn about Dask Gateway, Dask Client, Scheduler, Workers\n\nUnderstand out-of-core and speed-up limitations\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#parallel-computing-with-dask","position":3},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Context"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/dask#context","position":4},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Context"},"content":"We will be using \n\nDask with \n\nXarray to parallelize our data analysis.  We will continue to use the NDVI example.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#context","position":5},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Parallelize with Dask"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/dask#parallelize-with-dask","position":6},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Parallelize with Dask"},"content":"\n\nWe know from the previous exercise 2.4_chunking that chunking is key to analyzing large data sets. In this exercise, we will learn how to parallelize our data analysis using \n\nDask on our chunked dataset.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#parallelize-with-dask","position":7},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/dask#what-is-dask","position":8},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"content":"Dask is a powerful and flexible library for parallel computing in Python. It enables users to scale their computations effortlessly while working with large datasets that exceed available memory.\n\nWith minimal code changes, Dask accelerates existing Python libraries such as NumPy, Pandas, and Scikit-learn.\n\nDesigned for processing massive datasets, Dask is widely used in Earth Science and other data-intensive fields.\n\nIt scales effortlessly from a single laptop to clusters, cloud environments, and HPC systems.\n\nDask bridges the gap between exploratory analysis and large-scale production workflows.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#what-is-dask","position":9},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"How does Dask scale and accelerate your data analysis?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"type":"lvl4","url":"/lectures/formats-and-performance/exercises/dask#how-does-dask-scale-and-accelerate-your-data-analysis","position":10},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"How does Dask scale and accelerate your data analysis?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"content":"Dask proposes different abstractions to distribute your computation. In this Dask Introduction section, we will focus on \n\nDask Array which is widely used in the Pangeo ecosystem as a backend of Xarray.\n\nThe chunks of a Dask Array are standard NumPy arrays. By transforming large datasets into Dask Arrays and utilizing chunks, a large array is handled as many smaller NumPy arrays. This allows us to compute each of these chunks independently.\n\nImage source: Dask documentation (¬© 2025 Dask core developers, BSD-3 Licensed).\n\nNote\n\nXarray uses Dask Arrays instead of Numpy when chunking is enabled, and thus all Xarray operations are performed through Dask, which enables distributed processing.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#how-does-dask-scale-and-accelerate-your-data-analysis","position":11},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"How does Xarray with Dask distribute data analysis?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"type":"lvl4","url":"/lectures/formats-and-performance/exercises/dask#how-does-xarray-with-dask-distribute-data-analysis","position":12},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"How does Xarray with Dask distribute data analysis?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"content":"When we use chunks with Xarray, the real computation is only done when needed or explicitly requested, usually by invoking the compute() or load() functions. Dask generates a task graph describing the computations to be performed. When using \n\nDask Distributed, a Scheduler distributes these tasks across several Workers.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#how-does-xarray-with-dask-distribute-data-analysis","position":13},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"What is a Dask Distributed cluster?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"type":"lvl4","url":"/lectures/formats-and-performance/exercises/dask#what-is-a-dask-distributed-cluster","position":14},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"What is a Dask Distributed cluster?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"content":"A Dask Distributed cluster is made up of two main components:\n\nA Scheduler, responsible for handling the computation graph and distributing tasks to Workers.\n\nOne or several Workers, which compute individual tasks and store results and data in distributed memory (RAM and/or the worker‚Äôs local disk).\n\nA user typically needs Client and Cluster objects, as shown below, to use Dask Distributed.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#what-is-a-dask-distributed-cluster","position":15},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"Where can we deploy a Dask distributed cluster?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"type":"lvl4","url":"/lectures/formats-and-performance/exercises/dask#where-can-we-deploy-a-dask-distributed-cluster","position":16},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl4":"Where can we deploy a Dask distributed cluster?","lvl3":"What is Dask ?","lvl2":"Parallelize with Dask"},"content":"Dask distributed clusters can be deployed on your laptop or on distributed infrastructures (Cloud, HPC centers, Hadoop, etc.).  Dask distributed Cluster object is responsible of deploying and scaling a Dask Cluster on the underlying resources.\n\nImage source: Dask documentation (¬© 2025 Dask core developers, BSD-3 Licensed).\n\nTipA Dask Cluster can be created on a single machine (e.g. your laptop), which means you do not need dedicated computing resources. This corresponds to Vertical scaling as described in lecture \n\n2.4 Formats and Performance.\n\nHowever, the speed up is limited to the resources of your single machine. This is where horizontal scaling comes in.  By adding resources from other machines, you can expand your computing power!\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#where-can-we-deploy-a-dask-distributed-cluster","position":17},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Dask distributed Client","lvl2":"Parallelize with Dask"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/dask#dask-distributed-client","position":18},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Dask distributed Client","lvl2":"Parallelize with Dask"},"content":"The Dask distributed Client is what allows you to interact with Dask distributed Clusters. When using Dask distributed, you always need to create a Client object. Once a Client has been created, it will be used by default by each call to a Dask API, even if you do not explicitly use it.\n\nNo matter the Dask API (e.g. Arrays, Dataframes, Delayed, Futures, etc.) that you use, under the hood, Dask will create a Directed Acyclic Graph (DAG) of tasks by analysing the code. Client will be responsible to submit this DAG to the Scheduler along with the final result you want to compute. The Client will also gather results from the Workers, and aggregate it back in its underlying Python process.\n\nUsing Client() function with no argument, you will create a local Dask cluster with a number of workers and threads per worker corresponding to the number of cores in the ‚Äòlocal‚Äô machine. The ‚Äòlocal‚Äô machine is the jupyterlab you are using in the Cloud, and the number of cores is the number of cores available in the cloud instance (not on your laptop).\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)   # create a local dask cluster on the local machine specifying 4\nclient\n\n\n\nInspecting the Cluster Info section above gives us information about the created cluster: we have 2 or 4 workers and the same number of threads (e.g. 1 thread per worker).\n\nGo further\n\nYou can also create a local cluster with the `LocalCluster` constructor and use `n_workers` and `threads_per_worker` to manually specify the number of processes and threads you want to use. For instance, we could use `n_workers=2` and `threads_per_worker=2`.\n\nThis is sometimes preferable (in terms of performance), or when you run this tutorial on your PC, you can avoid dask to use all your resources you have on your PC!\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#dask-distributed-client","position":19},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Dask Dashboard","lvl2":"Parallelize with Dask"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/dask#dask-dashboard","position":20},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Dask Dashboard","lvl2":"Parallelize with Dask"},"content":"Dask comes with a really handy interface: the Dask Dashboard. It is a web interface that you can open in a separate tab of your browser.\n\nWe will learn how to use it through the \n\ndask jupyterlab extension.\n\nTo use Dask Dashboard through jupyterlab extension,\nyou just need to click on the orange icon shown in the following figure,\n\nYou can click several buttons marked with blue arrows in the following images, and then drag and drop them wherever you want.\n\nIt‚Äôs really helpful to understand your computation and how it‚Äôs distributed.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#dask-dashboard","position":21},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Dask Distributed computations on our dataset"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/dask#dask-distributed-computations-on-our-dataset","position":22},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Dask Distributed computations on our dataset"},"content":"Let‚Äôs open the Zarr dataset we‚Äôve prepared in \n\nprevious chunking excersise, select a single location over time, visualize the task graph generated by Dask, and observe the Dask Dashboard.\n\nimport xarray as xr\nndvi = xr.open_zarr('test.zarr')\nndvi\n\n\n\nndvi['data'].mean(dim='x')\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#dask-distributed-computations-on-our-dataset","position":23},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/dask#did-you-notice-something-on-the-dask-dashboard-when-running-the-two-previous-cells","position":24},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"content":"We didn‚Äôt actually ‚Äòcompute‚Äô anything. Instead, we built a Dask task graph, with its size indicated as a count above, but we didn‚Äôt ask Dask to return a result.\n\nYou can check the ‚ÄòDask graph‚Äô to see how many layers it contains, which will help you estimate the complexity of your computation.\n\nIt shows that you have ‚Äò4 graphs‚Äô. This can be optimized in the following steps.\n\nLet‚Äôs try to plot the Dask graph before computation and understand what the Dask workers will do to compute the result we asked for.\n\nndvi['data'].mean(dim='x').data.visualize()\n\n\n\nLet‚Äôs average the time dimension and see how it differs\n\nndvi['data'].mean(dim='time').data.visualize()\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#did-you-notice-something-on-the-dask-dashboard-when-running-the-two-previous-cells","position":25},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Let‚Äôs compute on the Dask workers","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/dask#lets-compute-on-the-dask-workers","position":26},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Let‚Äôs compute on the Dask workers","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"content":"\n\nndvi['data'].mean(dim='time').compute()\n\n\n\nCalling compute on our Xarray object triggered the execution on Dask Cluster side.\n\nYou should be able to see how Dask is working on Dask Dashboard.\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#lets-compute-on-the-dask-workers","position":27},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Let‚Äôs try a bigger computation","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/dask#lets-try-a-bigger-computation","position":28},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Let‚Äôs try a bigger computation","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"content":"We will re-use the procedure we‚Äôve learned in the previous chunking notebook.\n\n### Load data using stackstac (with specific chunk) \nimport stackstac\nimport pystac_client\nimport geopandas as gpd\nfrom shapely.geometry import mapping\nimport numpy as np\nimport warnings\nimport rioxarray\n\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(\"ignore\", RuntimeWarning)\n\naoi = gpd.read_file('./assets/catchment_outline.geojson', crs=\"EPGS:4326\")\naoi_geojson = mapping(aoi.iloc[0].geometry)\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    intersects=aoi_geojson,\n    collections=[\"sentinel-2-l2a\"],\n    datetime=\"2019-02-01/2019-03-28\",\n    query= {\"proj:epsg\": dict(eq=32632)}\n).item_collection()\n\nds = stackstac.stack(items,\n                     assets=['red','nir'],\n                     chunksize=(1,2,1024,1024)\n                    )\nds\n\n\n\nBy inspecting any of the variables in the representation above, you‚Äôll see that the data array represents over 188 GiB of data, which is much more than the available memory on this notebook server or the Dask Local Cluster we created earlier. But thanks to chunking, we can still analyze it!\n\n## Computing the NDVI\ndef NDVI(data):\n    red = data.sel(band=\"red\")\n    nir = data.sel(band=\"nir\")\n    ndvi = (nir - red)/(nir + red)\n    return ndvi\n\nndvi_xr = NDVI(ds)\n\n## Clip the data\naoi_utm32 = aoi.to_crs(epsg=32632)\ngeom_utm32 = aoi_utm32.iloc[0]['geometry']\nndvi_xr.rio.write_crs(\"EPSG:32632\", inplace=True)\nndvi_xr.rio.set_nodata(np.nan, inplace=True)\nndvi_xr = ndvi_xr.rio.clip([geom_utm32])\nndvi_xr.rio.set_nodata(np.nan, inplace=True)\n\nndvi_xr = ndvi_xr.groupby(ndvi_xr.time.dt.floor('D')).max(skipna=True)\nndvi_xr = ndvi_xr.chunk(chunks = {'x':'auto', 'y': 'auto'})\nndvi_xr\n\n\n\nPerform the same steps as in the previous exercise to convert to a Dataset and clean out problematic attributes\n\nndvi_xr = ndvi_xr.to_dataset(name='data')\n\ndef remove_attrs(obj, to_remove):\n    new = obj.copy()\n    new.attrs = {k: v for k, v in obj.attrs.items() if k not in to_remove}\n    return new\n\ndef encode(obj):\n    object_coords = [name for name, coord in obj.coords.items() if coord.dtype.kind == \"O\"]\n    return obj.drop_vars(object_coords).pipe(remove_attrs, [\"spec\", \"transform\"])\n\nndvi_xr = ndvi_xr.pipe(encode)\n\n\n\n%%time\nimport numpy\nwith np.errstate(all=\"ignore\"):\n    ndvi_xr.to_zarr('ndvi.zarr',mode='w')\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#lets-try-a-bigger-computation","position":29},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Close client to terminate local dask cluster","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"type":"lvl3","url":"/lectures/formats-and-performance/exercises/dask#close-client-to-terminate-local-dask-cluster","position":30},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl3":"Close client to terminate local dask cluster","lvl2":"Did you notice something on the Dask Dashboard when running the two previous cells?"},"content":"\n\nThe Client and associated LocalCluster object will be automatically closed when your Python session ends. When using Jupyter notebooks, we recommend to close it explicitely whenever you are done with your local Dask cluster.\n\nclient.close()\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/dask#close-client-to-terminate-local-dask-cluster","position":31},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Conclusion"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/dask#conclusion","position":32},{"hierarchy":{"lvl1":"2.4 Scaling with Pangeo","lvl2":"Conclusion"},"content":"\n\nIn this exercise, we created a Dask cluster on a single machine, which is equivalent to vertical scaling as described in 2.4 Formats and Performance.\n\nHowever, the speedup is limited to the resources of your single machine. This is where horizontal scaling comes in. By adding resources from other machines, you can expand your computing power! In the case of dask, you can use the \n\nDask Gatway.The Pangeo community hosts tutorials using \n\nPangeo@EOSC. With the Pangeo EOSC deployment, you can learn how to use Dask Gateway to manage Dask clusters over Kubernetes, allowing us to run our data analysis in parallel, e.g. distribute tasks across multiple workers.\nIf you are a user of HPC center, you can also consider using \n\nDask-jobque or Dask HPC for this.","type":"content","url":"/lectures/formats-and-performance/exercises/dask#conclusion","position":33},{"hierarchy":{"lvl1":"2.4 Formats and Performance"},"type":"lvl1","url":"/lectures/formats-and-performance/exercises/energy-consumption","position":0},{"hierarchy":{"lvl1":"2.4 Formats and Performance"},"content":"\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/energy-consumption","position":1},{"hierarchy":{"lvl1":"2.4 Formats and Performance","lvl2":"Usage of resources"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/energy-consumption#usage-of-resources","position":2},{"hierarchy":{"lvl1":"2.4 Formats and Performance","lvl2":"Usage of resources"},"content":"\n\nWhen executing code on your local machine or in the cloud, it is important to remember that each operation uses resources.\n\nIn this section we showcase a python library codecarbon that can be used to estimate the energy consumption of your code.\n\nPlease note that benchmarking systems and profiling code is a complex topic with many variables. These examples merely illustrate very rough estimates.\n\nWe start by copying the data files needed to complete the exercise using the following shell commands\n\nStart by import all the necessary libraries and utilities.\n\nimport random\nimport numpy as np\nimport openeo\nfrom openeo.local import LocalConnection\n\nfrom codecarbon_utils import calculate_emission_equivalents, CustomEmissionsTracker\nlocal_conn = LocalConnection('')\n\n\n\nSetting up the emission tracker for ‚Äúoffline‚Äù use to not interact with the CodeCarbon API.\n\nChanging the country code will alter the carbon intensity value used to calculate the emissions.\n\nTo learn more about the methodology behind codecarbon see the \n\ndocumentation.\n\nFeel free to change the country code to wherever you are to see how it affects the carbon emissions. ISO codes can be found on \n\nWikipedia.\n\ntracker = CustomEmissionsTracker(\n    country_iso_code=\"ITA\",\n    log_level=\"error\",\n    save_to_file=True,\n    output_dir=\"./\",\n)\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/energy-consumption#usage-of-resources","position":3},{"hierarchy":{"lvl1":"2.4 Formats and Performance","lvl2":"Simple example"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/energy-consumption#simple-example","position":4},{"hierarchy":{"lvl1":"2.4 Formats and Performance","lvl2":"Simple example"},"content":"This example performs a simple matrix multiplication using numpy and estimates the energy consumption of the operation.\n\nAs a comparison, we also perform a matrix multiplication using the standard library functionality of Python.\n\nWhat you will see is that utilizing high performance libraries like numpy can dramatically increase the efficiency.\n\ntracker.start_experiment(experiment_id=1)\n\ndef matrix_numpy(size: int, iterations: int) -> np.ndarray:\n    \"\"\"\n    Compute the product of a square matrix with itself for a number of iterations using numpy.\n\n    Args:\n        size (int): Size of the matrix (size x size).\n        iterations (int): Number of iterations for matrix multiplication.\n\n    Returns:\n        The result of the matrix product.\n    \"\"\"\n    matrix = np.random.rand(size, size)\n    for _ in range(iterations):\n        matrix = np.dot(matrix, matrix)\n    return matrix\n\nmatrix_numpy(100, 100)\n\ntracker.stop_experiment()\n\n\n\ncalculate_emission_equivalents(experiment_id=1)\n\n\n\ntracker.start_experiment(experiment_id=2)\n\ndef matrix_python(size: int, iterations: int) -> list:\n    \"\"\"\n    Compute the product of a square matrix with itself for a number of iterations using pure Python.\n    \n    Args:\n        size (int): Size of the matrix (size x size).\n        iterations (int): Number of iterations for matrix multiplication.\n    \n    Returns:\n        The result of the matrix product.\n    \"\"\"\n\n    matrix = [[random.random() for _ in range(size)] for _ in range(size)]\n\n    for _ in range(iterations):\n        result = [[0] * size for _ in range(size)]\n        for i in range(size):\n            for j in range(size):\n                for k in range(size):\n                    result[i][j] += matrix[i][k] * matrix[k][j]\n        matrix = result\n\n    return matrix\n\nmatrix_python(100, 100)\n\ntracker.stop_experiment()\n\n\n\ncalculate_emission_equivalents(experiment_id=2)\n\n\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/energy-consumption#simple-example","position":5},{"hierarchy":{"lvl1":"2.4 Formats and Performance","lvl2":"Using a real world example"},"type":"lvl2","url":"/lectures/formats-and-performance/exercises/energy-consumption#using-a-real-world-example","position":6},{"hierarchy":{"lvl1":"2.4 Formats and Performance","lvl2":"Using a real world example"},"content":"In this example, we will use our tracker to estimate the energy consumption of an NDVI workflow from previous exercise.\n\nWe need to squeeze the code into a single cell to be able to track the energy consumption more easily.\n\nFor more detailed explanation of the workflow, please refer to the previous exercise.\n\nOverview of the workflow:\n\nWe select  Sentinel-2 data for a specific area and time frame of 1 year.\n\nWe perform temporal aggregation to get monthly composites.\n\nWe calculate the NDVI for each composite.\n\ntracker.start_experiment(experiment_id=3)\n\nurl = \"https://earth-search.aws.element84.com/v1/collections/sentinel-2-l2a\"\n\nspatial_extent = {\"west\": 11.4, \"east\": 11.42, \"south\": 45.5, \"north\": 45.52}\ntemporal_extent = [\"2023-01-01\", \"2023-12-31\"]\nbands = [\"red\",\"nir\"]\n\ns2_cube = local_conn.load_stac(url=url,\n   spatial_extent=spatial_extent,\n   temporal_extent=temporal_extent,\n   bands=bands\n)\n\ns2_monthly_mean = s2_cube.aggregate_temporal_period(period=\"month\", reducer=\"mean\")\n\nred = s2_monthly_mean.band(\"red\")\nnir = s2_monthly_mean.band(\"nir\")\n\nndvi = (nir - red) / (nir + red)\n\nndvi.execute().compute()\n\ntracker.stop_experiment()\n\n\n\nQuiz hint: look carefully energy emission equivalents\n\ncalculate_emission_equivalents(experiment_id=3)\n\n","type":"content","url":"/lectures/formats-and-performance/exercises/energy-consumption#using-a-real-world-example","position":7},{"hierarchy":{"lvl1":"3.1 Data Processing"},"type":"lvl1","url":"/lectures/data-processing/data-processing","position":0},{"hierarchy":{"lvl1":"3.1 Data Processing"},"content":"","type":"content","url":"/lectures/data-processing/data-processing","position":1},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/data-processing/data-processing#learning-objectives","position":2},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Learning Objectives"},"content":"Carry out an EO workflow on a cloud platform\n\nSelect suitable data\n\nChain processes to form an EO processing chain/workflow\n\nVisualize the results","type":"content","url":"/lectures/data-processing/data-processing#learning-objectives","position":3},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Introduction"},"type":"lvl2","url":"/lectures/data-processing/data-processing#introduction","position":4},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Introduction"},"content":"In this lecture we are going to combine the knowledge and hands-on experience we have gathered so far to create a full EO workflow on a cloud platform.\nWe will\n\ndefine a research question,\n\nchoose and load the necessary data sources,\n\ndefine the data cube to our needs,\n\nuse functions to process the data,\n\nvisualize the result\n\nand track the resources we are consuming on the platform.","type":"content","url":"/lectures/data-processing/data-processing#introduction","position":5},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Case Study: Snow Cover in the Alps"},"type":"lvl2","url":"/lectures/data-processing/data-processing#case-study-snow-cover-in-the-alps","position":6},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Case Study: Snow Cover in the Alps"},"content":" \n\nVideo content in cooperation with \n\nMatteo Dall‚ÄôAmico (MobyGIS - Waterjade) and \n\nFederico Di Paolo (MobyGIS - Waterjade). \n‚ÄúHow much snow is stored in that mountain, when will it melt?‚Äù We answered this question using EO cloud platforms! Have a look: \n\nEO4Alps Snow","type":"content","url":"/lectures/data-processing/data-processing#case-study-snow-cover-in-the-alps","position":7},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl3":"Research Question","lvl2":"Case Study: Snow Cover in the Alps"},"type":"lvl3","url":"/lectures/data-processing/data-processing#research-question","position":8},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl3":"Research Question","lvl2":"Case Study: Snow Cover in the Alps"},"content":"Snow serves as a water reservoir and is thus important for any hydrological management activity, such as irrigation planning, drink water supply or hydro power generation. Knowing precisely, when and where snow is present is a critical source of information for these acitivities. Satellite earth observation plays an important role in describing the snow cover, both globally and in local mountain ranges. This is due to it‚Äôs ability to sense information throughout space (complete coverage of the globe) and time (repeated measurements). Our goal is to create a time series of the snow covered area of the catchment of interest. We will use this time series to compare it to the run off at the main outlet of the catchment. And study the relationship between snow dynamics and runoff.","type":"content","url":"/lectures/data-processing/data-processing#research-question","position":9},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl3":"Approach","lvl2":"Case Study: Snow Cover in the Alps"},"type":"lvl3","url":"/lectures/data-processing/data-processing#approach","position":10},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl3":"Approach","lvl2":"Case Study: Snow Cover in the Alps"},"content":"In this exercise we are going to derive the snow cover in an alpine catchment using Sentinel-2 data. Sentinel-2 carries an optical sensor, it is measuring the reflected light of the earths surface in differenct wavelenghts. At a 20 m spatial resolution and at a 6 day repeat rate. We are using the Green and SWIR bands to calculate the Normalized Difference Snow Index (NDSI). It is calculated as follows:NDSI = \\\\frac {GREEN - SWIR} {GREEN + SWIR}\n\nSnow typically has very high visible (VIS) reflectance and very low reflectance in the shortwave infrared (SWIR), a characteristic used to detect snow by distinguishing between snow and most cloud types. The NDSI expresses this phenomenon as a formula. It results in a value between -1 and 1. The higher the value is, the more probable it is that the surface is covered with snow. In order to create a binary snow map we apply a threshold of NDSI < 0.4. This is a commonly used value for discriminating snowcovered and snow free areas. Then we spatially aggregate the snow free and snow covered pixels in the catchment area by summing them up. In order to get the snow covered area of the catchment we multiply the number of snow covered pixels by the pixel resolution. Additionally, we have to deal with cloud cover. We use the Sentinel-2 cloud mask that is provided with the data and exclude all images that have a cloud cover over 25 % in our study area. Ideally we should fill the gaps the clouds generate, since they are introducing uncertainty. Nevertheless, for a first try our approach should be good enough to get a general idea about the snow cover in our area of interest. In the end we receive a time series with the snow covered area in the catchment.\nThe approach we are using is very basic. There are many assumptions and simplifications involved. A critical analysis of the workflow and possible improvements follows in Section \n\n3.3 Validation.","type":"content","url":"/lectures/data-processing/data-processing#approach","position":11},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl3":"Workflow Description","lvl2":"Case Study: Snow Cover in the Alps"},"type":"lvl3","url":"/lectures/data-processing/data-processing#workflow-description","position":12},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl3":"Workflow Description","lvl2":"Case Study: Snow Cover in the Alps"},"content":"get data:\n\nload_collection()\n\ncalculate ndsi:\n\nfilter_bands(), reduce_dimension()\n\ncreates a -1 to 1 map, 1 signifies high probability of snow\n\ncreate binary snow classification (by threshold):\n\ngt(), mask(),\n\ncreate a binary snow classification: 0 = no snow, 1 = snow\n\ncloud masking:\n\neq(), mask()\n\ncreate a binary cloud mask using the S2 scene classification\n\nApply the mask to the binary snow map: 0 = no snow, 1 = snow, NA = cloud\n\nThis gives us the cloudfree snow covered area for our catchment as an image time series (x, y, time, sca)\n\ncatchment statistics - cloud pixels, no snow pixels, snow pixels:\n\neq(), gt(), merge_cubes(), aggregate_spatial(), sum()\n\ncreate 3 binary data cubes: pixel in catchment, pixel cloud, pixel snow.\n\nmerge the 3 cubes: x, y, time, band (catchment, cloud, snow)\n\naggregate_spatial: sum up the values to get the total number of pixels per time step per band (catchment, cloud, snow)\n\ncalculate the percentages of clouds and snow per time step.\n\nfilter timeseries according to cloud coverage:\n\nfilter the dates that have cloud coverages > 25%\n\nPlot the time series of the snow covered area in the catchment.","type":"content","url":"/lectures/data-processing/data-processing#workflow-description","position":13},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl4":"Animated Content: Embed Process Graph","lvl3":"Workflow Description","lvl2":"Case Study: Snow Cover in the Alps"},"type":"lvl4","url":"/lectures/data-processing/data-processing#animated-content-embed-process-graph","position":14},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl4":"Animated Content: Embed Process Graph","lvl3":"Workflow Description","lvl2":"Case Study: Snow Cover in the Alps"},"content":"","type":"content","url":"/lectures/data-processing/data-processing#animated-content-embed-process-graph","position":15},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Exercise"},"type":"lvl2","url":"/lectures/data-processing/data-processing#exercise","position":16},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Exercise"},"content":"Now we have covered the most important topics of our use case in theory. Let‚Äôs move on to produce some results!\n\n:warning: The applied workflow is a simple approach used for educational reasons to learn how to use EO cloud platforms.\n\nComplete firstly the exercise using openEO:\n\nExercise 3.1 Processing openEO\n\nAfterwards, do the same using Pangeo:\n\nExercise 3.1 Processing Pangeo","type":"content","url":"/lectures/data-processing/data-processing#exercise","position":17},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/data-processing/data-processing#quiz","position":18},{"hierarchy":{"lvl1":"3.1 Data Processing","lvl2":"Quiz"},"content":"What is the city at the outlet of the catchment? Answer in the exercises: 31_data_processing_openeo.ipynb and 31_data_processing_pangeo.ipynb section ‚ÄòRegion of Interest‚Äô[(x)] Meran\n[( )] Innsbruck\n[( )] Grenoble\n\nHow many images are available in the time range (‚Äú2018-02-01‚Äù and ‚Äú2018-06-30‚Äù)? Answer in the exercises: 31_data_processing_openeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô, might require you to run some additional code blocks[( )] 0-20\n[( )] 21-40\n[(x)] 41-60\n\nHow many resources did the computation of the data cube take?  Answer in the exercise: 31_data_processing_openeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô[(x)] 0 - 20\n[( )] 21 - 100\n[( )] 101 - 200\n\nHow many snow covered pixels are there across all time steps? Answer in the exercise: 31_data_processing_openeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô[( )] 10,000,000 - 50,000,000\n[(x)] 50,000,001 - 100,000,000\n[( )] 100,000,001 - 200,000,000\n\nHow many pixels are in the temporally aggregated data cube? (date * y * x) Answer in the exercises: 31_data_processing_pangeo.ipynb section ‚ÄòAggregate Data‚Äô[( )] 50,000,000 - 100,000,000\n[(x)] 100,000,001 - 200,000,000\n[( )] 200,000,001 - 500,000,000\n\nHow many cloud covered pixels are there across all time steps in the Pangeo exercise? Answer in the exercise: 31_data_processing_pangeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô[(x)] 5,000,000 - 50,000,000\n[( )] 100,000,001 - 200,000,000\n[( )] 50,000,001 - 100,000,000\n\nAt which time step is the maximum snow cover reached in the openEO exercise?  Answer in the exercise: 31_data_processing_openeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô[( )] 2018-02-13\n[( )] 2018-02-21\n[(x)] 2018-03-08\n\nWhat is the number of snow covered pixels on that date?  Answer in the exercise: 31_data_processing_openeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô[( )] 4,000,001 - 6,000,000\n[( )] 0 - 2,000,000\n[(x)] 2,000,001 - 4,000,000\n\nWhat does that represent in area (km2)?  Answer in the exercise: 31_data_processing_openeo.ipynb section ‚ÄòCalculate Catchment Statistics‚Äô[(x)] 201 - 400\n[( )] 0 - 200\n[( )] 401 - 600","type":"content","url":"/lectures/data-processing/data-processing#quiz","position":19},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO"},"type":"lvl1","url":"/lectures/data-processing/exercises/data-processing-openeo","position":0},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO"},"content":"\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo","position":1},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Snow Cover mapping with openEO"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#snow-cover-mapping-with-openeo","position":2},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Snow Cover mapping with openEO"},"content":"In this exercise we will build a complete EO workflow on a cloud platform; from data access to obtaining the result. In this example we will analyse snow cover in the Alps.\n\nWe are going to follow these steps in our analysis:\n\nLoad satellite collections\n\nSpecify the spatial, temporal extents and the features we are interested in\n\nProcess the satellite data to retrieve snow cover information\n\nAggregate information to get catchment statistics over time\n\nVisualize and analyse the results\n\nMore information on the openEO Python Client: \n\nhttps://‚Äãopen‚Äã-eo‚Äã.github‚Äã.io‚Äã/openeo‚Äã-python‚Äã-client‚Äã/index‚Äã.html\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#snow-cover-mapping-with-openeo","position":3},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#libraries","position":4},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Libraries"},"content":"\n\nWe start by creating the shared folders and data files needed to complete the exercise using the following shell commands\n\n# platform libraries\nimport openeo\n\n# utility libraries\nfrom datetime import date\nimport numpy as np\nimport xarray as xr\nimport rioxarray\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport leafmap.foliumap as leafmap\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#libraries","position":5},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Connect to a cloud platform"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#connect-to-a-cloud-platform","position":6},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Connect to a cloud platform"},"content":"Connect to the Copernicus Dataspace Ecosystem. Being connected allows for data discovery.\n\nconn = openeo.connect('https://openeo.dataspace.copernicus.eu/')\n\n\n\nAnd login. Being logged in allows to use the full range of functionality including processing!\n\nconn.authenticate_oidc()\n\n\n\nCheck if the login worked.\n\nconn.describe_account()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#connect-to-a-cloud-platform","position":7},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Region of Interest"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#region-of-interest","position":8},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Region of Interest"},"content":"\n\nOur region of interest is the Val Passiria Catchment in the South Tyrolian Alps (Italy). Let‚Äôs load the catchment area.\n\ncatchment_outline = gpd.read_file('31_data/catchment_outline.geojson')\n\n\n\ncenter = (float(catchment_outline.centroid.y), float(catchment_outline.centroid.x))\nm = leafmap.Map(center=center, zoom=10)\nm.add_vector('31_data/catchment_outline.geojson', layer_name=\"catchment\")\nm\n\n\n\nQuiz hint: Look closely at the end of the displayed catchment area to identify the outlet\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#region-of-interest","position":9},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Inspect Metadata"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#inspect-metadata","position":10},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Inspect Metadata"},"content":"We need to set the following configurations to define the content of the data cube we want to access:\n\ndataset name\n\nband names\n\ntime range\n\nthe area of interest specifed via bounding box coordinates\n\nspatial resolution\n\nTo select the correct dataset we can first list all the available datasets.\n\nprint(conn.list_collection_ids())\n\n\n\nWe want to use the Sentinel-2 L2A product. It‚Äôs name is 'SENTINEL2_L2A'.\n\nWe get the metadata for this collection as follows. This is an important step to familiarize yourself with the data collection (e.g. learn the band names).\n\nconn.describe_collection(\"SENTINEL2_L2A\")\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#inspect-metadata","position":11},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Define a workflow"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#define-a-workflow","position":12},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Define a workflow"},"content":"We will define our workflow now. And chain all the processes together we need for analyzing the snow cover in the catchment.\n\nLoad a data cube with specific filters\n\nCalculate the Normalized Difference Snow Index\n\nClassify snow and no-snow using a threshold yielding the Snow Covered Area\n\nCreate and apply a cloud mask to remove cloudy pixels\n\nVisualize one date of the snow map and crop it to the exact catchment outline\n\nCalculate catchment statistics to get a timeseries on snow cover and cloud cover\n\nFilter the time series by the cloud percentage and visualize the time series graph\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#define-a-workflow","position":13},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Define the data cube","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#define-the-data-cube","position":14},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Define the data cube","lvl2":"Define a workflow"},"content":"We define all extents of our data cube. We use the catchment as spatial extent. As a time range we will focus on the snow melting season 2018, in particular from Febraury to June 2018. We are only interested in the green and short wave infrared band, band 3 and 11. And we directly remove time slices with a cloud cover >= 90 %.\n\nbbox = catchment_outline.bounds.iloc[0]\nbbox\n\n\n\nfrom openeo.processes import lte\ncollection      = 'SENTINEL2_L2A'\nspatial_extent  = {'west':bbox[0],'east':bbox[2],'south':bbox[1],'north':bbox[3],'crs':4326}\ntemporal_extent = [\"2018-02-01\", \"2018-06-30\"]\nbands           = ['B03', 'B11', 'SCL']\nproperties={\"eo:cloud_cover\": lambda x: lte(x, 90)}\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#define-the-data-cube","position":15},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Load the data cube","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#load-the-data-cube","position":16},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Load the data cube","lvl2":"Define a workflow"},"content":"We have defined the extents we are interested in. Now we use these definitions to load the data cube.\n\ns2 = conn.load_collection(collection,\n                          spatial_extent=spatial_extent,\n                          bands=bands,\n                          temporal_extent=temporal_extent,\n                          properties=properties)\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#load-the-data-cube","position":17},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"NDSI - Normalized Difference Snow Index","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#ndsi-normalized-difference-snow-index","position":18},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"NDSI - Normalized Difference Snow Index","lvl2":"Define a workflow"},"content":"The Normalized Difference Snow Index (NDSI) is computed as:NDSI = \\frac {GREEN - SWIR} {GREEN +SWIR}\n\nWe have created a Sentinel-2 data cube with bands B03 (green) and B11 (SWIR). We will use the green and SWIR band to calculate a the NDSI. This process is reducing the band dimension of the data cube to generate new information, the NDSI.\n\ngreen = s2.band(\"B03\")\nswir = s2.band(\"B11\")\nndsi = (green - swir) / (green + swir)\nndsi\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#ndsi-normalized-difference-snow-index","position":19},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Creating the Snow Map","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#creating-the-snow-map","position":20},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Creating the Snow Map","lvl2":"Define a workflow"},"content":"So far we have a time series map of NDSI values. We are intereseted in the presence of snow though. Ideally in a binary classification: snow and no snow.\nTo achieve this we are setting a threshold of 0.4 on the NDSI. This gives us a binary snow map.\n\nsnowmap = ( ndsi > 0.4 ) * 1.0  # the addition of \"* 1.00\" is a workaround for a backend specific implementation problem. Once solved on the CDSE openEO backend it could be removed\nsnowmap\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#creating-the-snow-map","position":21},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Creating a cloud mask","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#creating-a-cloud-mask","position":22},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Creating a cloud mask","lvl2":"Define a workflow"},"content":"We are going to use the Scene Classification of Sentinel-2, called the ‚ÄúSCL‚Äù band,  for creating a cloud mask and then applying it to the NDSI. The values we are interested in are: 8 = cloud medium probability, 9 = cloud high probability, 3 = cloud shadow\n\nHere is more information on the Scene Classification of Sentinel-2: \n\nhttps://‚Äãsentiwiki‚Äã.copernicus‚Äã.eu‚Äã/web‚Äã/s2‚Äã-processing‚Äã#S2‚Äã-Processing‚Äã-Scene‚Äã-Classification\n\nscl_band = s2.band(\"SCL\")\ncloud_mask = ( (scl_band == 8) | (scl_band == 9) | (scl_band == 3) ) * 1.0\ncloud_mask\n\n\n\nThe SCL layer has a ground sample distance of 20 meter while the other bands have 10 meter GSD\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#creating-a-cloud-mask","position":23},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#applying-the-cloud-mask-to-the-snowmap","position":24},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Define a workflow"},"content":"We will mask out all pixels that are covered by clouds. This will result in: 0 = no_snow, 1 = snow, 2 = cloud\n\nsnowmap_cloudfree = snowmap.mask(cloud_mask,replacement=2) # replacement is null by default\nsnowmap_cloudfree\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#applying-the-cloud-mask-to-the-snowmap","position":25},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Mask Polygon: From Bounding Box to Shape","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#mask-polygon-from-bounding-box-to-shape","position":26},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Mask Polygon: From Bounding Box to Shape","lvl2":"Define a workflow"},"content":"We have a cloud masked snow map data cube now. In order to keep only pixels within the exact chatchment boundaries we mask to the outline of the catchment. Values outside of the boundaries are set to NA.\n\ncatchment_outline['geometry'][0]\n\n\n\nsnowmap_cloudfree_masked = snowmap_cloudfree.mask_polygon(catchment_outline['geometry'][0])\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#mask-polygon-from-bounding-box-to-shape","position":27},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Visualize one time step of the timeseries","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#visualize-one-time-step-of-the-timeseries","position":28},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Visualize one time step of the timeseries","lvl2":"Define a workflow"},"content":"Let‚Äôs have a first look at a time slice of our snow map. So far we have not computed anything. We have only defined a set of functions that are going to be applied in sequence. This makes up our workflow or processing graph.\nTo reduce the data volume which we are going to download we are only selecting one time step of our data cube.\n\nIn order to start the processing we have to tell the cloud platform specifically that we want to execute our workflow. In this case we want to start the processing directly without registering a job on the backend. This solution is good for small amounts of data. For larger processing tasks batch jobs are preferred (we‚Äôll do that later).\n\nsnowmap_cloudfree_1d = snowmap_cloudfree_masked.filter_temporal('2018-02-10', '2018-02-12')\nsnowmap_cloudfree_1d.download('31_results/snowmap_cloudfree_1d.nc')\n\n\n\nOnce the processing is done on the cloud and the data is downloaded we can load the file into our working environment and plot it!\n\nThe area of interest is spread across two S2 tiles. This is visibile in the northern part of the plot because we chose one specific acquisition date where there is not data available for the northern tile.\n\nxr.open_dataarray('31_results/snowmap_cloudfree_1d.nc',decode_coords=\"all\")[0].plot.imshow()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#visualize-one-time-step-of-the-timeseries","position":29},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Calculate Catchment Statistics"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-openeo#calculate-catchment-statistics","position":30},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl2":"Calculate Catchment Statistics"},"content":"We are looking at the snow cover of a region over time and want to extract aggregated catchment statistics on snow cover and cloud cover. We do this by counting all the pixels in the catchment, counting the pixels that are covered by snow and the pixels covered by clouds.\n\nUltimately we are interested in the snow covered area (SCA) within the catchment. We count all snow covered pixels within the catchment for each time step. Multiplied by the pixel size that would be the snow covered area. The snow pixel count divided by the total number of pixels in the catchment is the percentage of pixels covered with snow. We will use this number.\n\nWe need to make sure that the information content meets our expected quality. Therefore, we calculate the cloud percentage for the catchment for each timestep. We use this information to filter the timeseries. All timesteps that have a cloud coverage of over 25% will be discarded.\n\nWe are going to\n\nGet number of pixels in the catchment: total, clouds, snow.\n\nCombine the three aggregated pixel counts into one data cube.\n\nCalculate cloud and snow percentages\n\nFilter cloudy time steps with the cloud percentage\n\nPlot the resulting time series\n\nQuiz hint: remember the pixel counts here for the final exercise\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#calculate-catchment-statistics","position":31},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Count pixels and aggregate spatially to the catchment","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#count-pixels-and-aggregate-spatially-to-the-catchment","position":32},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Count pixels and aggregate spatially to the catchment","lvl2":"Calculate Catchment Statistics"},"content":"\n\n# number of all pixels\nn_catchment = ((snowmap_cloudfree > -1) * 1.0).add_dimension(name=\"bands\",type=\"bands\",label=\"n_catchment\")\n\n# number of cloud pixels (no function needed, mask already created before)\nn_cloud = cloud_mask.add_dimension(name=\"bands\",type=\"bands\",label=\"n_cloud\")\n\n# number of snow pixels\nn_snow = ((snowmap_cloudfree == 1) * 1.0).add_dimension(name=\"bands\",type=\"bands\",label=\"n_snow\")\n\n# combine the binary data cubes into one data cube\nn_catchment_cloud_snow = n_catchment.merge_cubes(n_cloud).merge_cubes(n_snow)\n\n# aggregate to catchment\nn_pixels = n_catchment_cloud_snow.aggregate_spatial(geometries = catchment_outline['geometry'][0], reducer = 'sum')\nn_pixels\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#count-pixels-and-aggregate-spatially-to-the-catchment","position":33},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Register a batch job for processing","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#register-a-batch-job-for-processing","position":34},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Register a batch job for processing","lvl2":"Calculate Catchment Statistics"},"content":"We are starting the processing now with a batch job. This registers our job on the backend in our user space and assigns further information to the job, such as an ID, the job status, the process graph and further metadata. First we specifiy the end of our process graph with save_result() and specifiy the format (since we aggregated over the spatial dimension we will receive three arrays of data. So JSON is a suitable format). Then we create the batch job and start it.\n\n# Define the end of the process graph and the output format\nn_pixels_json = n_pixels.save_result(format=\"JSON\")\n# Create a batch job\njob = n_pixels_json.create_job(title=\"n_pixels_json\")\n# start the job and wait till it finishes\njob.start_and_wait()\n\n\n\nNow we can check the status of our job. We can download the result once the job has finished.\n\njob.status()\n\n\n\nif job.status() == \"finished\":\n    results = job.get_results()\n    results.download_files(\"31_results/\")\n\n\n\nQuick hint: take a look at the job description: e.g. job.describe_job()\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#register-a-batch-job-for-processing","position":35},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Load the resulting time series","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#load-the-resulting-time-series","position":36},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Load the resulting time series","lvl2":"Calculate Catchment Statistics"},"content":"Let‚Äôs load the result. It contains the total number of pixels in the catchment, number of cloud and snow pixels.\n\n# load the result\nwith open(\"31_results/timeseries.json\",\"r\") as file:\n    n_pixels_json = json.load(file)\n\n\n\n# check the first 5 entries to check the data structure.\nlist(n_pixels_json.items())[:3] # careful unsorted dates due to JSON format\n\n\n\nlen(n_pixels_json)\n\n\n\nQuick hint: what is the length of the time series JSON?\nlen(n_pixels_json)\n\nNow we do some data wrangling to get a structured data frame.\n\n# Create a Pandas DataFrame to hold the values\ndates = [k for k in n_pixels_json]\nn_catchment_vals = [n_pixels_json[k][0][0] for k in n_pixels_json]\nn_cloud_vals = [n_pixels_json[k][0][1] for k in n_pixels_json]\nn_snow_vals = [n_pixels_json[k][0][2] for k in n_pixels_json]\n\ndata = {\n        \"time\":pd.to_datetime(dates),\n        \"n_catchment_vals\":n_catchment_vals,\n        \"n_cloud_vals\":n_cloud_vals,\n        \"n_snow_vals\":n_snow_vals\n       }\ndf = pd.DataFrame(data=data).set_index(\"time\")\n# Sort the values by date\ndf = df.sort_values(axis=0,by=\"time\")\ndf[:3]\n\n\n\nCompute the total number of snow pixels in the selected temporal period:\n\ndf.n_snow_vals.sum()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#load-the-resulting-time-series","position":37},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Calculate the cloud percentage for filtering time steps","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#calculate-the-cloud-percentage-for-filtering-time-steps","position":38},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Calculate the cloud percentage for filtering time steps","lvl2":"Calculate Catchment Statistics"},"content":"Divide the number of cloudy pixels by the number of total pixels = cloud percentage\n\nperc_cloud = df[\"n_cloud_vals\"].values / df[\"n_catchment_vals\"].values * 100\ndf[\"perc_cloud\"] = perc_cloud\ndf[:3]\n\n\n\nQuick hint: The sum of the n_catchment_vals should give an overall idea of the total number of pixels in the datacube for the whole time-series df.n_catchment_vals.sum()\n\nQuick hint: a filter of the snow values can give an idea of when the maximum snow cover occurred df.where(df.n_snow_vals == df.n_snow_vals.max())\n\nQuick hint: a simplified approach for converting from pixel count to square kilometres is to use this simplified formula::\n\n{{Area (km^2)} = (\\frac{Spatial resolution (meters/pixel)^2}{1,000,000})\\times{\\text{Total pixel count}}}\n\nPlot the timeseries and the cloud threshold of 25%. If the cloud cover is higher the timestep will be excluded later on.\n\nPlot the cloud percentage with the threshold.\n\ndf.plot(y=\"perc_cloud\",rot=45,kind=\"line\",marker='o')\nplt.axhline(y = 25, color = \"r\", linestyle = \"-\")\nplt.show()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#calculate-the-cloud-percentage-for-filtering-time-steps","position":39},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Calculate the snow percentage","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#calculate-the-snow-percentage","position":40},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Calculate the snow percentage","lvl2":"Calculate Catchment Statistics"},"content":"Divide the number of snow pixels by the number of total pixels = snow percentage\n\nperc_snow = df[\"n_snow_vals\"].values / df[\"n_catchment_vals\"].values * 100\ndf[\"perc_snow\"] = perc_snow\ndf[:3]\n\n\n\nPlot the unfiltered snow percentage\n\ndf.plot(y=\"perc_snow\",rot=45,kind=\"line\",marker='o')\nplt.show()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#calculate-the-snow-percentage","position":41},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Filter out cloudy time steps","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#filter-out-cloudy-time-steps","position":42},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Filter out cloudy time steps","lvl2":"Calculate Catchment Statistics"},"content":"Keep only the dates with cloud coverage less than the threshold\n\ndf_filtered = df.loc[df[\"perc_cloud\"]<25]\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#filter-out-cloudy-time-steps","position":43},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Plot and save the cloud free snow percentage time series","lvl2":"Calculate Catchment Statistics"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-openeo#plot-and-save-the-cloud-free-snow-percentage-time-series","position":44},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO","lvl3":"Plot and save the cloud free snow percentage time series","lvl2":"Calculate Catchment Statistics"},"content":"Plot the cloud filtered snow percentage\n\ndf_filtered.plot(y=\"perc_snow\",rot=45,kind=\"line\",marker='o')\nplt.show()\n\n\n\nSave the cloud filtered snow percentage\n\ndf_filtered.to_csv(\"31_results/filtered_snow_perc.csv\")\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-openeo#plot-and-save-the-cloud-free-snow-percentage-time-series","position":45},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo"},"type":"lvl1","url":"/lectures/data-processing/exercises/data-processing-pangeo","position":0},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo"},"content":"\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo","position":1},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Snow Cover mapping with Pangeo ecosystem"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#snow-cover-mapping-with-pangeo-ecosystem","position":2},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Snow Cover mapping with Pangeo ecosystem"},"content":"In this exercise we will build a complete EO workflow using the Pangeo ecosystem on a cloud platform (EOxHub); from data access to obtaining the result. In this example we will analyse snow cover in the Alps.\n\nWe are going to follow these steps in our analysis:\n\nLoad satellite collections\n\nSpecify the spatial, temporal extents and the features we are interested in\n\nProcess the satellite data to retrieve snow cover information\n\nAggregate information to get catchment statistics over time\n\nVisualize and analyse the results\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#snow-cover-mapping-with-pangeo-ecosystem","position":3},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#libraries","position":4},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Libraries"},"content":"\n\nWe start by creating the shared folders and data files needed to complete the exercise using the following shell commands\n\n# platform libraries\n# utility libraries\nfrom datetime import date\nimport numpy as np\nimport xarray as xr\nimport rioxarray\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nfrom shapely.geometry import mapping\nimport pyproj\n\n# STAC Catalogue Libraries\nimport pystac_client\nimport stackstac\n\n# Data Visualization Libraries\nimport holoviews as hv\nimport hvplot.xarray\nimport hvplot.pandas\nimport folium\n\n# Dask library\nfrom dask.distributed import Client, progress, LocalCluster\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#libraries","position":5},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Get a client from the Dask  Cluster","lvl2":"Libraries"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#get-a-client-from-the-dask-cluster","position":6},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Get a client from the Dask  Cluster","lvl2":"Libraries"},"content":"Creating a Dask Client is mandatory in order to perform following Dask computations on your local Dask Cluster.\n\ncluster = LocalCluster(n_workers=2)\nclient = Client(cluster)  # create a local dask cluster on the machine.\nclient\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#get-a-client-from-the-dask-cluster","position":7},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Region of Interest"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#region-of-interest","position":8},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Region of Interest"},"content":"\n\nWe will use the catchment as our area of interest (AOI) for the analysis. Our region of interest is the Val Passiria Catchment in the South Tyrolian Alps (Italy). Let‚Äôs load the catchment area.\nThe catchment is defined by a polygon, which we will load from a GeoJSON file.\nThe GeoJSON file contains the geometry of the catchment in the WGS84 coordinate reference system (EPSG:4326) and that has to be defined.\n\ncatchment_outline = gpd.read_file('./31_data/catchment_outline.geojson', crs=\"EPGS:4326\")\naoi_geojson = mapping(catchment_outline.iloc[0].geometry)\n\n\n\ncenter_loc = catchment_outline.to_crs('+proj=cea').centroid.to_crs(epsg=\"4326\")\n\n\n\n# OpenStreetMap\nmap = folium.Map(location=[float(center_loc.y.iloc[0]), float(center_loc.x.iloc[0])], tiles=\"OpenStreetMap\", zoom_start=9)\nfor _, r in catchment_outline.iterrows():\n    sim_geo = gpd.GeoSeries(r[\"geometry\"]).simplify(tolerance=0.001)\n    geo_j = sim_geo.to_json()\n    geo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"fillColor\": \"orange\"})\n    folium.Popup(r[\"HYBAS_ID\"]).add_to(geo_j)\n    geo_j.add_to(map)\nmap\n\n\n\nQuiz hint: Look closely at the end of the displayed catchment area to identify the outlet\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#region-of-interest","position":9},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Satellite collections","lvl2":"Region of Interest"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#satellite-collections","position":10},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Satellite collections","lvl2":"Region of Interest"},"content":"","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#satellite-collections","position":11},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":"Search for satellite data using STAC","lvl3":"Satellite collections","lvl2":"Region of Interest"},"type":"lvl4","url":"/lectures/data-processing/exercises/data-processing-pangeo#search-for-satellite-data-using-stac","position":12},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":"Search for satellite data using STAC","lvl3":"Satellite collections","lvl2":"Region of Interest"},"content":"We will utilize the pystac_client to search for satellite data in this exercise, specifically leveraging data provided by AWS/Element84. When querying the satellite data we can add various filters such as spatial range, time period, and other specific metadata. This API is constructed based on the STAC specification, a collaborative, community-driven standard aimed at enhancing the discoverability and usability of satellite data. Numerous data providers, including AWS, Google Earth Engine, and Microsoft Planetary Computer and Copernicus Data Space Ecosystem (CDSE), among others, have implemented the STAC API, exemplifying its widespread adoption and utility in accessing diverse satellite datasets.\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#search-for-satellite-data-using-stac","position":13},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl5":"Set query filters","lvl4":"Search for satellite data using STAC","lvl3":"Satellite collections","lvl2":"Region of Interest"},"type":"lvl5","url":"/lectures/data-processing/exercises/data-processing-pangeo#set-query-filters","position":14},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl5":"Set query filters","lvl4":"Search for satellite data using STAC","lvl3":"Satellite collections","lvl2":"Region of Interest"},"content":"We define all extents before querying satellite data. For the purposes of this exercise, we will limit the search to the Sentinel 2 L2A collection, which is a collection of Sentinel 2 data that has been processed to surface reflectance (Top Of Canopy).\n\nWe are only interested in the green and short wave infrared bands, corresponding to band 3 (B03) and 11 (B11). We directly remove time slices with a cloud cover >= 90 %. We will also limit the search to the time period between 1st February 2018 and 10th June 2018 and to the extent of the catchment.\n\nbbox = catchment_outline.bounds.iloc[0]\nbbox\n\n\n\n#                  West,     South,     East,      North\nspatial_extent = [bbox[\"minx\"], bbox[\"miny\"], bbox[\"maxx\"], bbox[\"maxy\"]]\ntemporal_extent = ['2018-02-01T00:00:00Z','2018-06-30T00:00:00Z']\n\nbands = ['green', 'swir16', 'scl']\ncloud_coverage = [\"eo:cloud_cover<=90\"]\n\n\n\nURL = \"https://earth-search.aws.element84.com/v1\"\ncatalog = pystac_client.Client.open(URL)\nitems = catalog.search(\n    bbox=spatial_extent,\n    datetime=temporal_extent,\n    query=cloud_coverage,\n    collections=[\"sentinel-2-l2a\"]\n).item_collection()\n\n\n\nlen(items)\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#set-query-filters","position":15},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":"Get bands information","lvl3":"Satellite collections","lvl2":"Region of Interest"},"type":"lvl4","url":"/lectures/data-processing/exercises/data-processing-pangeo#get-bands-information","position":16},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":"Get bands information","lvl3":"Satellite collections","lvl2":"Region of Interest"},"content":"As the original data provides bands with different names than the original Sentinel 2 bands, we need to get the information about the bands.\n\n# Get bands information\nselected_item = items[1]\nfor key, asset in selected_item.assets.items():\n    print(f\"{key}: {asset.title}\")\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#get-bands-information","position":17},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":"Load data","lvl3":"Satellite collections","lvl2":"Region of Interest"},"type":"lvl4","url":"/lectures/data-processing/exercises/data-processing-pangeo#load-data","position":18},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":"Load data","lvl3":"Satellite collections","lvl2":"Region of Interest"},"content":"We will use the stackstac library to load the data. The stackstac library is a library that allows loading data from a STAC API into an xarray Dataset.\nHere we will load the green and swir16 bands (on the original dataset named B03 and B11), which are the bands we will use to calculate the snow cover. We will also load the scl band, which is the scene classification layer, which we will use to mask out clouds.\nSpatial resolution of 20m is selected for the analysis. The data is loaded in chunks of 2048x2048 pixels.\n\nStackstac is not the only way to create a xarray dataset from a STAC API. Other libraries can be used, such as \n\nxpystac or \n\nodc.stac. The choice of the library depends on the use case and specific needs.\n\ns2_cube = stackstac.stack(items,\n                     bounds_latlon=spatial_extent,\n                     resolution=20,\n                     assets=bands\n)\n\n\n\nExtract the data CRS (Coordinate Reference System), we will need it later on\n\ns2_crs = s2_cube.rio.crs\ns2_crs\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#load-data","position":19},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Calculate snow cover"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#calculate-snow-cover","position":20},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Calculate snow cover"},"content":"We will calculate the Normalized Difference Snow Index (NDSI) to calculate the snow cover. The NDSI is calculated as the difference between the green and the swir16 bands divided by the sum of the green and the swir16 bands:NDSI = \\frac {GREEN - SWIR} {GREEN +SWIR}\n\nFor a matter of clarity we will define the green and the swir16 bands as variables. Other approaches can be used to manage the data, but this is the one we will use in this exercise.\n\ngreen = s2_cube.sel(band='green')\nswir = s2_cube.sel(band='swir16')\nscl = s2_cube.sel(band='scl')\n\n\n\nLet‚Äôs compute the NDSI and mask out the clouds.\n\nndsi = (green - swir) / (green + swir).where((green + swir) != 0) \nndsi\n\n\n\nDask Method Differences: `.compute()` vs `.persist()`\n\nDask provides two primary methods for executing computations: .compute() and .persist(). Below is an overview of each method and their typical use cases.","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#calculate-snow-cover","position":21},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":".compute()","lvl2":"Calculate snow cover"},"type":"lvl4","url":"/lectures/data-processing/exercises/data-processing-pangeo#id-compute","position":22},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":".compute()","lvl2":"Calculate snow cover"},"content":"Functionality: Executes the Dask computation and blocks until the result is available. It then collects and returns the final result to the local process.\n\nUse Case: Invoke .compute() when you need to bring the computed result into your local memory. It is typically used as the final step in a Dask workflow after all transformations and computations have been defined.\n\nEvaluation: Eager - runs immediately and provides results.","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#id-compute","position":23},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":".persist()","lvl2":"Calculate snow cover"},"type":"lvl4","url":"/lectures/data-processing/exercises/data-processing-pangeo#id-persist","position":24},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl4":".persist()","lvl2":"Calculate snow cover"},"content":"Functionality: Begins computing the result in the background while immediately returning a new Dask object that represents the ongoing computation.\n\nUse Case: Utilize .persist() in a distributed environment when working with large datasets or complex computations that have expensive intermediate steps. This will keep the intermediate results in the cluster‚Äôs distributed memory, improving performance for subsequent computations.\n\nEvaluation: Lazy - computations are started but the method returns a reference to the future result without waiting for the completion.\n\nEach method plays a crucial role in optimizing and managing the execution of large-scale computations using Dask, particularly when balancing memory usage and computational efficiency in a distributed setting.\n\nndsi = ndsi.persist()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#id-persist","position":25},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Creating the Snow Map","lvl2":"Calculate snow cover"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#creating-the-snow-map","position":26},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Creating the Snow Map","lvl2":"Calculate snow cover"},"content":"So far we have a time series map of NDSI values. We are intereseted in the presence of snow though. Ideally in a binary classification: snow and no snow.\nTo achieve this we are setting a threshold of 0.4 on the NDSI. This gives us a binary snow map.\n\nsnow = xr.where((ndsi > 0.4) & ~np.isnan(ndsi), 1, ndsi)\nsnowmap = xr.where((snow <= 0.4) & ~np.isnan(snow), 0, snow)\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#creating-the-snow-map","position":27},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Creating a cloud mask","lvl2":"Calculate snow cover"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#creating-a-cloud-mask","position":28},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Creating a cloud mask","lvl2":"Calculate snow cover"},"content":"We are going to use the Scene Classification layer of Sentinel-2 (SCL) to create a cloud mask and then applying it to the NDSI. We will mask out the clouds, which are identified by the values 8 (cloud medium probability), 9 (cloud high probability) and 3 (cloud shadow) in the SCL layer.\n\nMore detailed info can be found here: \n\nhttps://‚Äãsentiwiki‚Äã.copernicus‚Äã.eu‚Äã/web‚Äã/s2‚Äã-processing‚Äã#S2‚Äã-Processing‚Äã-Scene‚Äã-Classification\n\ncloud_mask = np.logical_not(scl.isin([8, 9, 3])) \n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#creating-a-cloud-mask","position":29},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Calculate snow cover"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#applying-the-cloud-mask-to-the-snowmap","position":30},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Calculate snow cover"},"content":"We will mask out all pixels that are covered by clouds. This will result in: 0 = no_snow, 1 = snow, 2 = cloud\n\nsnowmap_cloudfree = xr.where(cloud_mask, snowmap, 2)\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#applying-the-cloud-mask-to-the-snowmap","position":31},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Process snow cover data"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#process-snow-cover-data","position":32},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Process snow cover data"},"content":"","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#process-snow-cover-data","position":33},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Mask data","lvl2":"Process snow cover data"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#mask-data","position":34},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Mask data","lvl2":"Process snow cover data"},"content":"As we are only interested to the snow cover in the catchment, we will mask out the data outside the catchment. To achieve it we need to convert the catchment geometry to the same coordinate reference system as the data. The data is in the UTM32N coordinate reference system (EPSG:32632), which is set in the s2_crs variable previously extracted from the datacube.\n\naoi_utm32 = catchment_outline.to_crs(crs=s2_crs)\ngeom_utm32 = aoi_utm32.iloc[0]['geometry']\n\n\n\nAs we are going to use the rioXarray library to mask out the data, we need to add some more information to the data. The rioXarray library is a library that allows to manipulate geospatial data in xarray datasets. Underneath it uses the rasterio library that is a library built on top of GDAL.\n\nWe need first to specify the coordinate reference system and the nodata value. Both information can be found in the metadata of the data but we need to reinforce it so that rioXarray can use it.\n\nsnowmap_cloudfree.rio.write_crs(s2_crs, inplace=True)\nsnowmap_cloudfree.rio.set_nodata(np.nan, inplace=True)\n\n\n\nLet‚Äôs clip the snow_cloud object using the catchment geometry in the UTM32N coordinate reference system.\n\nsnowmap_clipped = snowmap_cloudfree.rio.clip([geom_utm32])\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#mask-data","position":35},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Aggregate data","lvl2":"Process snow cover data"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#aggregate-data","position":36},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Aggregate data","lvl2":"Process snow cover data"},"content":"Data aggregation is a very important step in the analysis. It allows to reduce the amount of data and to make the analysis more efficient. Moreover, as in this case, we are going to aggregate the date to daily values, this will allow use to compute statistic on the data at the basin scale later on.\n\nThe groupby method allows to group the data by a specific dimension. We will group the data by the time dimension, aggregating to the date and removing the time information, once the group is obtained we will aggregate the data by taking the maximum value.\n\nclipped_date = snowmap_clipped.groupby(snowmap_clipped.time.dt.floor('D')).max(skipna=True)\n\n\n\nAs the data has been aggregated to daily values, we need to rename the floor dimension to something more meaningful as date.\n\nclipped_date = clipped_date.rename({'floor': 'date'})\nclipped_date\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#aggregate-data","position":37},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Calculate snow cover with apply_ufunc","lvl2":"Process snow cover data"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#calculate-snow-cover-with-apply-ufunc","position":38},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Calculate snow cover with apply_ufunc","lvl2":"Process snow cover data"},"content":"Calculate snow cover using Xarray's apply_ufunc\n\nThe procedure for computing snow cover can also be summed up as following python function.\n\nWe first verify that Green, swir16 and scl are in the order of 0,1,2 the variable in band variable. Then we simply copy and past all the python codes in a function.\n\ndef calculate_ndsi_snow_cloud(data):\n    green = data[0]\n    swir = data[1]\n    scl = data[2]\n    ndsi = np.where((green + swir) == 0, np.nan, (green - swir) / (green + swir)) \n    ndsi_mask = ( ndsi > 0.4 )& ~np.isnan(ndsi)\n    snow = np.where(ndsi_mask, 1, ndsi)\n    snowmap = np.where((snow <= 0.4) & ~np.isnan(snow), 0, snow)\n    mask = ~( (scl == 8) | (scl == 9) | (scl == 3) )\n    snow_cloud = np.where(mask, snowmap, 2)\n    return snow_cloud\n\n\n\n%%time\nda = stackstac.stack(items,\n                    bounds_latlon=catchment_outline.iloc[0].geometry.bounds,\n                    resolution=20,\n                    assets=bands)\n\n#Mask data\ngeom_utm32 = catchment_outline.to_crs(s2_crs).iloc[0]['geometry']\nda.rio.write_crs(s2_crs, inplace=True)\nda.rio.set_nodata(np.nan, inplace=True)\nda = da.rio.clip([geom_utm32])\n\nsnow_cloud_clipped = xr.apply_ufunc(\n    calculate_ndsi_snow_cloud\n    ,da\n    ,input_core_dims=[[\"band\",\"y\",\"x\"]]\n    ,output_core_dims=[[\"y\",\"x\"]]\n    ,exclude_dims=set([\"band\"])\n    ,vectorize=True\n    ,output_dtypes=[da.dtype],\n    dask=\"parallelized\",\n    dask_gufunc_kwargs={\"allow_rechunk\":True}\n    ).assign_attrs({'long_name': 'snow_cloud'}).to_dataset(name='snow_cloud')\n\nclipped_date = snow_cloud_clipped.groupby(snow_cloud_clipped.time.dt.floor('D')).max(skipna=True)\nclipped_date = clipped_date.rename({'floor': 'date'})\n\nclipped_date\n\n\n\nInspect the data dimentions!\n\nHow did change from input (da) to output (snow_cloud_clipped)?\n\nWhat is set as input_core_dims?\n\nWhat is set as output_core_dims?\n\nWhat is set as exclude_dims?\n\nDid you see 'time' dimension?\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#calculate-snow-cover-with-apply-ufunc","position":39},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Visualize data","lvl2":"Process snow cover data"},"type":"lvl3","url":"/lectures/data-processing/exercises/data-processing-pangeo#visualize-data","position":40},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl3":"Visualize data","lvl2":"Process snow cover data"},"content":"We will use the hvplot library to visualize the data. The library allows to visualize data in xarray datasets. It is based on the holoviews library, which is a library that allows to visualize multidimensional data.\nTo visualize the data on a map, we need to specify the coordinate reference system of the data. The data is in the UTM32N coordinate reference system (EPSG:32632). This will allow the library to project the data on a map.\nMore info on the hvplot library can be found here: \n\nhttps://‚Äãhvplot‚Äã.holoviz‚Äã.org/\n\nPlease note: running the next cell might take a couple of minutes. When interacting with the date scrollbar, the map takes some time to be updated, be patient.\n\nclipped_date.hvplot.image(\n    x='x',\n    y='y',\n    groupby='date',\n    crs=str(s2_crs),\n    cmap='Pastel2',\n    clim=(-1, 2),\n    frame_width=500,\n    frame_height=500,\n    title='Snowmap',\n    geo=True, tiles='OSM')\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#visualize-data","position":41},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Calculate Catchment Statistics"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#calculate-catchment-statistics","position":42},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Calculate Catchment Statistics"},"content":"Our objective is to monitor a specific area over a period of time, ensuring the data quality meets our standards. To achieve this, we determine the proportion of clouds in the watershed at each interval. This cloud coverage data serves to refine our timeline: we exclude any interval where cloud cover exceeds 25%.\n\nOur primary focus is on quantifying the Snow Covered Area (SCA) in the watershed. We tally the number of pixels depicting snow for each interval and calculate the SCA by multiplying the snowy pixels by the pixel‚Äôs area. To determine the extent of snow coverage, we calculate the percentage of snow-covered area by comparing the number of snowy pixels to the total pixel count within the watershed. This percentage is a key metric in our analysis.\n\nWe need to gather the total pixel counts for the entire watershed, as well as those specific to cloud and snow coverages.\n\n# number of cloud pixels\ncloud = xr.where(clipped_date == 2, 1, np.nan).count(dim=['x', 'y']).persist()\n\n\n\n# number of all pixels per each single date\naot_total = clipped_date.count(dim=['x', 'y']).persist()\n\n\n\n# Cloud fraction per each single date expressed in % \ncloud_fraction = (cloud / aot_total * 100).persist()\n\n\n\nRunning the next cell may require up to a couple of minutes.\n\n# Visualize cloud fraction\ncloud_fraction.hvplot.line(title='Cloud cover %', ylabel=\"&\") * hv.HLine(25).opts(\n    color='red',\n    line_dash='dashed',\n    line_width=2.0,\n)\n\n\n\nCompute the total number of cloudy pixels in the selected temporal period:\n\ncloud.snow_cloud.sum(dim=\"date\").values\n\n\n\nWe are going to get the same information for the snow cover.\n\nsnow = xr.where(clipped_date == 1, 1, np.nan).count(dim=['x', 'y'])\n\n\n\nsnow_fraction = (snow / aot_total * 100).persist()\n\n\n\n# visualize snow fraction\nsnow_fraction.hvplot.line(title='Snow cover area (%)', ylabel=\"%\")\n\n\n\n# mask out cloud fraction > 25% \nmasked_cloud_fraction = cloud_fraction.snow_cloud < 25\n\n\n\nsnow_selected = snow_fraction.sel(date=masked_cloud_fraction)\nsnow_selected\n\n\n\nsnow_selected = snow_selected.rename({\"snow_cloud\":\"SCA\"})\nsnow_selected.hvplot.line(title=\"Snow fraction\")\n\n\n\nSave the cloud filtered snow fraction\n\nsnow_selected.to_dataframe().to_csv(\"31_results/filtered_snow_fraction_pangeo.csv\")\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#calculate-catchment-statistics","position":43},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Shutdown and Close local Dask cluster"},"type":"lvl2","url":"/lectures/data-processing/exercises/data-processing-pangeo#shutdown-and-close-local-dask-cluster","position":44},{"hierarchy":{"lvl1":"3.1 Data Processing Pangeo","lvl2":"Shutdown and Close local Dask cluster"},"content":"\n\nclient.shutdown()\n\n\n\nclient.close()\n\n","type":"content","url":"/lectures/data-processing/exercises/data-processing-pangeo#shutdown-and-close-local-dask-cluster","position":45},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform"},"type":"lvl1","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform","position":0},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform"},"content":"\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform","position":1},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Snow Cover mapping with openEO Platform"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#snow-cover-mapping-with-openeo-platform","position":2},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Snow Cover mapping with openEO Platform"},"content":"\n\nIn this exercise we will build a complete EO workflow on a cloud platform; from data access to obtaining the result. In this example we will analyse snow cover in the Alps.\n\nWe are going to follow these steps in our analysis:\n\nLoad satellite collections\n\nSpecify the spatial, temporal extents and the features we are interested in\n\nProcess the satellite data to retrieve snow cover information\n\nAggregate information to get catchment statistics over time\n\nVisualize and analyse the results\n\nMore information on the openEO Python Client: \n\nhttps://‚Äãopen‚Äã-eo‚Äã.github‚Äã.io‚Äã/openeo‚Äã-python‚Äã-client‚Äã/index‚Äã.html\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#snow-cover-mapping-with-openeo-platform","position":3},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#libraries","position":4},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Libraries"},"content":"\n\n%%capture\npip install openeo rioxarray geopandas leafmap h5netcdf netcdf4 fiona\n\n\n\n# platform libraries\nimport openeo\n\n# utility libraries\nfrom datetime import date\nimport numpy as np\nimport xarray as xr\nimport rioxarray\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport leafmap.foliumap as leafmap\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#libraries","position":5},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Login"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#login","position":6},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Login"},"content":"Connect to the copernicus dataspace ecosystem.\n\nconn = openeo.connect('https://openeo.cloud/')\n\n\n\nAnd login\n\nconn.authenticate_oidc()\n\n\n\n\n\n\n\n\n\nCheck if the login worked.\n\nconn.describe_account()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#login","position":7},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Region of Interest"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#region-of-interest","position":8},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Region of Interest"},"content":"\n\nLoad the catchment area.\n\ncatchment_outline = gpd.read_file('../31_data/catchment_outline.geojson')\n\n\n\ncenter = (float(catchment_outline.centroid.y), float(catchment_outline.centroid.x))\nm = leafmap.Map(center=center, zoom=10)\nm.add_vector('../31_data/catchment_outline.geojson', layer_name=\"catchment\")\nm\n\n\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#region-of-interest","position":9},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Inspect Metadata"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#inspect-metadata","position":10},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Inspect Metadata"},"content":"We need to set the following configurations to define the content of the data cube we want to access:\n\ndataset name\n\nband names\n\ntime range\n\nthe area of interest specifed via bounding box coordinates\n\nspatial resolution\n\nTo select the correct dataset we can first list all the available datasets.\n\nconn.list_collections()\n\n\n\nWe want to use the Sentinel-2 L2A product. It‚Äôs name is 'SENTINEL2_L2A'.\n\nWe get the metadata for this collection as follows.\n\nconn.describe_collection(\"SENTINEL2_L2A\")\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#inspect-metadata","position":11},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Define a workflow"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#define-a-workflow","position":12},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Define a workflow"},"content":"We will define our workflow now. And chain all the processes together we need for analyzing the snow cover in the catchment.\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#define-a-workflow","position":13},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Define the data cube","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#define-the-data-cube","position":14},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Define the data cube","lvl2":"Define a workflow"},"content":"We define all extents of our data cube. We use the catchment as spatial extent. As a time range we will focus on the snow melting season 2018, in particular from Febraury to June 2018.\n\nbbox = catchment_outline.bounds.iloc[0]\nbbox\n\n\n\nfrom openeo.processes import lte\ncollection      = 'SENTINEL2_L2A'\nspatial_extent  = {'west':bbox[0],'east':bbox[2],'south':bbox[1],'north':bbox[3],'crs':4326}\ntemporal_extent = [\"2022-02-01\", \"2022-06-30\"]\nbands           = ['B03', 'B11']\nproperties={\"eo:cloud_cover\": lambda x: lte(x, 90)}\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#define-the-data-cube","position":15},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Load the data cube","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#load-the-data-cube","position":16},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Load the data cube","lvl2":"Define a workflow"},"content":"We have defined the extents we are interested in. Now we use these definitions to load the data cube.\n\ns2 = conn.load_collection(collection,\n                          spatial_extent=spatial_extent,\n                          bands=bands,\n                          temporal_extent=temporal_extent,\n                          properties=properties)\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#load-the-data-cube","position":17},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"NDSI - Normalized Difference Snow Index","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#ndsi-normalized-difference-snow-index","position":18},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"NDSI - Normalized Difference Snow Index","lvl2":"Define a workflow"},"content":"The Normalized Difference Snow Index (NDSI) is computed as:NDSI = \\frac {GREEN - SWIR} {GREEN +SWIR}\n\nWe have created a Sentinel-2 data cube with bands B03 (green), B11 (SWIR) and the cloud mask (CLM). We will use the green and SWIR band to calculate a the NDSI. This process is reducing the band dimension of the data cube to generate new information, the NDSI.\n\ngreen = s2.band(\"B03\")\nswir = s2.band(\"B11\")\nndsi = (green - swir) / (green + swir)\nndsi\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#ndsi-normalized-difference-snow-index","position":19},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Creating the Snow Map","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#creating-the-snow-map","position":20},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Creating the Snow Map","lvl2":"Define a workflow"},"content":"So far we have a timeseries of NDSI values. We are intereseted in the presence of snow though. Ideally in a binary classification: snow and no snow.\nTo achieve this we are setting a threshold of 0.42 on the NDSI. This gives us a binary snow map.\n\nsnowmap = ( ndsi > 0.42 ) * 1.0\nsnowmap\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#creating-the-snow-map","position":21},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Creating a cloud mask","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#creating-a-cloud-mask","position":22},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Creating a cloud mask","lvl2":"Define a workflow"},"content":"We are going to use ‚ÄúSCL‚Äù band for creating a cloud mask and then applying it to the NDSI.\n8 = cloud medium probability, 9 = cloud high probability, 3 = cloud shadow\n\nHere is more information on the Scene Classification \n\nhttps://‚Äãsentinels‚Äã.copernicus‚Äã.eu‚Äã/web‚Äã/sentinel‚Äã/technical‚Äã-guides‚Äã/sentinel‚Äã-2‚Äã-msi‚Äã/level‚Äã-2a‚Äã/algorithm‚Äã-overview\n\nValue\n\nLabel\n\n0\n\nNO_DATA\n\n1\n\nSATURATED_OR_DEFECTIVE\n\n2\n\nCAST_SHADOWS\n\n3\n\nCLOUD_SHADOWS\n\n4\n\nVEGETATION\n\n5\n\nNOT_VEGETATED\n\n6\n\nWATER\n\n7\n\nUNCLASSIFIED\n\n8\n\nCLOUD_MEDIUM_PROBABILITY\n\n9\n\nCLOUD_HIGH_PROBABILITY\n\n10\n\nTHIN_CIRRUS\n\n11\n\nSNOW or ICE\n\nscl_cube =conn.load_collection(\n    \"SENTINEL2_L2A\",\n    spatial_extent=spatial_extent,\n    bands=[\"SCL\"],\n    temporal_extent=temporal_extent,\n    max_cloud_cover=90,\n    \n)\n\n\n\nscl_band = scl_cube.band(\"SCL\")\ncloud_mask = ( (scl_band == 8) | (scl_band == 9) | (scl_band == 3) ) * 1.0\ncloud_mask\n\n\n\nThe SCL layer has a ground sample distance of 20 meter while the other bands have 10 meter GSD\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#creating-a-cloud-mask","position":23},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#applying-the-cloud-mask-to-the-snowmap","position":24},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Define a workflow"},"content":"We will mask out all pixels that are covered by clouds. This will result in: 0 = no_snow, 1 = snow, 2 = cloud\n\nsnowmap_cloudfree = snowmap.mask(cloud_mask,replacement=2) # replacement is null by default\nsnowmap_cloudfree\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#applying-the-cloud-mask-to-the-snowmap","position":25},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Mask Polygon: From Bounding Box to Shape","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#mask-polygon-from-bounding-box-to-shape","position":26},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl3":"Mask Polygon: From Bounding Box to Shape","lvl2":"Define a workflow"},"content":"Filter to the exact outline of the catchment: this should mask out the pixels outside of the catchment.\n\ncatchment_outline['geometry'][0]\n\n\n\nsnowmap_cloudfree_masked = snowmap_cloudfree.mask_polygon(catchment_outline['geometry'][0])\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#mask-polygon-from-bounding-box-to-shape","position":27},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl4":"Visualize one time step of the timeseries","lvl3":"Mask Polygon: From Bounding Box to Shape","lvl2":"Define a workflow"},"type":"lvl4","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#visualize-one-time-step-of-the-timeseries","position":28},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl4":"Visualize one time step of the timeseries","lvl3":"Mask Polygon: From Bounding Box to Shape","lvl2":"Define a workflow"},"content":"Let‚Äôs download the whole image time series as a netcdf file to have a look how our first results look like\n\nsnowmap_cloudfree_1d = snowmap_cloudfree_masked.filter_temporal('2022-02-10', '2022-02-12')\nsnowmap_cloudfree_1d.download('results/snowmap_cloudfree_1d.nc')\n\n\n\nxr.open_dataarray('results/snowmap_cloudfree_1d.nc',decode_coords=\"all\")[0].plot.imshow()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#visualize-one-time-step-of-the-timeseries","position":29},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Calculate Catchment Statistics"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#calculate-catchment-statistics","position":30},{"hierarchy":{"lvl1":"3.1 Data Processing OpenEO Platform","lvl2":"Calculate Catchment Statistics"},"content":"We are looking at a region over time. We need to make sure that the information content meets our expected quality. Therefore, we calculate the cloud percentage for the catchment for each timestep. We use this information to filter the timeseries. All timesteps that have a cloud coverage of over 25% will be discarded.\n\nUltimately we are interested in the snow covered area (SCA) within the catchment. We count all snow covered pixels within the catchment for each time step. Multiplied by the pixel size that would be the snow covered area. Divided the pixel count by the total number of pixels in the catchment is the percentage of pixels covered with snow. We will use this number.\n\nGet number of pixels in catchment: total, clouds, snow.\n\n# number of all pixels\nn_catchment = ((snowmap_cloudfree > -1) * 1.0).add_dimension(name=\"bands\",type=\"bands\",label=\"n_catchment\")\n\n# number of cloud pixels (no function needed, mask already created before)\nn_cloud = cloud_mask.add_dimension(name=\"bands\",type=\"bands\",label=\"n_cloud\")\n\n# number of snow pixels\nn_snow = ((snowmap_cloudfree == 1) * 1.0).add_dimension(name=\"bands\",type=\"bands\",label=\"n_snow\")\n\n# combine the binary data cubes into one data cube\nn_catchment_cloud_snow = n_catchment.merge_cubes(n_cloud).merge_cubes(n_snow)\n\n# aggregate to catchment\nn_pixels = n_catchment_cloud_snow.aggregate_spatial(geometries = catchment_outline['geometry'][0], reducer = 'sum')\nn_pixels\n\n\n\nCreate batch job to start processing on the backend.\n\n# Create a batch job\nn_pixels_json = n_pixels.save_result(format=\"JSON\")\njob = n_pixels_json.create_job(title=\"n_pixels_json\")\njob.start_job()\n\n\n\njob.status()\n\n\n\nif job.status() == \"finished\":\n    results = job.get_results()\n    results.download_files(\"results_openeo_platform/\")\n\n\n\nLoad the result. It contains the number of pixels in the catchment, clouds and snow.\n\nWe can calculate the percentages of cloud and snow pixels in the catchment.\n\nwith open(\"results_openeo_platform/timeseries.json\",\"r\") as file:\n    n_pixels_json = json.load(file)\n\n\n\n# check the first 5 entries\nlist(n_pixels_json.items())[:3] # careful unsorted dates due to JSON format\n\n\n\n# Create a Pandas DataFrame to contain the values\ndates = [k for k in n_pixels_json]\nn_catchment_vals = [n_pixels_json[k][0][0] for k in n_pixels_json]\nn_cloud_vals = [n_pixels_json[k][0][1] for k in n_pixels_json]\nn_snow_vals = [n_pixels_json[k][0][2] for k in n_pixels_json]\n\ndata = {\n        \"time\":pd.to_datetime(dates),\n        \"n_catchment_vals\":n_catchment_vals,\n        \"n_cloud_vals\":n_cloud_vals,\n        \"n_snow_vals\":n_snow_vals\n       }\ndf = pd.DataFrame(data=data).set_index(\"time\")\n# Sort the values by date\ndf = df.sort_values(axis=0,by=\"time\")\ndf[:3]\n\n\n\nDivide the number of cloudy pixels by the number of total pixels = cloud percentage\n\nperc_cloud = df[\"n_cloud_vals\"].values / df[\"n_catchment_vals\"].values * 100\ndf[\"perc_cloud\"] = perc_cloud\ndf[:3]\n\n\n\nPlot the timeseries and the cloud threshold of 25%. If the cloud cover is higher the timestep will be excluded later on.\n\nPlot the cloud percentage with the threshold.\n\ndf.plot(y=\"perc_cloud\",rot=45,kind=\"line\",marker='o')\nplt.axhline(y = 25, color = \"r\", linestyle = \"-\")\nplt.show()\n\n\n\nDivide the number of snow pixels by the number of total pixels = snow percentage\n\nperc_snow = df[\"n_snow_vals\"].values / df[\"n_catchment_vals\"].values * 100\ndf[\"perc_snow\"] = perc_snow\ndf[:3]\n\n\n\nPlot the unfiltered snow percentage\n\ndf.plot(y=\"perc_snow\",rot=45,kind=\"line\",marker='o')\nplt.show()\n\n\n\nKeep only the dates with cloud coverage less than the threshold\n\ndf_filtered = df.loc[df[\"perc_cloud\"]<25]\n\n\n\nPlot the cloud filtered snow percentage\n\ndf_filtered.plot(y=\"perc_snow\",rot=45,kind=\"line\",marker='o')\nplt.show()\n\n\n\nSave the cloud filtered snow percentage\n\ndf_filtered.to_csv(\"results_openeo_platform/filtered_snow_perc.csv\")\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-openeo-platform#calculate-catchment-statistics","position":31},{"hierarchy":{"lvl1":"3.1 Data Processing STAC"},"type":"lvl1","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac","position":0},{"hierarchy":{"lvl1":"3.1 Data Processing STAC"},"content":"\n\nIn this exercise we will build a complete EO workflow using cloud provided data (STAC Catalogue), processing it locally; from data access to obtaining the result. In this example we will analyse snow cover in the Alps.\n\nWe are going to follow these steps in our analysis:\n\nLoad satellite collections\n\nSpecify the spatial, temporal extents and the features we are interested in\n\nProcess the satellite data to retrieve snow cover information\n\naggregate information in data cubes\n\nVisualize and analyse the results\n\nMore information on the openEO Python Client: \n\nhttps://‚Äãopen‚Äã-eo‚Äã.github‚Äã.io‚Äã/openeo‚Äã-python‚Äã-client‚Äã/index‚Äã.html\n\nMore information on the Client Side Processing and load_stac functionalities: \n\nhttps://‚Äãopen‚Äã-eo‚Äã.github‚Äã.io‚Äã/openeo‚Äã-python‚Äã-client‚Äã/cookbook‚Äã/localprocessing‚Äã.html\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac","position":1},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Install missing and update packages:"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#install-missing-and-update-packages","position":2},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Install missing and update packages:"},"content":"\n\n%%capture\npip install rioxarray geopandas leafmap\n\n\n\n%%capture\npip install openeo[localprocessing] --upgrade\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#install-missing-and-update-packages","position":3},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#libraries","position":4},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Libraries"},"content":"\n\n# platform libraries\nfrom openeo.local import LocalConnection\n\n# utility libraries\nfrom datetime import date\nimport numpy as np\nimport xarray as xr\nimport rioxarray\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport leafmap.foliumap as leafmap\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#libraries","position":5},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Region of Interest"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#region-of-interest","position":6},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Region of Interest"},"content":"\n\nLoad the catchment area.\n\ncatchment_outline = gpd.read_file('../31_data/catchment_outline.geojson')\n\n\n\ncenter = (float(catchment_outline.centroid.y), float(catchment_outline.centroid.x))\nm = leafmap.Map(center=center, zoom=10)\nm.add_vector('../31_data/catchment_outline.geojson', layer_name=\"catchment\")\nm\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#region-of-interest","position":7},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Inspect STAC Metadata"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#inspect-stac-metadata","position":8},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Inspect STAC Metadata"},"content":"We need to set the following configurations to define the content of the data cube we want to access:\n\nSTAC Collection URL\n\nband names\n\ntime range\n\nthe area of interest specifed via bounding box coordinates\n\nWe use the Sentinel-2 L2A Collection from Microsoft: \n\nhttps://‚Äãplanetarycomputer‚Äã.microsoft‚Äã.com‚Äã/api‚Äã/stac‚Äã/v1‚Äã/collections‚Äã/sentinel‚Äã-2‚Äã-l2a\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#inspect-stac-metadata","position":9},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Define a workflow"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#define-a-workflow","position":10},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Define a workflow"},"content":"We will define our workflow now. And chain all the processes together we need for analyzing the snow cover in the catchment.\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#define-a-workflow","position":11},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Define the data cube","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#define-the-data-cube","position":12},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Define the data cube","lvl2":"Define a workflow"},"content":"We define all extents of our data cube. We use the catchment as spatial extent. As a time range we will focus on the snow melting season 2018, in particular from Febraury to June 2018.\n\nbbox = catchment_outline.bounds.iloc[0]\nbbox\n\n\n\nWe know that the catchment area is almost fully covered by the Sentinel-2 32TPS tile and therefore we use this information in the properties filter, along with a first filter on the cloud coverage.\n\nlocal_conn = LocalConnection(\"./\")\n\nurl = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a\"\nspatial_extent  = {\"west\":bbox[0],\"east\":bbox[2],\"south\":bbox[1],\"north\":bbox[3]}\ntemporal_extent = [\"2018-02-01\", \"2018-06-30\"]\n\nbands_11_scl    = [\"B11\", \"SCL\"]\nband_03        = [\"B03\"]\n\nproperties = {\"eo:cloud_cover\": dict(lt=75),\n              \"s2:mgrs_tile\": dict(eq=\"32TPS\")}\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#define-the-data-cube","position":13},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Load the data cube","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#load-the-data-cube","position":14},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Load the data cube","lvl2":"Define a workflow"},"content":"We have defined the extents we are interested in. Now we use these definitions to load the data cube.\n\nSince the B03 band have resolution of 10m and B11 and SCL 20m, we load them separately and then align in a second step using openEO.\n\ns2_B11_SCL = local_conn.load_stac(\n    url=url,\n    spatial_extent=spatial_extent,\n    temporal_extent=temporal_extent,\n    bands=bands_11_scl,\n    properties=properties,\n)\n\n\n\ns2_B03 = local_conn.load_stac(\n    url=url,\n    spatial_extent=spatial_extent,\n    temporal_extent=temporal_extent,\n    bands=band_03,\n    properties=properties,\n)\n\n\n\nUncomment the content of the next three cells if you would like to download the data first and then use the netCDFs to proceed.\n\nIt will download ~3 GB of data, make sure to have enough free space.\n\n# %%time\n# s2_11_scl_xr = s2_B11_SCL.execute()\n# # Remove problematic attributes and coordinates, which prevent to write a valid netCDF file\n# for at in s2_11_scl_xr.attrs:\n#     # allowed types: str, Number, ndarray, number, list, tuple\n#     if not isinstance(s2_11_scl_xr.attrs[at], (int, float, str, np.ndarray, list, tuple)):\n#         s2_11_scl_xr.attrs[at] = str(s2_11_scl_xr.attrs[at])\n\n# for c in s2_11_scl_xr.coords:\n#     if s2_11_scl_xr[c].dtype==\"object\":\n#         s2_11_scl_xr = s2_11_scl_xr.drop_vars(c)\n\n# s2_11_scl_xr.to_dataset(dim=\"band\").to_netcdf(\"s2_11_scl_xr.nc\")\n\n\n\n# %%time\n# s2_03_xr = s2_B03.execute()\n# # Remove problematic attributes and coordinates, which prevent to write a valid netCDF file\n# for at in s2_03_xr.attrs:\n#     # allowed types: str, Number, ndarray, number, list, tuple\n#     if not isinstance(s2_03_xr.attrs[at], (int, float, str, np.ndarray, list, tuple)):\n#         s2_03_xr.attrs[at] = str(s2_03_xr.attrs[at])\n\n# for c in s2_03_xr.coords:\n#     if s2_03_xr[c].dtype==\"object\":\n#         s2_03_xr = s2_03_xr.drop_vars(c)\n\n# s2_03_xr.to_dataset(dim=\"band\").to_netcdf(\"s2_03_xr.nc\")\n\n\n\n# s2_B03 = local_conn.load_collection(\"s2_03_xr.nc\")\n# s2_B11_SCL = local_conn.load_collection(\"s2_11_scl_xr.nc\")\n\n\n\ns2_20m = s2_B03.resample_cube_spatial(target=s2_B11_SCL,method=\"average\").merge_cubes(s2_B11_SCL)\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#load-the-data-cube","position":15},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"NDSI - Normalized Difference Snow Index","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#ndsi-normalized-difference-snow-index","position":16},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"NDSI - Normalized Difference Snow Index","lvl2":"Define a workflow"},"content":"The Normalized Difference Snow Index (NDSI) is computed as:NDSI = \\frac {GREEN - SWIR} {GREEN +SWIR}\n\nWe have created a Sentinel-2 data cube with bands B03 (green), B11 (SWIR) and the scene classification mask (SCL). We will use the green and SWIR band to calculate a the NDSI. This process is reducing the band dimension of the data cube to generate new information, the NDSI.\n\ngreen = s2_20m.band(\"B03\")\nswir = s2_20m.band(\"B11\")\nndsi = (green - swir) / (green + swir)\nndsi\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#ndsi-normalized-difference-snow-index","position":17},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Creating the Snow Map","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#creating-the-snow-map","position":18},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Creating the Snow Map","lvl2":"Define a workflow"},"content":"So far we have a timeseries of NDSI values. We are intereseted in the presence of snow though. Ideally in a binary classification: snow and no snow.\nTo achieve this we are setting a threshold of 0.42 on the NDSI. This gives us a binary snow map.\n\nndsi_mask = ( ndsi > 0.42 )\nsnowmap = ndsi_mask.add_dimension(name=\"band\",label=\"snow_map\",type=\"bands\")\nsnowmap\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#creating-the-snow-map","position":19},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Creating a cloud mask","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#creating-a-cloud-mask","position":20},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Creating a cloud mask","lvl2":"Define a workflow"},"content":"We are going to use ‚ÄúSCL‚Äù band for creating a cloud mask and then applying it to the NDSI.\n8 = cloud medium probability, 9 = cloud high probability, 3 = cloud shadow\n\nHere is more information on the Scene Classification \n\nhttps://‚Äãsentinels‚Äã.copernicus‚Äã.eu‚Äã/web‚Äã/sentinel‚Äã/technical‚Äã-guides‚Äã/sentinel‚Äã-2‚Äã-msi‚Äã/level‚Äã-2a‚Äã/algorithm‚Äã-overview\n\nValue\n\nLabel\n\n0\n\nNO_DATA\n\n1\n\nSATURATED_OR_DEFECTIVE\n\n2\n\nCAST_SHADOWS\n\n3\n\nCLOUD_SHADOWS\n\n4\n\nVEGETATION\n\n5\n\nNOT_VEGETATED\n\n6\n\nWATER\n\n7\n\nUNCLASSIFIED\n\n8\n\nCLOUD_MEDIUM_PROBABILITY\n\n9\n\nCLOUD_HIGH_PROBABILITY\n\n10\n\nTHIN_CIRRUS\n\n11\n\nSNOW or ICE\n\nscl_band = s2_20m.band(\"SCL\")\ncloud_mask = ( (scl_band == 8) | (scl_band == 9) | (scl_band == 3) ).add_dimension(name=\"band\",label=\"cloud_mask\",type=\"bands\")\ncloud_mask\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#creating-a-cloud-mask","position":21},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#applying-the-cloud-mask-to-the-snowmap","position":22},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Applying the cloud mask to the snowmap","lvl2":"Define a workflow"},"content":"We will mask out all pixels that are covered by clouds. This will result in: 0 = no_snow, 1 = snow, 2 = cloud\n\nsnowmap_cloudfree = snowmap.mask(cloud_mask,replacement=2) # replacement is null by default\nsnowmap_cloudfree\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#applying-the-cloud-mask-to-the-snowmap","position":23},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Visualize one time step of the timeseries","lvl2":"Define a workflow"},"type":"lvl3","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#visualize-one-time-step-of-the-timeseries","position":24},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl3":"Visualize one time step of the timeseries","lvl2":"Define a workflow"},"content":"Let‚Äôs create the lazy xarray view of the result and look how our first results look like\n\nsnowmap_cloudfree_1d = snowmap_cloudfree.filter_temporal('2018-02-10', '2018-02-12').mask_polygon(catchment_outline[\"geometry\"][0])\nsnowmap_cloudfree_1d_xr = snowmap_cloudfree_1d.execute()\n\n\n\nsnowmap_cloudfree_1d_xr[0,0].plot.imshow()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#visualize-one-time-step-of-the-timeseries","position":25},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Calculate Catchment Statistics"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#calculate-catchment-statistics","position":26},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Calculate Catchment Statistics"},"content":"We are looking at a region over time. We need to make sure that the information content meets our expected quality. Therefore, we calculate the cloud percentage for the catchment for each timestep. We use this information to filter the timeseries. All timesteps that have a cloud coverage of over 25% will be discarded.\n\nUltimately we are interested in the snow covered area (SCA) within the catchment. We count all snow covered pixels within the catchment for each time step. Multiplied by the pixel size that would be the snow covered area. Divided the pixel count by the total number of pixels in the catchment is the percentage of pixels covered with snow. We will use this number.\n\nGet number of pixels in catchment: total, clouds, snow.\n\nsnow_cloud_map_0 = (snowmap_cloudfree == 1).merge_cubes(cloud_mask)\nsnow_cloud_map = (ndsi_mask > -1).add_dimension(name=\"band\",label=\"valid_px\",type=\"bands\").merge_cubes(snow_cloud_map_0)\n\n\n\nAggregate to catchment using the aggregate_spatial process.\n\nsnow_cloud_map_timeseries = snow_cloud_map.aggregate_spatial(geometries=catchment_outline[\"geometry\"][0],reducer=\"sum\")\nsnow_cloud_map_timeseries\n\n\n\nGet the result as a Dask based xArray object\n\nsnow_cloud_map_timeseries_xr = snow_cloud_map_timeseries.execute()\nsnow_cloud_map_timeseries_xr\n\n\n\n\n\nCompute the result. Please note, this will trigger the download and processing of the requested data.\n\nsnow_cloud_map_timeseries_xr = snow_cloud_map_timeseries_xr.compute()\n\n\n\nsnow_cloud_map_timeseries_xr\n\n\n\nPlot the timeseries and the cloud threshold of 25%. If the cloud cover is higher the timestep will be excluded later on.\n\nPlot the cloud percentage with the threshold.s\n\ncloud_percent = (snow_cloud_map_timeseries_xr.loc[dict(band=\"cloud_mask\")] / snow_cloud_map_timeseries_xr.loc[dict(band=\"valid_px\")]) * 100\ncloud_percent.plot(marker='o')\n# plot the cloud percentage and a threshold\nplt.axhline(y = 25, color = \"r\", linestyle = \"-\")\nplt.show()\n\n\n\nPlot the unfiltered snow percentage\n\nsnow_percent = (snow_cloud_map_timeseries_xr.loc[dict(band=\"snow_map\")] / snow_cloud_map_timeseries_xr.loc[dict(band=\"valid_px\")]) * 100\nsnow_percent.plot(marker='o')\nplt.show()\n\n\n\nKeep only the dates with cloud coverage less than the threshold\n\nsnow_percent = snow_percent.where(cloud_percent<25,drop=True)\n\n\n\nPlot the cloud filtered snow percentage\n\nsnow_percent.plot(marker='o')\nplt.show()\n\n\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#calculate-catchment-statistics","position":27},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Compare to discharge data"},"type":"lvl2","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#compare-to-discharge-data","position":28},{"hierarchy":{"lvl1":"3.1 Data Processing STAC","lvl2":"Compare to discharge data"},"content":"Load the discharge data at Meran. The main outlet of the catchment.\n\ndischarge_ds = pd.read_csv('../../../3.2_validation/exercises/32_data/ADO_DSC_ITH1_0025.csv', sep=',', index_col='Time', parse_dates=True)\ndischarge_ds.head()\n\n\n\nCompare the discharge data to the snow covered area.\n\nfig,ax0 = plt.subplots(1, figsize=(10,5),sharey=True)\nax1 = ax0.twinx() \n\nstart_date = date(2018, 2, 1)\nend_date = date(2018, 6, 30)\n# filter discharge data to start and end dates\ndischarge_ds = discharge_ds.loc[start_date:end_date]\n\ndischarge_ds.discharge_m3_s.plot(label='Discharge', xlabel='', ylabel='Discharge (m$^3$/s)',ax=ax0)\nsnow_percent.plot(marker='o',ax=ax1,color='orange')\nax0.legend(loc='center left', bbox_to_anchor=(0, 0.6))\nax1.set_ylabel('Snow cover area (%)')\nax1.legend(loc='center left', bbox_to_anchor=(0, 0.5),labels=['SCA'])\nplt.show()\n\n","type":"content","url":"/lectures/data-processing/exercises/alternatives/data-processing-stac#compare-to-discharge-data","position":29},{"hierarchy":{"lvl1":"Validation"},"type":"lvl1","url":"/lectures/validation/validation","position":0},{"hierarchy":{"lvl1":"Validation"},"content":"","type":"content","url":"/lectures/validation/validation","position":1},{"hierarchy":{"lvl1":"Validation","lvl2":"Learning objectives"},"type":"lvl2","url":"/lectures/validation/validation#learning-objectives","position":2},{"hierarchy":{"lvl1":"Validation","lvl2":"Learning objectives"},"content":"Get to know typical validation strategies in EO\n\nUnderstand where uncertainties appear in a workflow by design\n\nUnderstand why validation is important\n\nThe validation of large scale mapping products\n\nValidate some pixels of the snow cover area map","type":"content","url":"/lectures/validation/validation#learning-objectives","position":3},{"hierarchy":{"lvl1":"Validation","lvl2":"What is validation"},"type":"lvl2","url":"/lectures/validation/validation#what-is-validation","position":4},{"hierarchy":{"lvl1":"Validation","lvl2":"What is validation"},"content":"The validation process typically involves comparing a model or a developed Earth Observation (EO) product with reference data, and the level of agreement is assessed using validation metrics. Validating EO products is crucial to prevent misinterpretation or error propagation when utilizing the data for purposes such as area quantification, subsequent modeling, or planning, particularly in contexts like nature conservation or risk assessment. However, validating EO products poses significant challenges.\n\nIn this tutorial, we will explain how to derive validation metrics and how to interpret them. Our primary focus will center on the difficulties and limitations inherent in the accuracy assessment process, especially in the context of large-scale (global) mapping products based on Earth Observation.","type":"content","url":"/lectures/validation/validation#what-is-validation","position":5},{"hierarchy":{"lvl1":"Validation","lvl2":"Critically Analyse a workflow"},"type":"lvl2","url":"/lectures/validation/validation#critically-analyse-a-workflow","position":6},{"hierarchy":{"lvl1":"Validation","lvl2":"Critically Analyse a workflow"},"content":"Identify sources of uncertainty in the applied workflow\n\nProcess graph with pop-ups of sources of uncertainties\n\nStrategies of how to improve\n\nNow that we have carried out a very basic approach to solve our research question we should take some time to identify possible sources of uncertainty and think about how to improve them:\n\nOptical earth observation has some inherent drawbacks, most importantly: clouds. Especially in mountain regions.\n\nWe are excluding images where a certain cloud coverage is exceeded. There would still be some information available.\n\nWe are not filling in the gaps that clouds generate. This leaves us with some uncertainty.\n\nUse data fusion techniques and include SAR data, that can penetrate the clouds.\n\nSentinel-2 has a 6 day repeat rate. This means we do not know what happens with the snow cover in between two acquisitions.\n\nUse data fusion techniques and include other optical sensors and SAR data\n\nUse physical snow models or heuristics to estimate the snow cover in between\n\nWe are using a threshold for discriminating between snow and no snow. Changing this arbitrary value will influence our results.\n\nThere are better, more complex ways to identify snow.\n\nSnow Cover does not represent the amount of snow.\n\nTherefore we would need to calculate the snow depth.\n\nOr better the Snow Water Equivalent.","type":"content","url":"/lectures/validation/validation#critically-analyse-a-workflow","position":7},{"hierarchy":{"lvl1":"Validation","lvl2":"Typical validation approaches"},"type":"lvl2","url":"/lectures/validation/validation#typical-validation-approaches","position":8},{"hierarchy":{"lvl1":"Validation","lvl2":"Typical validation approaches"},"content":"","type":"content","url":"/lectures/validation/validation#typical-validation-approaches","position":9},{"hierarchy":{"lvl1":"Validation","lvl3":"Reference data","lvl2":"Typical validation approaches"},"type":"lvl3","url":"/lectures/validation/validation#reference-data","position":10},{"hierarchy":{"lvl1":"Validation","lvl3":"Reference data","lvl2":"Typical validation approaches"},"content":"Reference data for EO are commonly obtained through field surveys or visual expert assessments of the underlying EO data. These reference datasets play a dual role, serving not only for validation purposes but also as essential components for training models, particularly when the EO product relies on predictions from a data-driven model. Consequently, when referring to reference data, a broad distinction is made between training and test data. The training dataset is employed in the model development phase, while the test dataset is crucial for evaluating the quality of the resulting product, specifically assessing the accuracy of predictions generated by the model.","type":"content","url":"/lectures/validation/validation#reference-data","position":11},{"hierarchy":{"lvl1":"Validation","lvl3":"Model validation and map validation","lvl2":"Typical validation approaches"},"type":"lvl3","url":"/lectures/validation/validation#model-validation-and-map-validation","position":12},{"hierarchy":{"lvl1":"Validation","lvl3":"Model validation and map validation","lvl2":"Typical validation approaches"},"content":"Many EO products are generated using data-driven models, which can range from simple rule-based models to more complex machine learning models. In the process of creating such EO products, two distinct validation steps are crucial: model validation and map validation.\nDuring model validation, we evaluate the model‚Äôs ability to predict the target variable (e.g., snow cover) based on EO data (e.g., optical satellite imagery). This evaluation often involves cross-validation, where the training data are divided into multiple folds. Iteratively, one fold is withheld during model training, and these reserved data are used to test the model‚Äôs performance in predicting unseen data. Cross-validation typically includes tuning the model (adjusting hyperparameters, selecting variables) to identify the optimal model for predicting the held-back data.\nIf (and only if) the training data and the derived cross-validation folds are representative for the prediction area (see discussion later), the cross-validation performance may be used as an indicator for the map accuracy.\n\nTo properly measure the map accuracy, a probability sample of the prediction area is required. This might be a random sample of the entire area that is used to describe the fit between the prediction (i.e. the map) and the reference.\nHowever, in numerous scientific publications, this essential step is often omitted, and model performance alone is presented as the sole indicator for map accuracy. The following section outlines the risks associated with this practice.","type":"content","url":"/lectures/validation/validation#model-validation-and-map-validation","position":13},{"hierarchy":{"lvl1":"Validation","lvl3":"Validation metrics","lvl2":"Typical validation approaches"},"type":"lvl3","url":"/lectures/validation/validation#validation-metrics","position":14},{"hierarchy":{"lvl1":"Validation","lvl3":"Validation metrics","lvl2":"Typical validation approaches"},"content":"Validation metrics summarize the fit between predictions and reference. For continuous variables (e.g. snow depth), Root Mean Square Error or Coefficient of Determination are commonly used validation metrics. For categorical variables (e.g. land cover), Accuracy or F1 score are frequently used summary statistics. For binary classifications, the area under the ROC curve may be used. However, there are many more validation metrics expressing the fit between prediction and reference by focusing on different aspects.","type":"content","url":"/lectures/validation/validation#validation-metrics","position":15},{"hierarchy":{"lvl1":"Validation","lvl2":"Validation strategies in the absence of a probability sample"},"type":"lvl2","url":"/lectures/validation/validation#validation-strategies-in-the-absence-of-a-probability-sample","position":16},{"hierarchy":{"lvl1":"Validation","lvl2":"Validation strategies in the absence of a probability sample"},"content":"When reference data are randomly distributed across the prediction area, validation metrics can be computed by comparing predictions and reference through a randomly selected subset of the entire reference dataset used as test data. Alternatively, in cross-validation, the training data may be randomly partitioned into multiple folds. However, the availability of design-based samples is infrequent, particularly in large-scale mapping endeavors like global applications.\nTypically, reference data are sourced from extensive databases, such as soil profiles or vegetation surveys, resulting in high clustering within areas that have been extensively studied or are easily accessible. Conversely, certain areas may entirely lack reference data, as illustrated in the accompanying figure.\n\nFigure 1: Comparison between 1000 randomly sampled reference data (left) and a highly clustered sample of the same size (right) that is typical for many environmental data sets. Reference: \n\nMeyer & Pebesma (2022)\n\nWhen such data are randomly split into training and test sets or cross-validation folds, a significant issue arises: the lack of independence between training and test data. This stems from the fact that both sets originate from the same geographic areas, whereas the trained model is deployed to make predictions for much larger areas without available reference data. Ploton et al., 2020, illustrate the consequences: overly optimistic validation statistics that deviate from the actual quality of the map.\n\nTo address this challenge, various spatial data splitting methods have been proposed. These methods involve splitting reference data based on spatial units, spatial blocks, or by considering spatial autocorrelation, all with the aim of ensuring independence between training and test data (e.g. Brenning 2012, Roberts et al., 2017, Valavi et al., 2019) or representativeness for the prediction task (Mila 2022, Linnenbrink 2023).","type":"content","url":"/lectures/validation/validation#validation-strategies-in-the-absence-of-a-probability-sample","position":17},{"hierarchy":{"lvl1":"Validation","lvl2":"Limits to accuracy assessment"},"type":"lvl2","url":"/lectures/validation/validation#limits-to-accuracy-assessment","position":18},{"hierarchy":{"lvl1":"Validation","lvl2":"Limits to accuracy assessment"},"content":"Employing validation strategies tailored for spatial data enables us to offer the most accurate estimates of map accuracy, albeit with certain limitations. Reflecting on the reference data illustrated in Figure 1, large areas lack coverage from any reference data. While it is technically feasible to generate predictions for these areas, the question arises: is this a reasonable approach?\n\nAssuming that new geographical spaces often goes along with new environmental conditions, it becomes likely that our models may not be applicable to these environments due to non-applicable relationships. For instance, consider reference data for vegetation traits sampled in low elevations; it raises questions about the model‚Äôs applicability to high elevations where the traits might be influenced by different factors. This challenge is particularly pronounced when employing machine learning models, as their extrapolation abilities are often limited. When making predictions, the model is compelled to extend its predictions into unknown areas, making predictions for regions beyond the trained data range highly uncertain.\nThis, however, is not reflected by the validation statistics that were calculated based on the reference data, hence knowledge on the performance in the data-poor regions is not included. As a result, the statistics fail to reflect the accuracy of the model in these areas, and predictions for regions outside the original data range should be approached with caution due to their inherent uncertainty.\n\nIt is therefore important to limit predictions to the area where the model was trained and validated for. Meyer and Pebesma 2021 provide one suggestion to derive the ‚Äúarea of applicability‚Äù of prediction models that is based on distances to reference data in the predictor space. Other suggestion limit predictions to the geographic proximity of reference data (Sabatini et al., 2022).","type":"content","url":"/lectures/validation/validation#limits-to-accuracy-assessment","position":19},{"hierarchy":{"lvl1":"Validation","lvl2":"Communication of validation"},"type":"lvl2","url":"/lectures/validation/validation#communication-of-validation","position":20},{"hierarchy":{"lvl1":"Validation","lvl2":"Communication of validation"},"content":"As outlined above, the step of accuracy assessment involves considerable considerations on the data used for evaluation and requires awareness on the area these statistics are considered valid for. This challenge becomes particularly crucial when reference data fail to represent a comprehensive sample of the entire prediction area, a common scenario in many geoscience applications - to avoid overly optimistic performance estimates and hence false conclusions on the map accuracy. The validation procedure hence needs to be carefully communicated alongside the predictions. The resulting EO product (i.e. the prediction map) should be limited to the area for which the model was enabled to learn about relationships and for which performance estimates can be reliably provided for. This can be done by either masking the map or by providing an additional quality layer.\n\n\n\nVideo content in cooperation with \n\nHannah Meyer (University of M√ºnster). \n‚ÄúValidation isn‚Äôt optional. It‚Äôs a must.‚Äù","type":"content","url":"/lectures/validation/validation#communication-of-validation","position":21},{"hierarchy":{"lvl1":"Validation","lvl2":"Exercise"},"type":"lvl2","url":"/lectures/validation/validation#exercise","position":22},{"hierarchy":{"lvl1":"Validation","lvl2":"Exercise"},"content":"Let‚Äôs apply some validation steps on a cloud platform in practice!\n\nComplete firstly the exercise using openEO:\n\nExercise 3.2 Validation with openEO\n\nAfterwards, do the same using Pangeo:\n\nExercise 3.2 Validation with Pangeo","type":"content","url":"/lectures/validation/validation#exercise","position":23},{"hierarchy":{"lvl1":"Validation","lvl2":"Quiz"},"type":"lvl2","url":"/lectures/validation/validation#quiz","position":24},{"hierarchy":{"lvl1":"Validation","lvl2":"Quiz"},"content":"","type":"content","url":"/lectures/validation/validation#quiz","position":25},{"hierarchy":{"lvl1":"Validation","lvl3":"Theory","lvl2":"Quiz"},"type":"lvl3","url":"/lectures/validation/validation#theory","position":26},{"hierarchy":{"lvl1":"Validation","lvl3":"Theory","lvl2":"Quiz"},"content":"What are common problems in creating and validating global maps?[[x]] The spatial distribution of reference data: There are usually areas in the world where reference data is clustered and areas where there is hardly any data available.\n[[ ]] None: We have cloud computing that can scale to produce global maps and machine learning models can automatically account for data sparse regions.\n[[x]] The availablility of reference data: Some biophysical indicators are not measured frequently in space and time in the field (e.g. leaf area index, snow water equivalent)\n\nWhat is the Area of Applicability?[( )] It's the topic the map covers (e.g. vegetation cover)\n[( )] It's the extent of the map.\n[(x)] It's the area of the map where the values are representable.","type":"content","url":"/lectures/validation/validation#theory","position":27},{"hierarchy":{"lvl1":"Validation","lvl3":"Exercises","lvl2":"Quiz"},"type":"lvl3","url":"/lectures/validation/validation#exercises","position":28},{"hierarchy":{"lvl1":"Validation","lvl3":"Exercises","lvl2":"Quiz"},"content":"How many snow stations are in the catchment? Answer in the exercise: 32_validation_openeo.ipynb section ‚ÄòLoad snow-station in-situ data‚Äô[( )] 3\n[( )] 7\n[(x)] 5\n\nWhich openEO process is used to extract the time series of the snow covered area at the station locations? Answer in the exercise: 32_validation_openeo.ipynb[( )] reduce_spatial\n[( )] resample_cube\n[(x)] aggregate_spatial\n\nWith Pangeo, which package and method are used to group data by the time dimension? Answer in the exercise: 32_validation_pangeo.ipynb[(x)] Xarray Python package and the groupby method\n[( )] Dask Python package and the clip method\n[( )] Xarray Python package with both clip and groupby methods\n\nWhich is the station where the mapped snow cover has the lowest accuracy? Answer in the exercise: 32_validation_openeo.ipynb section ‚ÄòValidate the SCA results with the snow station measurements‚Äô[( )] Rifiano Beobachter\n[( )] Saint Leonardo in Passiria Osservatore\n[(x)] Plata Osservatore\n\nWhen is the date with the maximum runoff/discharge? Answer in the exercise: 32_validation_openeo.ipynb section ‚ÄòCompare to discharge data‚Äô[( )] 2018-06-03\n[( )] 2018-04-17\n[(x)] 2018-05-03\n\nHow is the relation between snow cover and runoff/discharge? Answer in the exercise: 32_validation_openeo.ipynb or 32_validation_pangeo.ipynb section ‚ÄòCompare to discharge data‚Äô[( )] When the snow cover is high, also the runoff is high.\n[(x)] Snow melt is followed by increased runoff.\n[( )] Snow melt is followed by reduced runoff.","type":"content","url":"/lectures/validation/validation#exercises","position":29},{"hierarchy":{"lvl1":"Validation","lvl2":"Further Reading"},"type":"lvl2","url":"/lectures/validation/validation#further-reading","position":30},{"hierarchy":{"lvl1":"Validation","lvl2":"Further Reading"},"content":"Video: \n\nMeyer, Hanna: Machine learning-based maps of the environment: challenges of extrapolation and overfitting. OpenGeoHub Summer School 2022 - KISTE project workshop, OpenGeoHub Foundation, 2022. https://doi.org/10.5446/59412","type":"content","url":"/lectures/validation/validation#further-reading","position":31},{"hierarchy":{"lvl1":"Validation","lvl2":"References"},"type":"lvl2","url":"/lectures/validation/validation#references","position":32},{"hierarchy":{"lvl1":"Validation","lvl2":"References"},"content":"Meyer, H., Pebesma, E. Machine learning-based global maps of ecological variables and the challenge of assessing them. Nat Commun 13, 2208 (2022). \n\nMeyer & Pebesma (2022)\n\nMeyer, H., & Pebesma, E. (2021). Predicting into unknown space? Estimating the area of applicability of spatial prediction models. Methods in Ecology and Evolution, 12, 1620‚Äì1633. \n\nMeyer & Pebesma (2021)\n\nLoew et al. (2017): \n\nLoew et al. (2017)\n\nBrenning et al. (2012): \n\nBrenning (2012)\n\nValavi et al. (2019): \n\nValavi et al. (2018)\n\nRoberts et al. (2017): \n\nRoberts et al. (2017)\n\nPloton et al. (2020): \n\nPloton et al. (2020)\n\nMil√† et al. (2022): \n\nMil√† et al. (2022)\n\nLinnenbrink et al. (2023): \n\nLinnenbrink et al. (2023)\n\nSabatini et al. (2022): \n\nSabatini et al. (2022)","type":"content","url":"/lectures/validation/validation#references","position":33},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO"},"type":"lvl1","url":"/lectures/validation/exercises/validation-openeo","position":0},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO"},"content":"\n\n\n\nIn this exercise, we focus on the validation of the results we have produced. In general, the accuracy of a satellite derived product is expressed by comparing it to in-situ measurements. Furthermore, we will compare the resuling snow cover time series to the runoff of the catchment to check the plausibility of the observed relationship.\n\nThe steps involved in this analysis:\n\nGenerate Datacube time-series of snowmap,\n\nLoad in-situ datasets: snow depth station measurements,\n\nPre-process and filter in-situ datasets to match area of interest,\n\nPerform validation of snow-depth measurements,\n\nPlausibility check with runoff of the catchment\n\nMore information on the openEO Python Client: \n\nhttps://‚Äãopen‚Äã-eo‚Äã.github‚Äã.io‚Äã/openeo‚Äã-python‚Äã-client‚Äã/index‚Äã.html\n\nStart by creating the folders and data files needed to complete the exercise.\n\n!mkdir -p 32_results\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo","position":1},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#libraries","position":2},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Libraries"},"content":"\n\nimport json\nfrom datetime import date\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr\nimport rioxarray as rio\n\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom rasterio.plot import show\n\nimport geopandas as gpd\nimport leafmap.foliumap as leafmap\n\nimport openeo\nfrom _32_openeo_utilities import ( calculate_sca,\n                                 station_temporal_filter,\n                                 station_spatial_filter,\n                                 binarize_snow,\n                                 format_date,\n                                 assign_site_snow,\n                                 validation_metrics)\n\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#libraries","position":3},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Login"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#login","position":4},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Login"},"content":"Connect to the Copernicus Dataspace Ecosystem.\n\nconn = openeo.connect('https://openeo.dataspace.copernicus.eu/')\n\n\n\nLogin.\n\nconn.authenticate_oidc()\n\n\n\nCheck if the login worked.\n\nconn.describe_account()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#login","position":5},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Region of Interest"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#region-of-interest","position":6},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Region of Interest"},"content":"\n\nLoad the Val Passiria Catchment, our region of interest. And plot it.\n\ncatchment_outline = gpd.read_file('32_data/catchment_outline.geojson')\n\n\n\ncenter = (float(catchment_outline.centroid.y), float(catchment_outline.centroid.x))\nm = leafmap.Map(center=center, zoom=10)\nm.add_vector('32_data/catchment_outline.geojson', layer_name=\"catchment\")\nm\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#region-of-interest","position":7},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Generate Datacube of Snowmap"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#generate-datacube-of-snowmap","position":8},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Generate Datacube of Snowmap"},"content":"\n\nWe have prepared the workflow to generate the snow map as a python function calculate_sca(). The calculate_sca() is from _32_openeo_utilities and is used to reproduce the snow map process graph in openEO\n\nbbox = catchment_outline.bounds.iloc[0]\ntemporal_extent = [\"2018-02-01\", \"2018-06-30\"]\nsnow_map_cloud_free = calculate_sca(conn, bbox, temporal_extent)\nsnow_map_cloud_free\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#generate-datacube-of-snowmap","position":9},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Load snow-station in-situ data"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#load-snow-station-in-situ-data","position":10},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Load snow-station in-situ data"},"content":"Load the in-situ datasets, snow depth station measurements. They have been compiled in the ClirSnow project and are available here: \n\nSnow Cover in the European Alps with stations in our area of interest.\n\nWe have made the data available for you already. We can load it directly.\n\n# load snow station datasets from zenodo:: https://zenodo.org/record/5109574\nstation_df = pd.read_csv(\"32_data/data_daily_IT_BZ.csv\")\nstation_df = station_df.assign(Date=station_df.apply(format_date, axis=1))\n# the format_date function, from _32_openeo_utilities was used to stringify each Datetime object in the dataframe\n# station_df.head()\n\n\n\n# load additional metadata for acessing the station geometries\nstation_df_meta = pd.read_csv(\"32_data/meta_all.csv\")\nstation_df_meta.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#load-snow-station-in-situ-data","position":11},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#pre-process-and-filter-in-situ-snow-station-measurements","position":12},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"","type":"content","url":"/lectures/validation/exercises/validation-openeo#pre-process-and-filter-in-situ-snow-station-measurements","position":13},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Filter Temporally","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#filter-temporally","position":14},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Filter Temporally","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Filter the in-situ datasets to match the snow-map time series using the function station_temporal_filter() from _32_openeo_utilities.py, which merges the station dataframe with additional metadata needed for the Lat/Long information and convert them to geometries\n\nstart_date = \"2018-02-01\"\nend_date = \"2018-06-30\"\n\nsnow_stations = station_temporal_filter(station_daily_df = station_df, \n                                        station_meta_df = station_df_meta,\n                                        start_date = start_date,\n                                        end_date = end_date)\nsnow_stations.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#filter-temporally","position":15},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Filter Spatially","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#filter-spatially","position":16},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Filter Spatially","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Filter the in-situ datasets into the catchment area of interest using station_spatial_filter() from _32_openeo_utilities.py.\n\ncatchment_stations = station_spatial_filter(snow_stations, catchment_outline)\ncatchment_stations.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#filter-spatially","position":17},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Plot the filtered stations","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#plot-the-filtered-stations","position":18},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Plot the filtered stations","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Visualize location of snow stations\n\nprint(\"There are\", len(np.unique(catchment_stations.Name)), \"unique stations within our catchment area of interest\")\n\n\n\nQuick Hint: Remember the number of stations within the catchment for the final quiz exercise\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#plot-the-filtered-stations","position":19},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Convert snow depth to snow presence","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#convert-snow-depth-to-snow-presence","position":20},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Convert snow depth to snow presence","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"The stations are measuring snow depth. We only need the binary information on the presence of snow (yes, no). We use the binarize_snow()  function from _32_openeo_utilities.py to assign 0 for now snow and 1 for snow in the snow_presence column.\n\ncatchment_stations = catchment_stations.assign(snow_presence=catchment_stations.apply(binarize_snow, axis=1))\ncatchment_stations.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#convert-snow-depth-to-snow-presence","position":21},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Save the pre-processed snow station measurements","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#save-the-pre-processed-snow-station-measurements","position":22},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Save the pre-processed snow station measurements","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Save snow stations within catchment as GeoJSON\n\nwith open(\"32_results/catchment_stations.geojson\", \"w\") as file:\n    file.write(catchment_stations.to_json())\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#save-the-pre-processed-snow-station-measurements","position":23},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Extract SCA from the data cube per station"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#extract-sca-from-the-data-cube-per-station","position":24},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Extract SCA from the data cube per station"},"content":"\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#extract-sca-from-the-data-cube-per-station","position":25},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Prepare snow station data for usage in openEO","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#prepare-snow-station-data-for-usage-in-openeo","position":26},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Prepare snow station data for usage in openEO","lvl2":"Extract SCA from the data cube per station"},"content":"Create a buffer of approximately 80 meters (0.00075 degrees) around snow stations and visualize them.\n\ncatchment_stations_gpd = gpd.read_file(\"32_results/catchment_stations.geojson\")\nmappy = leafmap.Map(center=center, zoom=16)\nmappy.add_vector('32_data/catchment_outline.geojson', layer_name=\"catchment\")\nmappy.add_gdf(catchment_stations_gpd, layer_name=\"catchment_station\")\n\ncatchment_stations_gpd[\"geometry\"] = catchment_stations_gpd.geometry.buffer(0.00075)\nmappy.add_gdf(catchment_stations_gpd, layer_name=\"catchment_station_buffer\")\nmappy\n\n\n\nConvert the unique geometries to Feature Collection to be used in a openEO process.\n\ncatchment_stations_fc = json.loads(\n    catchment_stations_gpd.geometry.iloc[:5].to_json()\n)\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#prepare-snow-station-data-for-usage-in-openeo","position":27},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Extract SCA from the data cube per station","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#extract-sca-from-the-data-cube-per-station-1","position":28},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Extract SCA from the data cube per station","lvl2":"Extract SCA from the data cube per station"},"content":"We exgtract the SCA value of our data cube at the buffered station locations. Therefore we use the process aggregate_spatial() with the aggregation method median(). This gives us the most common value in the buffer (snow or snowfree).\n\nsnowmap_per_station= snow_map_cloud_free.aggregate_spatial(catchment_stations_fc, reducer=\"median\")\nsnowmap_per_station\n\n\n\nCreate a batch job on the cloud platform. And start it.\n\nsnowmap_cloudfree_json = snowmap_per_station.save_result(format=\"JSON\")\njob = snowmap_cloudfree_json.create_job(title=\"snow_map_\")\njob.start_and_wait()\n\n\n\nCheck the status of the job. And download once it‚Äôs finished.\n\njob.status()\n\n\n\nif job.status() == \"finished\":\n    results = job.get_results()\n    results.download_files(\"32_results/snowmap/\")\n\n\n\nOpen the snow covered area time series extracted at the stations. We‚Äôll have a look at it in a second.\n\nwith open(\"32_results/snowmap/timeseries.json\",\"r\") as file:\n    snow_time_series = json.load(file)\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#extract-sca-from-the-data-cube-per-station-1","position":29},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#combine-station-measurements-and-the-extracted-sca-from-our-data-cube","position":30},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"The station measurements are daily and all of the stations are combined in one csv file.\nThe extracted SCA values are in the best case six-daily (Sentinel-2 repeat rate) and also all stations are in one json file.\nWe will need to join the the extracted SCA with the station measurements by station (and time (selecting the corresponding time steps)\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#combine-station-measurements-and-the-extracted-sca-from-our-data-cube","position":31},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Extract snow values from SCA extracted at the station location","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#extract-snow-values-from-sca-extracted-at-the-station-location","position":32},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Extract snow values from SCA extracted at the station location","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"Let‚Äôs have a look at the data structure first\n\ndates = [k.split(\"T\")[0] for k in snow_time_series]\nsnow_val_smartino = [snow_time_series[k][0][0] for k in snow_time_series]\nsnow_val_rifiano = [snow_time_series[k][1][0] for k in snow_time_series]\nsnow_val_plata = [snow_time_series[k][2][0] for k in snow_time_series]\nsnow_val_sleonardo = [snow_time_series[k][3][0] for k in snow_time_series]\nsnow_val_scena = [snow_time_series[k][4][0] for k in snow_time_series]\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#extract-snow-values-from-sca-extracted-at-the-station-location","position":33},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Match in-situ measurements to dates in SCA","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#match-in-situ-measurements-to-dates-in-sca","position":34},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Match in-situ measurements to dates in SCA","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"Let‚Äôs have a look at the in-situ measurement data set.\n\ncatchment_stations_gpd.sample(10)\n\n\n\nWe are going to extract each station and keep only the dates that are available in the SCA results.\n\ncatchment_stations_gpd_smartino = catchment_stations_gpd.query(\"Name == 'S_Martino_in_Passiria_Osservatore'\")\ncatchment_stations_gpd_smartino = catchment_stations_gpd_smartino[\n    catchment_stations_gpd_smartino.id.isin(dates)\n]\n\ncatchment_stations_gpd_rifiano = catchment_stations_gpd.query(\"Name == 'Rifiano_Beobachter'\")\ncatchment_stations_gpd_rifiano = catchment_stations_gpd_rifiano[\n    catchment_stations_gpd_rifiano.id.isin(dates)\n]\n\ncatchment_stations_gpd_plata = catchment_stations_gpd.query(\"Name == 'Plata_Osservatore'\")\ncatchment_stations_gpd_plata = catchment_stations_gpd_plata[\n    catchment_stations_gpd_plata.id.isin(dates)\n]\n\ncatchment_stations_gpd_sleonardo = catchment_stations_gpd.query(\"Name == 'S_Leonardo_in_Passiria_Osservatore'\")\ncatchment_stations_gpd_sleonardo = catchment_stations_gpd_sleonardo[\n    catchment_stations_gpd_sleonardo.id.isin(dates)\n]\n\ncatchment_stations_gpd_scena = catchment_stations_gpd.query(\"Name == 'Scena_Osservatore'\")\ncatchment_stations_gpd_scena = catchment_stations_gpd_scena[\n    catchment_stations_gpd_scena.id.isin(dates)\n]\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#match-in-situ-measurements-to-dates-in-sca","position":35},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Combine in-situ measurements with SCA results at the stations","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl3","url":"/lectures/validation/exercises/validation-openeo#combine-in-situ-measurements-with-sca-results-at-the-stations","position":36},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl3":"Combine in-situ measurements with SCA results at the stations","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"The in situ measurements and the SCA are combined into one data set per station. This will be the basis for the validation.\n\nsmartino_snow = assign_site_snow(catchment_stations_gpd_smartino, snow_val_smartino)\nrifiano_snow = assign_site_snow(catchment_stations_gpd_rifiano, snow_val_rifiano)\nplata_snow = assign_site_snow(catchment_stations_gpd_plata, snow_val_plata)\nsleonardo_snow = assign_site_snow(catchment_stations_gpd_sleonardo, snow_val_sleonardo)\nscena_snow = assign_site_snow(catchment_stations_gpd_scena, snow_val_scena)                                                                    \n\n\n\nLet‚Äôs have a look at the SCA extracted at the station Plata Osservatore and it‚Äôs in situ measurements.\n\ncatchment_stations_gpd_plata.sample(5)\n\n\n\nDisplay snow presence threshold in in-situ data for Plato Osservatore\n\ncatchment_stations_gpd_plata.plot(x=\"id\", y=\"HS_after_gapfill\",rot=45,kind=\"line\",marker='o')\nplt.axhline(y = 0.4, color = \"r\", linestyle = \"-\")\nplt.show()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#combine-in-situ-measurements-with-sca-results-at-the-stations","position":37},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Validate the SCA results with the snow station measurements"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#validate-the-sca-results-with-the-snow-station-measurements","position":38},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Validate the SCA results with the snow station measurements"},"content":"Now that we have combined the SCA results with the snow station measurements we can start the actual validation. A confusion matrix compares the classes of the station data to the classes of the SCA result. The numbers can be used to calculate the accuracy (correctly classified cases / all cases).\n\n\n\nno_snow\n\nsnow\n\nno_snow\n\ncorrect\n\nerror\n\nsnow\n\nerror\n\ncorrect\n\nplata_snow\n\n\n\nvalidation_metrics(smartino_snow)[1]\n\n\n\nimport seaborn as sns\n\n\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 6))\n\nfig.suptitle(\"Error matrices for snow stations within our selected Catchment\")\nsns.heatmap(validation_metrics(smartino_snow)[1], cmap=\"Purples\", annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax1)\nax1.set_title(\"San Martino in Passiria Osservatore\")\nax1.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n\nsns.heatmap(validation_metrics(rifiano_snow)[1], cmap=\"Greens\", annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax2)\nax2.set_title(\"Rifiano Beobachter\")\nax2.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n\nsns.heatmap(validation_metrics(plata_snow)[1], cmap=\"Oranges\", annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax3)\nax3.set_title(\"Plata Osservatore\")\nax3.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n\nsns.heatmap(validation_metrics(scena_snow)[1], cmap=\"Reds\", annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax4)\nax4.set_title(\"Scena Osservatore\")\nax4.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\nfig.tight_layout()\n\n\n\nThe accuracy of the snow estimate from the satellite image computation for each station is shown below:\n\nOn-site snow station\n\nAccuracy\n\nSan Martino in Passiria Osservatore\n\n100.00%\n\nRifiano Beobachter\n\n100.00%\n\nPlata Osservatore\n\n82.61%\n\nSan Leonardo in Passiria Osservatore\n\nNaN\n\nScena Osservatore\n\n96.15%\n\nThe foruth station San Leonardo in Passiria Osservatore recorded NaNs for snow depths for our selected dates, which could potentially be as a results of malfunctioning on-site equipments. Hence, we are not able to verify for it. But overall, the validation shows a 100% accuracy for stations San Martino in Passiria Osservatore and Rifiano Beobachter, while stations Plata Osservatore and Scena Osservatore have a more False Positive than the other stations. This shows a good match between estimated snow values from satellite datasets and on-the ground measurements of the presence of snow.\n\n","type":"content","url":"/lectures/validation/exercises/validation-openeo#validate-the-sca-results-with-the-snow-station-measurements","position":39},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Compare to discharge data"},"type":"lvl2","url":"/lectures/validation/exercises/validation-openeo#compare-to-discharge-data","position":40},{"hierarchy":{"lvl1":"3.2 Validation of the results with openEO","lvl2":"Compare to discharge data"},"content":"In addition to computing metrics for validating the data, we also check the plausibility of our results. We compare our results with another measure with a known relationship. In this case, we compare the snow cover area time series with the discharge time-series at the main outlet of the catchment. We suspect that after snow melting starts, with a temporal lag, the runoff will increase. Let‚Äôs see if this holds true.\n\nLoad the discharge data at Meran, the main outlet of the catchment. We have prepared this data set for you, it‚Äôs extracted from Eurac‚Äôs \n\nEnvironmental Data Platform Alpine Drought Observatory Discharge Hydrological Datasets).\n\ndischarge_ds = pd.read_csv('32_data/ADO_DSC_ITH1_0025.csv', \n                           sep=',', index_col='Time', parse_dates=True)\ndischarge_ds.head()\n\n\n\nLoad the SCA time series we have generated in a previous exercise. It‚Äôs the time series of the aggregated snow cover area percentage for the whole catchment.  Please note: you need to complete the 3.1 exercise before proceeding!\n\nsnow_perc_df = pd.read_csv(\"../../3.1_data_processing/exercises/31_results/filtered_snow_perc.csv\", \n                          sep=',', index_col='time', parse_dates=True)\n\n\n\nLet‚Äôs plot the relationship between the snow covered area and the discharge in the catchment.\n\nstart_date = date(2018, 2, 1)\nend_date = date(2018, 6, 30)\n# filter discharge data to start and end dates\ndischarge_ds = discharge_ds.loc[start_date:end_date]\n\nax1 = discharge_ds.discharge_m3_s.plot(label='Discharge', xlabel='', ylabel='Discharge (m$^3$/s)')\nax2 = snow_perc_df[\"perc_snow\"].plot(marker='o', secondary_y=True, label='SCA', xlabel='', ylabel='Snow cover area (%)')\nax1.legend(loc='center left', bbox_to_anchor=(0, 0.6))\nax2.legend(loc='center left', bbox_to_anchor=(0, 0.5))\nplt.show()\n\n\n\nThe relationship looks as expected! Once the snow cover decreases the runoff increases!","type":"content","url":"/lectures/validation/exercises/validation-openeo#compare-to-discharge-data","position":41},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo"},"type":"lvl1","url":"/lectures/validation/exercises/validation-pangeo","position":0},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo"},"content":"\n\n\n\nIn this exercise, we focus on the validation of the results we have produced when using the Pangeo ecosystem. In general, the accuracy of a satellite derived product is expressed by comparing it to in-situ measurements. Furthermore, we will compare the resulting snow cover time series to the runoff of the catchment to check the plausibility of the observed relationship.\n\nThe steps involved in this analysis:\n\nGenerate Datacube time-series of snowmap,\n\nLoad in-situ datasets: snow depth station measurements,\n\nPre-process and filter in-situ datasets to match area of interest,\n\nPerform validation of snow-depth measurements,\n\nPlausibility check with runoff of the catchment\n\nStart by creating the folders and data files needed to complete the exercise.\n\n!mkdir -p 32_results\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo","position":1},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#libraries","position":2},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Libraries"},"content":"\n\nimport json\nfrom datetime import date\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr\nimport rioxarray as rio\n\nimport matplotlib.pyplot as plt\nimport rasterio\nfrom rasterio.plot import show\n\nimport geopandas as gpd\nimport folium\n\nfrom _32_pangeo_utilities import ( calculate_sca,\n                                 station_temporal_filter,\n                                 station_spatial_filter,\n                                 binarize_snow,\n                                 assign_site_snow,\n                                 validation_metrics)\nimport os\nimport warnings;\nwarnings.filterwarnings('ignore');\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#libraries","position":3},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Region of Interest"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#region-of-interest","position":4},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Region of Interest"},"content":"\n\nLoad the Val Passiria Catchment, our region of interest. And plot it.\n\ncatchment_outline = gpd.read_file('32_data/catchment_outline.geojson', crs=\"EPGS:4326\")\ncatchment_outline\n\n\n\ncenter_loc = catchment_outline.to_crs('+proj=cea').centroid.to_crs(epsg=\"4326\")\n\n\n\n# OpenStreetMap\nmap = folium.Map(location=[float(center_loc.y.iloc[0]), float(center_loc.x.iloc[0])], tiles=\"OpenStreetMap\", zoom_start=9)\ngeo_j = catchment_outline[\"geometry\"].to_json()\ngeo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"fillColor\": \"orange\"})\ngeo_j.add_to(map)\nmap\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#region-of-interest","position":5},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Generate Datacube of Snowmap"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#generate-datacube-of-snowmap","position":6},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Generate Datacube of Snowmap"},"content":"\n\nWe have prepared the workflow to generate the snow map as a python function calculate_sca(). The calculate_sca() is from _32_pangeo_utilities and is used to reproduce the snow map process graph using the Pangeo software stack.\n\nstart_date = \"2018-02-01\"\nend_date = \"2018-06-30\"\nbbox = tuple(catchment_outline.bounds.iloc[0])\ntemporal_extent = [start_date, end_date]\nsnow_map_cloud_free = calculate_sca(bbox, temporal_extent)\nsnow_map_cloud_free\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#generate-datacube-of-snowmap","position":7},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Load snow-station in-situ data"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#load-snow-station-in-situ-data","position":8},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Load snow-station in-situ data"},"content":"Load the in-situ datasets, snow depth station measurements. They have been compiled in the ClirSnow project and are available here: \n\nSnow Cover in the European Alps with stations in our area of interest.\n\nWe have made the data available for you already. We can load it directly.\n\n# load snow station datasets from zenodo:: https://zenodo.org/record/5109574\nstation_df = pd.read_csv(\"32_data/data_daily_IT_BZ.csv\", parse_dates=[\"Date\"], date_format=\"%d.%m.%y\")\nstation_df.head()\n\n\n\n# load additional metadata for acessing the station geometries\nstation_df_meta = pd.read_csv(\"32_data/meta_all.csv\")\nstation_df_meta.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#load-snow-station-in-situ-data","position":9},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#pre-process-and-filter-in-situ-snow-station-measurements","position":10},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"","type":"content","url":"/lectures/validation/exercises/validation-pangeo#pre-process-and-filter-in-situ-snow-station-measurements","position":11},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Filter Temporally","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#filter-temporally","position":12},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Filter Temporally","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Filter the in-situ datasets to match the snow-map time series using the function station_temporal_filter() from _32_pangeo_utilities.py, which merges the station dataframe with additional metadata needed for the Lat/Long information and convert them to geometries\n\nsnow_stations = station_temporal_filter(station_daily_df = station_df, \n                                        station_meta_df = station_df_meta,\n                                        start_date = start_date,\n                                        end_date = end_date)\nsnow_stations.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#filter-temporally","position":13},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Filter Spatially","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#filter-spatially","position":14},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Filter Spatially","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Filter the in-situ datasets into the catchment area of interest using station_spatial_filter() from cubes_utilities.py.\n\ncatchment_stations = station_spatial_filter(snow_stations, catchment_outline)\ncatchment_stations.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#filter-spatially","position":15},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Plot the filtered stations","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#plot-the-filtered-stations","position":16},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Plot the filtered stations","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Visualize location of snow stations\n\nprint(\"There are\", len(np.unique(catchment_stations.Name)), \"unique stations within our catchment area of interest\")\n\n\n\nQuick Hint: Remember the number of stations within the catchment for the final quiz exercise\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#plot-the-filtered-stations","position":17},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Convert snow depth to snow presence","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#convert-snow-depth-to-snow-presence","position":18},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Convert snow depth to snow presence","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"The stations are measuring snow depth. We only need the binary information on the presence of snow (yes, no). We use the binarize_snow()  function from cubes_utilities.py to assign 0 for now snow and 1 for snow in the snow_presence column.\n\ncatchment_stations = catchment_stations.assign(snow_presence=catchment_stations.apply(binarize_snow, axis=1))\ncatchment_stations.head()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#convert-snow-depth-to-snow-presence","position":19},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Save the pre-processed snow station measurements","lvl2":"Pre-process and filter in-situ snow station measurements"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#save-the-pre-processed-snow-station-measurements","position":20},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Save the pre-processed snow station measurements","lvl2":"Pre-process and filter in-situ snow station measurements"},"content":"Save snow stations within catchment as GeoJSON\n\nwith open(\"32_results/catchment_stations_pangeo.geojson\", \"w\") as file:\n    file.write(catchment_stations.to_json())\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#save-the-pre-processed-snow-station-measurements","position":21},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Extract SCA from the data cube per station"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#extract-sca-from-the-data-cube-per-station","position":22},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Extract SCA from the data cube per station"},"content":"\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#extract-sca-from-the-data-cube-per-station","position":23},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Prepare snow station data for usage in Pangeo","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#prepare-snow-station-data-for-usage-in-pangeo","position":24},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Prepare snow station data for usage in Pangeo","lvl2":"Extract SCA from the data cube per station"},"content":"Create a buffer of approximately 80 meters (0.00075 degrees) around snow stations and visualize them.\n\ncatchment_stations_gpd = gpd.read_file(\"32_results/catchment_stations_pangeo.geojson\")\n\n# OpenStreetMap\nmap = folium.Map(location=[float(center_loc.y.iloc[0]), float(center_loc.x.iloc[0])], tiles=\"OpenStreetMap\", zoom_start=10)\n\n# catchment\ncatchment_layer = folium.FeatureGroup(name=\"catchment\", show=True).add_to(map)\nfolium.GeoJson(data=catchment_outline[\"geometry\"].to_json(), style_function=lambda x: {\"fillColor\": \"orange\"}).add_to(catchment_layer)\n\n# catchment stations\nstations_layer = folium.FeatureGroup(name=\"catchment stations\", show=True).add_to(map)\n\nfor _, r in catchment_stations_gpd[[\"Longitude\", \"Latitude\"]].drop_duplicates().iterrows():\n    # Place the markers with the popup labels and data\n    folium.Marker(location=[r[\"Latitude\"], r[\"Longitude\"]],\n                  popup=\"Latitude: \" + str(r[\"Latitude\"]) \n                  + \"<br>\" \n                  + \"Longitude: \" + str(r[\"Longitude\"])\n                 ).add_to(stations_layer)\n    \n# catchment buffer\nbuffer_layer = folium.FeatureGroup(name=\"catchment station buffer\", show=True).add_to(map)\ncatchment_stations_gpd[\"geometry\"] = catchment_stations_gpd.geometry.buffer(0.00075)\n\nfor _, r in catchment_stations_gpd[[\"geometry\"]].drop_duplicates().iterrows():\n    # Place the markers with the popup labels and data\n    folium.GeoJson(data=catchment_stations_gpd[\"geometry\"].to_json(), style_function=lambda x: {\"color\": \"#0F7229\", \"fillOpacity\": 0}).add_to(buffer_layer)\n\n\nfolium.LayerControl().add_to(map)\nmap\n\n\n\nGet the unique geometries for each catchment station buffer\n\ncatchment_stations_gpd.geometry.drop_duplicates()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#prepare-snow-station-data-for-usage-in-pangeo","position":25},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Extract SCA from the data cube per station","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#extract-sca-from-the-data-cube-per-station-1","position":26},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Extract SCA from the data cube per station","lvl2":"Extract SCA from the data cube per station"},"content":"We extract the SCA value of our data cube at the buffered station locations. Therefore we use the process aggregate_spatial() with the aggregation method median(). This gives us the most common value in the buffer (snow or snowfree).\n\nsnow_map_cloud_free.rio.write_crs(\"EPSG:32632\", inplace=True)\nsnow_map_cloud_free.rio.set_nodata(np.nan, inplace=True)\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#extract-sca-from-the-data-cube-per-station-1","position":27},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Reduce the amount of data by selecting a small Area Of Interest (AOI)","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#reduce-the-amount-of-data-by-selecting-a-small-area-of-interest-aoi","position":28},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Reduce the amount of data by selecting a small Area Of Interest (AOI)","lvl2":"Extract SCA from the data cube per station"},"content":"\n\ncatchment_stations_gpd_utm32 = catchment_stations_gpd.to_crs(epsg=32632)\nminx, miny, maxx, maxy = catchment_stations_gpd_utm32[[\"geometry\"]].drop_duplicates().total_bounds\n\n\n\nsnowmap_clipped = snow_map_cloud_free.sel(x=slice(minx,maxx), y = slice(maxy, miny))\nsnowmap_clipped\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#reduce-the-amount-of-data-by-selecting-a-small-area-of-interest-aoi","position":29},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Aggregate to daily values","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#aggregate-to-daily-values","position":30},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Aggregate to daily values","lvl2":"Extract SCA from the data cube per station"},"content":"Data aggregation is a very important step in the analysis. It allows to reduce the amount of data and to make the analysis more efficient. Moreover as in this case we are going to aggregate the date to daily values, this will allow use to compute statistic on the data at the basin scale later on.\n\nThe groupby method allows to group the data by the time dimension, aggregating to the date and removing the time information, once the group is obtained we will aggregate the data by taking the max value.\n\ngeoms = []\nfor _, r in catchment_stations_gpd_utm32[[\"geometry\"]].drop_duplicates().iterrows():\n    geoms.append(r[\"geometry\"])\n\nsnowmap_clipped = snow_map_cloud_free.rio.clip(geoms).groupby(snow_map_cloud_free.time.dt.floor('D')).max(dim=\"time\")\nsnowmap_clipped = snowmap_clipped.rename({\"floor\":\"time\"})\n\n\n\nIt‚Äôs time to persist the data in memory. We will use the persist method to load the data in memory and keep it there until the end of the analysis.\n\n%%time\nsnowmap_clipped.persist()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#aggregate-to-daily-values","position":31},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Extract SCA from the data cube per station","lvl2":"Extract SCA from the data cube per station"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#extract-sca-from-the-data-cube-per-station-2","position":32},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Extract SCA from the data cube per station","lvl2":"Extract SCA from the data cube per station"},"content":"We extract the SCA value of our data cube at the buffered station locations. Therefore we use the aggregation method median(). This gives us the most common value in the buffer (snow or snowfree).\n\nPlease note: this step may take around 5 minutes!\n\n%%time\nx = []\nfor _, r in catchment_stations_gpd_utm32[[\"geometry\"]].drop_duplicates().iterrows():\n    snowmap_station = snowmap_clipped.rio.clip([r[\"geometry\"]])\n    snowmap_station.persist()\n    median = snowmap_station.median([\"x\",\"y\"])\n    x.append(median.to_pandas())\n\n\n\nSave the values into csv files\n\n%%time\n\nif not os.path.exists(\"32_results/snowmap_pangeo/\"):\n    os.makedirs(\"32_results/snowmap_pangeo\")\nfor idx,r in catchment_stations_gpd_utm32[[\"Name\"]].drop_duplicates().iterrows():\n    print(idx, r[\"Name\"])\n    x[idx].to_csv(\"32_results/snowmap_pangeo/\" + r[\"Name\"] + \".csv\")\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#extract-sca-from-the-data-cube-per-station-2","position":33},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#combine-station-measurements-and-the-extracted-sca-from-our-data-cube","position":34},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"The station measurements are daily and all of the stations are combined in one csv file.\nThe extracted SCA values are in the best case six-daily (Sentinel-2 repeat rate) and also all stations are in one json file.\nWe will need to join the the extracted SCA with the station measurements by station (and time (selecting the corresponding time steps)\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#combine-station-measurements-and-the-extracted-sca-from-our-data-cube","position":35},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Extract snow values from SCA extracted at the station location","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#extract-snow-values-from-sca-extracted-at-the-station-location","position":36},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Extract snow values from SCA extracted at the station location","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"Let‚Äôs have a look at the data structure first\n\nOpen the snow covered area time series extracted at the stations. We‚Äôll have a look at it in a second.\n\nx = []\nfor idx,r in catchment_stations_gpd[[\"Name\"]].drop_duplicates().iterrows():\n    print(idx, r[\"Name\"])\n    x.append(pd.read_csv(\"32_results/snowmap_pangeo/\" + r[\"Name\"] + \".csv\", parse_dates=[\"time\"], index_col=\"time\"))\n\n\n\ndates = x[0].index.tolist()\nsnow_val_smartino = [y[0] for y in x[0].values]\nsnow_val_rifiano = [y[0] for y in x[1].values]\nsnow_val_plata = [y[0] for y in x[2].values]\nsnow_val_sleonardo = [y[0] for y in x[3].values]\nsnow_val_scena = [y[0] for y in x[4].values]\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#extract-snow-values-from-sca-extracted-at-the-station-location","position":37},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Match in-situ measurements to dates in SCA","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#match-in-situ-measurements-to-dates-in-sca","position":38},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Match in-situ measurements to dates in SCA","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"Let‚Äôs have a look at the in-situ measurement data set.\n\ncatchment_stations_gpd = gpd.read_file(\"32_results/catchment_stations_pangeo.geojson\")\n\n\n\nConvert column ‚Äúid‚Äù from strings to dates to enable selection by dates\n\ncatchment_stations_gpd[\"id\"] = pd.to_datetime(catchment_stations_gpd[\"id\"])\n\n\n\nWe are going to extract each station and keep only the dates that are available in the SCA results.\n\ncatchment_stations_gpd_smartino = catchment_stations_gpd.query(\"Name == 'S_Martino_in_Passiria_Osservatore'\")\ncatchment_stations_gpd_smartino = catchment_stations_gpd_smartino[\n    catchment_stations_gpd_smartino.id.isin(dates)\n]\n\ncatchment_stations_gpd_rifiano = catchment_stations_gpd.query(\"Name == 'Rifiano_Beobachter'\")\ncatchment_stations_gpd_rifiano = catchment_stations_gpd_rifiano[\n    catchment_stations_gpd_rifiano.id.isin(dates)\n]\n\ncatchment_stations_gpd_plata = catchment_stations_gpd.query(\"Name == 'Plata_Osservatore'\")\ncatchment_stations_gpd_plata = catchment_stations_gpd_plata[\n    catchment_stations_gpd_plata.id.isin(dates)\n]\n\ncatchment_stations_gpd_sleonardo = catchment_stations_gpd.query(\"Name == 'S_Leonardo_in_Passiria_Osservatore'\")\ncatchment_stations_gpd_sleonardo = catchment_stations_gpd_sleonardo[\n    catchment_stations_gpd_sleonardo.id.isin(dates)\n]\n\ncatchment_stations_gpd_scena = catchment_stations_gpd.query(\"Name == 'Scena_Osservatore'\")\ncatchment_stations_gpd_scena = catchment_stations_gpd_scena[\n    catchment_stations_gpd_scena.id.isin(dates)\n]\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#match-in-situ-measurements-to-dates-in-sca","position":39},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Combine in-situ measurements with SCA results at the stations","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"type":"lvl3","url":"/lectures/validation/exercises/validation-pangeo#combine-in-situ-measurements-with-sca-results-at-the-stations","position":40},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl3":"Combine in-situ measurements with SCA results at the stations","lvl2":"Combine station measurements and the extracted SCA from our data cube"},"content":"The in situ measurements and the SCA are combined into one data set per station. This will be the basis for the validation.\n\nsmartino_snow = assign_site_snow(catchment_stations_gpd_smartino, snow_val_smartino)\nrifiano_snow = assign_site_snow(catchment_stations_gpd_rifiano, snow_val_rifiano)\nplata_snow = assign_site_snow(catchment_stations_gpd_plata, snow_val_plata)\nsleonardo_snow = assign_site_snow(catchment_stations_gpd_sleonardo, snow_val_sleonardo)\nscena_snow = assign_site_snow(catchment_stations_gpd_scena, snow_val_scena)   \n\n\n\nLet‚Äôs have a look at the SCA extracted at the station Plata Osservatore and it‚Äôs in situ measurements.\n\ncatchment_stations_gpd_plata.sample(5)\n\n\n\nDisplay snow presence threshold in in-situ data for Plata Osservatore\n\ncatchment_stations_gpd_plata.plot(x=\"id\", y=\"HS_after_gapfill\",rot=45,kind=\"line\",marker='o')\nplt.axhline(y = 0.4, color = \"r\", linestyle = \"-\")\nplt.show()\n\n\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#combine-in-situ-measurements-with-sca-results-at-the-stations","position":41},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Validate the SCA results with the snow station measurements"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#validate-the-sca-results-with-the-snow-station-measurements","position":42},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Validate the SCA results with the snow station measurements"},"content":"Now that we have combined the SCA results with the snow station measurements we can start the actual validation. A confusion matrix compares the classes of the station data to the classes of the SCA result. The numbers can be used to calculate the accuracy (correctly classified cases / all cases).\n\n\n\nno_snow\n\nsnow\n\nno_snow\n\ncorrect\n\nerror\n\nsnow\n\nerror\n\ncorrect\n\nimport seaborn as sns\n\n\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(10, 6))\n\nfig.suptitle(\"Error matrices for snow stations within our selected Catchment\")\nsns.heatmap(validation_metrics(smartino_snow)[1], annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax1)\nax1.set_title(\"San Martino in Passiria Osservatore\")\nax1.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n\nsns.heatmap(validation_metrics(rifiano_snow)[1], annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax2)\nax2.set_title(\"Rifiano Beobachter\")\nax2.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n\nsns.heatmap(validation_metrics(plata_snow)[1], annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax3)\nax3.set_title(\"Plata Osservatore\")\nax3.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\n\nsns.heatmap(validation_metrics(scena_snow)[1], annot=True, xticklabels=[\"No Snow\", \"Snow\"], yticklabels=[\"No Snow\", \"Snow\"], ax=ax4)\nax4.set_title(\"Scena Osservatore\")\nax4.set(xlabel=\"Predicted label\", ylabel=\"True label\")\n\nfig.tight_layout()\n\n\n\nThe accuracy of the snow estimate from the satellite image computation for each station is shown below:\n\nOn-site snow station\n\nAccuracy\n\nSan Martino in Passiria Osservatore\n\n100.00%\n\nRifiano Beobachter\n\n100.00%\n\nPlata Osservatore\n\n92.3%\n\nSan Leonardo in Passiria Osservatore\n\nNaN\n\nScena Osservatore\n\n100.00%\n\nThe fourth station San Leonardo in Passiria Osservatore recorded NaNs for snow depths for our selected dates, which could potentially be as a results of malfunctioning on-site equipments. Hence, we are not able to verify for it. But overall, the validation shows a 100% accuracy for stations San Martino in Passiria Osservatore, Rifiano Beobachter and Scena Osservatore, while station Plata Osservatore has False Positives decreasing the overall accuracy. This shows a good match between estimated snow values from satellite datasets and on-the ground measurements of the presence of snow.\n\n","type":"content","url":"/lectures/validation/exercises/validation-pangeo#validate-the-sca-results-with-the-snow-station-measurements","position":43},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Compare to discharge data"},"type":"lvl2","url":"/lectures/validation/exercises/validation-pangeo#compare-to-discharge-data","position":44},{"hierarchy":{"lvl1":"3.2 Validation of the results with Pangeo","lvl2":"Compare to discharge data"},"content":"In addition to computing metrics for validating the data, we also check the plausibility of our results. We compare our results with another measure with a known relationship. In this case, we compare the snow cover area time series with the discharge time-series at the main outlet of the catchment. We suspect that after snow melting starts, with a temporal lag, the runoff will increase. Let‚Äôs see if this holds true.\n\nLoad the discharge data at Meran, the main outlet of the catchment. We have prepared this data set for you, it‚Äôs extracted from Eurac‚Äôs \n\nEnvironmental Data Platform Alpine Drought Observatory Discharge Hydrological Datasets).\n\ndischarge_ds = pd.read_csv('32_data/ADO_DSC_ITH1_0025.csv', \n                           sep=',', index_col='Time', parse_dates=True)\ndischarge_ds.head()\n\n\n\nLoad the SCA time series we have generated in a previous exercise. It‚Äôs the time series of the aggregated snow cover area percentage for the whole catchment. Please note: you need to complete the 3.1 exercise before proceeding!\n\nsnow_perc_df = pd.read_csv(\"../../3.1_data_processing/exercises/31_results/filtered_snow_fraction_pangeo.csv\", \n                          sep=',', index_col='date', parse_dates=True)\nsnow_perc_df.head()\n\n\n\nLet‚Äôs plot the relationship between the snow covered area and the discharge in the catchment.\n\nstart_date = date(2018, 2, 1)\nend_date = date(2018, 6, 30)\n# filter discharge data to start and end dates\ndischarge_ds = discharge_ds.loc[start_date:end_date]\n\nax1 = discharge_ds.discharge_m3_s.plot(label='Discharge', xlabel='', ylabel='Discharge (m$^3$/s)')\nax2 = snow_perc_df[\"SCA\"].plot(marker='o', secondary_y=True, label='SCA', xlabel='', ylabel='Snow cover area (%)')\nax1.legend(loc='center left', bbox_to_anchor=(0, 0.6))\nax2.legend(loc='center left', bbox_to_anchor=(0, 0.5))\nplt.show()\n\n\n\nThe relationship looks as expected! Once the snow cover decreases the runoff increases!","type":"content","url":"/lectures/validation/exercises/validation-pangeo#compare-to-discharge-data","position":45},{"hierarchy":{"lvl1":"3.3 Data Sharing"},"type":"lvl1","url":"/lectures/data-sharing/data-sharing","position":0},{"hierarchy":{"lvl1":"3.3 Data Sharing"},"content":"","type":"content","url":"/lectures/data-sharing/data-sharing","position":1},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Learning Objectives"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#learning-objectives","position":2},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Learning Objectives"},"content":"Carry out an EO workflow on a cloud platform independently\n\nMake our results open and FAIR\n\nCollaborate with a community of researchers to reach a common goal","type":"content","url":"/lectures/data-sharing/data-sharing#learning-objectives","position":3},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Introduction"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#introduction","position":4},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Introduction"},"content":"We have reached the last chapter of the course. You know about data cubes, cloud platforms and open science. Now it‚Äôs time to prove it! We will apply everything we have learned so far and complete our own EO workflow on a cloud platform adhering to the open science principles.\nWe have carried out a full EO workflow to produce snow cover information in an alpine catchment. To make our results impactful we need to make them openly available to other researchers and the general public. Therefore we are going to learn how to share our data set (and code) properly - following the FAIR principles. We have learned about the concepts of open science in lecture \n\n1.3 open science. Now we are going to apply them! We are going to create a snow cover area map of the alps together with all the participants of the course. Everyone adds their contribution to a shared map. With every participant another small patch of the alps gets mapped! The map is openly available so that everybody can track our progress, the data is openly available and you can point to the patch you have provided!\n\n \n\nVideo content in cooperation with \n\nLeandro Parente (OpenGeoHub). \n‚ÄúConnect - Create - Share - Repeat‚Äù \nLinks to OpenGeoHub‚Äôs open science projects mentioned in the video:\n\nopengeohub\n\nopen environmental data cube: \n\nwebgis, \n\nstac catalogue, \n\ncode and documentation, \n\npublication,\n\nopen earth monitor cyberinfrastructure","type":"content","url":"/lectures/data-sharing/data-sharing#introduction","position":5},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"The steps of your open science journey"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#the-steps-of-your-open-science-journey","position":6},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"The steps of your open science journey"},"content":"","type":"content","url":"/lectures/data-sharing/data-sharing#the-steps-of-your-open-science-journey","position":7},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl3":"Produce your own map","lvl2":"The steps of your open science journey"},"type":"lvl3","url":"/lectures/data-sharing/data-sharing#produce-your-own-map","position":8},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl3":"Produce your own map","lvl2":"The steps of your open science journey"},"content":"Reuse the workflow to generate the snow covered area data cube\n\nAdapt the workflow to generate your personal contribution to mapping the snow covered area of the alps\n\nChoose an extent on the map that hasn‚Äôt been mapped yet. We are producing patches of roughly 1km by 1km.\n\nChoose a time extent. The winter months of a given year.\n\nAdd a step to reduce the time dimension. We want to create one layer for the winter season of the given year.\n\nAdapt the file format to Cloud Optimized GeoTiff (perfectly suitable for a raster file with one time step and one band).\n\nDownload the result consisting of the STAC metadata and the COG","type":"content","url":"/lectures/data-sharing/data-sharing#produce-your-own-map","position":9},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl3":"Make the data FAIR and open","lvl2":"The steps of your open science journey"},"type":"lvl3","url":"/lectures/data-sharing/data-sharing#make-the-data-fair-and-open","position":10},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl3":"Make the data FAIR and open","lvl2":"The steps of your open science journey"},"content":"\n\nCustomize the STAC metadata (e.g. adding you as the author)\n\nTrigger the update of the STAC catalogue and web map by submitting your results. Your data will be openly available!\n\nA license is assigned to the whole collection of all the produced patches\n\nA doi is assigned to the whole collection of all the produced patches","type":"content","url":"/lectures/data-sharing/data-sharing#make-the-data-fair-and-open","position":11},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl3":"Evaluate how FAIR the result is","lvl2":"The steps of your open science journey"},"type":"lvl3","url":"/lectures/data-sharing/data-sharing#evaluate-how-fair-the-result-is","position":12},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl3":"Evaluate how FAIR the result is","lvl2":"The steps of your open science journey"},"content":"Do the FAIR self assessment tool after you‚Äôve created your results\n\nYou will get a score on how FAIR the dataset you have produced really is","type":"content","url":"/lectures/data-sharing/data-sharing#evaluate-how-fair-the-result-is","position":13},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Exercise"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#exercise","position":14},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Exercise"},"content":"Time to start your own open science journey. Produce a snow cover area map for a region that hasn‚Äôt been mapped yet. FAIRify your results and make them publicly available! Please run both exercises, mapping different areas in each exercise.\n\nExercise 3.3 Sharing with openEO\n\nExercise 3.3 Sharing with Pangeo","type":"content","url":"/lectures/data-sharing/data-sharing#exercise","position":15},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"FAIR Assessment"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#fair-assessment","position":16},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"FAIR Assessment"},"content":"This tool allows you to check how FAIR your results are. Give it a shot!\n\nEmbed: \n\nhttps://‚Äãardc‚Äã.edu‚Äã.au‚Äã/resource‚Äã/fair‚Äã-data‚Äã-self‚Äã-assessment‚Äã-tool/","type":"content","url":"/lectures/data-sharing/data-sharing#fair-assessment","position":17},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Community Mapping Project"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#community-mapping-project","position":18},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Community Mapping Project"},"content":"Have a look at your results! Have a look at the community mapping project you have contributed to!\n\nSTAC Browser Cubes and Clouds - Snow Cover","type":"content","url":"/lectures/data-sharing/data-sharing#community-mapping-project","position":19},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Congratulations"},"type":"lvl2","url":"/lectures/data-sharing/data-sharing#congratulations","position":20},{"hierarchy":{"lvl1":"3.3 Data Sharing","lvl2":"Congratulations"},"content":"Congrats! You‚Äôve made it through the Cubes and Clouds online course!\n\n ","type":"content","url":"/lectures/data-sharing/data-sharing#congratulations","position":21},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO"},"type":"lvl1","url":"/lectures/data-sharing/exercises/data-sharing-openeo","position":0},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO"},"content":"\n\n\n\nScience is much more impactful once it‚Äôs shared. Therefore, we are going to learn how to\nopen up our scientific output from a cloud platform, so that is openly available - and\nhas the chance to make the impact it should.\n\nReuse the workflow we have used before for creating the snow covered area\n\nSelect AOI,\n\nRecreate process graph,\n\nDownload results for one time-step\n\nA Snow Cover Area map in the COG format\n\nA STAC metadata item that is provided with the result from openEO at CDSE\n\nAdapt the STAC item\n\nUpload the results and make them available openly via a STAC browser and web map\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo","position":1},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#libraries","position":2},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Libraries"},"content":"\n\nStart by creating the folders and data files needed to complete the exercise\n\n!mkdir -p 33_results\n\n\n\nimport json\nimport os\nimport subprocess\nfrom datetime import datetime\n\nimport openeo\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nimport ipyleaflet\n\nimport rioxarray as rio\nfrom _33_cubes_utilities import (\n    calculate_sca,\n    create_bounding_box,\n    extract_metadata_geometry, \n    extract_metadata_time\n)\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#libraries","position":3},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Login"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#login","position":4},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Login"},"content":"\n\nConnect to the copernicus dataspace ecosystem.\n\nconn = openeo.connect('https://openeo.dataspace.copernicus.eu/')\n\n\n\nAuthenticate login\n\nconn.authenticate_oidc()\n\n\n\nCheck if the login worked\n\nconn.describe_account()\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#login","position":5},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Select an Area of Interest and Time Frame"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#select-an-area-of-interest-and-time-frame","position":6},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Select an Area of Interest and Time Frame"},"content":"Start by selecting a center point of the area you would like to analyse from the map shown below. The starting extent is the full alps. Zoom in to an area and choose a region that has not been mapped yet. Make sure not to overlap too much with already mapped areas by having a look at the \n\nSTAC Collection. It‚Äôs a community mapping project :)\nCreate a 1 km bounding box around it. This will be the area you are calculating the snow covered area for.\n\nAttention: Execute the cell below to show the map. Zoom to a location you want to analyze. Use the \"draw a circlemarker\" button to select a point. A marker appears on the map. This is the center of your area of interest\n\nm = ipyleaflet.Map(\n    center=(47.005, 11.507),\n    zoom=7.5)\n\ndraw_control = ipyleaflet.DrawControl()\n\nm.add(draw_control)\nm\n\n\n\nAttention:\nNow this cell will get the coordinates of the marker you have placed. This will create a 1 km bounding box around the chosen location. And visualize it in the map above. The marker moves to the center when you zoom in\n\ngeom = draw_control.last_draw[\"geometry\"]['coordinates']\n\n# set distance of 1 km around bbox\ndistance_km = 1\n\n# Create a bounding box around the point\nbbox = create_bounding_box(geom[0], geom[1], distance_km)\nbbox\n\n\n\nNow we‚Äôll select the time frame. We‚Äôll start with the winter months of 2024.\n\ntemporal_extent = [\"2024-12-20\", \"2025-03-20\"]\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#select-an-area-of-interest-and-time-frame","position":7},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Reuse the process graph of the snow covered area data cube"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#reuse-the-process-graph-of-the-snow-covered-area-data-cube","position":8},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Reuse the process graph of the snow covered area data cube"},"content":"We‚Äôve saved the python code that we had used to create the snow cover area data cube into a python function calculate_sca(). It‚Äôs stored in cubes_utilities.py. It creates a 4 dimensional data cube with the dimensions: x, y, time, bands.\nAs parameters we have exposed the bounding box and temporal extent. We will update them with the choices we have made above.\n\nsnow_map_4dcube = calculate_sca(conn, bbox, temporal_extent)\nsnow_map_4dcube\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#reuse-the-process-graph-of-the-snow-covered-area-data-cube","position":9},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Reduce the time dimension"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#reduce-the-time-dimension","position":10},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Reduce the time dimension"},"content":"We want to calculate the SCA for the winter period of a given year. Therefore, we need to reduce the values along the time dimension. We‚Äôll use the process reduce_dimension() with a median() to accomplish this. We are directly continuing to build on our process graph that we have loaded above.\n\nsnow_map_3dcube = snow_map_4dcube.reduce_dimension(reducer=\"median\", dimension=\"t\")\nsnow_map_3dcube\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#reduce-the-time-dimension","position":11},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Download result"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#download-result","position":12},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Download result"},"content":"To finish our process graph we add the save_result() process choosing the GTiff format. It creates a COG out of the box with openEO on CDSE.\n\n# create a batch job\nsnowmap_cog = snow_map_3dcube.save_result(format = \"GTiff\") #, options = {\"overviews\": \"AUTO\"})\n\n\n\nWe register the job as a batch job on the backend and start the processing. Depending on the traffic on the backend, this usually takes between 1 to 5 minutes.\n\njob = snowmap_cog.create_job(title=\"snowmap_cog\")\njob.start_and_wait()\n\n\n\nNow let‚Äôs wait until the job is finished and then download the results.\n\nif job.status() == \"finished\":\n    results = job.get_results()\n    results.download_files(\"33_results/\")\n\n\n\nAdd statistics to the dataset via gdal, such as a summary of the values within the dataset and also some metadata, i.e. the legend (TIFFTAGS).  And we reduce the datatype to the lowest possible datatype supported by COG uint8, since only have three values to represent (0, 1, 2). If you‚Äôre interested you can check what happened via !gdalinfo 33_results/openEO_uint8.tif\n\n!gdal_translate -mo {TIFFTAG_IMAGEDESCRIPTION}=SnowCoveredArea_0=nosnow_1=snow_2-nodatavalue=cloud -ot Byte -of COG -a_nodata 2 -stats \"33_results/openEO.tif\" \"33_results/openEO_uint8.tif\"\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#download-result","position":13},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Load results"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#load-results","position":14},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Load results"},"content":"Now we can open the COG and visualize it.\n\nsnowmap = rio.open_rasterio(\"33_results/openEO_uint8.tif\", decode_coords=\"all\")\nsnowmap\n\n\n\nNow, we check if the nodata value can be determined directly from the COG metadata\n\nsnowmap.rio.nodata\n\n\n\nNow, we make a plot of the snowmap keeping in mind that 0 = no snow, 1 = snow, and 2 = clouds (nodata value)\n\nsnowmap.plot(levels=[0, 1, 2])\nplt.title(\"Spatial distribution of snow, no snow and cloudy pixels\")\nplt.ylabel(\"Latitude\")\nplt.xlabel(\"Longitude\")\nplt.tight_layout()\n\n\n\nLet‚Äôs have a look at the histogram to understand the distribution of the values in our map\n\ndata = snowmap.values.flatten()\nsnowmap.plot.hist(xticks = [0, 1, 2], weights=np.ones(len(data)) / len(data))\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.gca().set_xticklabels([\"0 - No Snow\", \"1 - Snow\", \"2 - Cloud\"])\nplt.title(\"Distribution of no snow, snow and cloud pixels\")\nplt.show()\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#load-results","position":15},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Load STAC metadata"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-openeo#load-stac-metadata","position":16},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl2":"Load STAC metadata"},"content":"In addition to the COG we also receive STAC metadata for our result.\nLet‚Äôs have a look at it.\n\nstac_collection = results.get_metadata()\nstac_collection\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#load-stac-metadata","position":17},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl3":"Adding Author of the data","lvl2":"Load STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-openeo#adding-author-of-the-data","position":18},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl3":"Adding Author of the data","lvl2":"Load STAC metadata"},"content":"Add your information to become visible as author of the data -  description of each field can be found here: \n\nitem‚Äã-spec‚Äã/common‚Äã-metadata‚Äã.md\n\nPlease note that leaving the field empty will lead to failed validation of STAC item\n\nAttention: Enter your full name and a short description of the snowmap you generated e.g. name = \"Jane Doe\" and description = \"snow map of Merano\"\n\nname = \"Michele Claus\"\ndescription = \"Snow map of Monte Bondone\"\n\n\n\nauthor = [{\n    \"name\": name,\n    \"description\": description,\n    \"roles\": [\"processor\"],\n}]\n\nproviders = stac_collection[\"providers\"] + author\n\nauthor_id = [nam[:2] for nam in author[0][\"name\"].split(\" \")]\n\n# generate timestamp\nts = datetime.now().isoformat()\nts = ts.split(\"T\")[0]\n\n\n\nExtract bbox information and temporal extent from the STAC collection that was delivered with the result from OpenEO. We are reusing it to create our STAC item. We have prepared these function for you extract_metadata_geometry and extract_metadata_time\n\ngeometry = extract_metadata_geometry(stac_collection)[1]\n\n\n\nstart_time, end_time = extract_metadata_time(stac_collection)\n\n\n\nSince we calculated the statistics and renamed the file, we have to add this new file name to the STAC item.\n\nfilename = \"openEO_uint8.tif\"\n\n\n\nLet‚Äôs create the actual STAC item describing your data! As talked about in previous lessons, STAC item has various required fields which need to be present and filled correctly. For the field ID we assign the fixed name snowcover and the initials of your name. That will be visible on the STAC browser once you have submitted the result!\n\nstac_item = {\n    \"type\": \"Feature\", \n    \"stac_version\": stac_collection[\"stac_version\"],\n    \"stac_extensions\": [],\n    \"id\": \"snowcover_\" + \"\".join(author_id).lower()+ \"_openeo_\" + str(ts),\n    \"geometry\": geometry,\n    \"bbox\": bbox,\n    \"properties\": {\n       \"datetime\": None, \n        \"start_datetime\": start_time,\n        \"end_datetime\": end_time,\n        \"providers\" : providers\n                 },\n    \n    \"links\": stac_collection[\"links\"],\n    \"assets\": {\"visual\": {\n      \"href\": filename,\n      \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n      \"title\": \"Snow coverage\",\n      \"roles\": [\n        \"data\"\n              ]\n            }\n        },\n}\n\n\n\nstac_item\n\n\n\nSaving the resulting item as stac_item.json into results folder\n\nstac_json = json.dumps(stac_item)\nwith open(\"33_results/stac_item.json\", \"w\") as file:\n    file.write(stac_json)\n\n\n\nValidating that STAC item is important - non valid STAC will not be displayed in the STAC browser after upload\n\nfrom stac_validator import stac_validator\nimport requests\nstac = stac_validator.StacValidate()\nf = open('33_results/stac_item.json')\ndata = json.load(f)\nstac.validate_dict(data)\nprint(stac.message)\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#adding-author-of-the-data","position":19},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl3":"Now it is time to upload solution to the submission folder and make results visible in STAC browser","lvl2":"Load STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-openeo#now-it-is-time-to-upload-solution-to-the-submission-folder-and-make-results-visible-in-stac-browser","position":20},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl3":"Now it is time to upload solution to the submission folder and make results visible in STAC browser","lvl2":"Load STAC metadata"},"content":"\n\nUpload both the STAC json file and the final .tif file to ‚Äúsubmissions‚Äù folder in your home directory\n\nYou can use the code below to copy the results to the submissions folder\n\n!cp ./33_results/stac_item.json ~/submissions/\n!cp ./33_results/openEO_uint8.tif ~/submissions/\n\n\n\nAnd now by executing the cell below, update of the STAC browser will start. By this, you are uploading your results to the openly available STAC browser. This might take some minutes.\n\nenv_var1 = os.getenv('JUPYTERHUB_ROOT_DIR').rsplit(\"/\")[-1]\ncurl_command = f\"curl -X POST -F token=glptt-42d31ac6f592a9e321d0e4877e654dc50dcf4854 -F ref=main -F 'variables[USER_DIRECTORY]=\\\"{env_var1}\\\"' https://gitlab.eox.at/api/v4/projects/554/trigger/pipeline\" \nprocess = subprocess.Popen(curl_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nstdout, stderr = process.communicate()\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#now-it-is-time-to-upload-solution-to-the-submission-folder-and-make-results-visible-in-stac-browser","position":21},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl3":"Your results are online!","lvl2":"Load STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-openeo#your-results-are-online","position":22},{"hierarchy":{"lvl1":"3.3 Data Sharing with openEO","lvl3":"Your results are online!","lvl2":"Load STAC metadata"},"content":"You can now browse your results together with all the other submissions at the publicly available STAC Catalog! You can check your snow cover map, that you are correctly listed as the author and that your contribution has the correct name. The license on the STAC Collection ‚ÄúCubes and Clouds: Snow Cover‚Äù is CC-BY-4.0. The STAC Collection also has it‚Äôs own DOI.\n\nCongratulations you have just contributed to a community mapping project that is completely open source, open data and FAIR! Make sure to show it also to your friends, colleagues or potential employers :)\n\nhttps://‚Äãesa‚Äã.pages‚Äã.eox‚Äã.at‚Äã/cubes‚Äã-and‚Äã-clouds‚Äã-catalog‚Äã/browser‚Äã/‚Äã#‚Äã/‚Äã?‚Äã.language‚Äã=en\n\nIf you would like to redo your submission, you can still update your files in submissions folder and once ready, run again the code in the cell above.\n\nAttention: If you have previously opened the STAC browser, your old submission will be cached and not directly displayed. To circumvent this, open a private window from your browser.\n\nHappy coding!","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-openeo#your-results-are-online","position":23},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo"},"type":"lvl1","url":"/lectures/data-sharing/exercises/data-sharing-pangeo","position":0},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo"},"content":"\n\n\n\nScience becomes significantly more impactful when it is shared. Therefore, we will learn how to open up our scientific output from a cloud platform to ensure it is Findable, Accessible, Interoperable, and Reusable (FAIR), maximizing its potential to contribute to broader research and applications.\n\nWe will follow these steps:\n\nReuse the workflow previously used to create the snow-covered area.\n\nSelect the Area of Interest (AOI).\n\nDownload results for a single time-step:\n\nA Snow Cover Area map in the COG (Cloud Optimized GeoTIFF) format.\n\nA STAC (SpatioTemporal Asset Catalog) metadata item generated using the Pangeo ecosystem.\n\nAdapt the STAC metadata item to include standardized and machine-readable metadata, improving its findability and interoperability.\n\nUpload the results and make them openly available via:\n\nA STAC browser to enhance searchability and accessibility.\n\nA web map to facilitate visualization and usability for a wide range of stakeholders.\n\nBy ensuring the data is as compliant as possible with the FAIR principles, we aim to make it easier for researchers, policymakers, and other stakeholders to discover, access, and build upon the scientific outputs.\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo","position":1},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Libraries"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#libraries","position":2},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Libraries"},"content":"\n\nStart by creating the folders and data files needed to complete the exercise\n\n!mkdir -p 33_results\n\n\n\nimport json\nimport os\nimport subprocess\nfrom datetime import datetime, timezone\n\nimport numpy as np\n\nimport ipyleaflet\n\nimport pystac\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\n\nimport rioxarray as rio\n\nfrom _33_pangeo_utilities import (\n    calculate_sca,\n    create_bounding_box,\n    extract_metadata_stac, \n)\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#libraries","position":3},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Select an Area of Interest and Time Frame"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#select-an-area-of-interest-and-time-frame","position":4},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Select an Area of Interest and Time Frame"},"content":"Start by selecting a center point of the area you would like to analyse from the map shown below.\nUse the circle marker tool: \n\nThe starting extent is the full Alps. Zoom in to an area and choose a region that has not been mapped yet. Make sure not to overlap too much with already mapped areas by having a look at the \n\nSTAC Collection. It‚Äôs a community mapping project :)\nCreate a 1 km bounding box around it. This will be the area you are calculating the snow covered area for.\n\nAttention: Execute the cell below to show the map. Zoom to a location you want to analyze. Use the \"draw a circlemarker\" button to select a point. A marker appears on the map. This is the center of your area of interest\n\nm = ipyleaflet.Map(\n    center=(47.005, 11.507),\n    zoom=7.5)\n\ndraw_control = ipyleaflet.DrawControl()\n\nm.add(draw_control)\nm\n\n\n\nAttention:\nNow this cell will get the coordinates of the marker you have placed. This will create a 1 km bounding box around the chosen location. And visualize it in the map above. The marker moves to the center when you zoom in\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#select-an-area-of-interest-and-time-frame","position":5},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Create a bounding box once the location of interest is selected","lvl2":"Select an Area of Interest and Time Frame"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#create-a-bounding-box-once-the-location-of-interest-is-selected","position":6},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Create a bounding box once the location of interest is selected","lvl2":"Select an Area of Interest and Time Frame"},"content":"\n\ngeom = draw_control.last_draw[\"geometry\"]['coordinates']\n\n# set distance of 1 km around bbox\ndistance_km = 1\n\n# Create a bounding box around the point\nbbox = create_bounding_box(geom[0], geom[1], distance_km)\nbbox\n\n\n\nWe generate a polygon from the list points and add it on the map\n\n# Create polygon from lists of points\npolygon = ipyleaflet.Polygon(\n    locations=[(bbox[1], bbox[0]), (bbox[3], bbox[0]), (bbox[3], bbox[2]),(bbox[1], bbox[2])],\n    color=\"green\",\n    fill_color=\"green\"\n)\n\n# Add the polygon to the map\nm.add(polygon);\n\n\n\nNow we‚Äôll select the time frame. We‚Äôll start with the winter months of 2023.\n\ntemporal_extent = [\"2023-12-20\", \"2024-03-20\"]\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#create-a-bounding-box-once-the-location-of-interest-is-selected","position":7},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Reuse the function to compute the snow covered area data cube"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#reuse-the-function-to-compute-the-snow-covered-area-data-cube","position":8},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Reuse the function to compute the snow covered area data cube"},"content":"To enhance reusability and streamline our workflow, we‚Äôve encapsulated the Python code used to create the snow-covered area data cube into a reusable function, calculate_sca(). This function is stored in the _33_pangeo_utilities.py module.\n\nThe function generates a three-dimensional data cube with the dimensions x, y, and time. By exposing key parameters to the function, specifically the bounding box and temporal extent, we‚Äôve made it easy to adapt the function to different scenarios. For this exercise, we will update these parameters based on the choices we‚Äôve made above.","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#reuse-the-function-to-compute-the-snow-covered-area-data-cube","position":9},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Why Create Functions?","lvl2":"Reuse the function to compute the snow covered area data cube"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#why-create-functions","position":10},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Why Create Functions?","lvl2":"Reuse the function to compute the snow covered area data cube"},"content":"Creating reusable functions like calculate_sca() is a best practice in Python for several reasons:\n\nImproved Reusability: Functions allow you to reuse the same logic across multiple projects or analyses, reducing the need to rewrite code.\n\nModularity: Encapsulating code in functions helps organize scripts into logical sections, making them easier to understand and maintain.\n\nError Reduction: Reusing a thoroughly tested function minimizes the risk of introducing errors in your code.\n\nAdaptability: Exposing parameters like the bounding box and temporal extent allows you to customize the function for different datasets or regions without modifying the core logic.\n\nBy following this approach, we create more flexible, maintainable, and scalable workflows that can be easily shared with others or reused in future projects.\n\nWe will call the function with the bounding box and temporal extent we chose above.\n\nsnow_map_3dcube = calculate_sca(bbox, temporal_extent)\nsnow_map_3dcube\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#why-create-functions","position":11},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Reduce the time dimension"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#reduce-the-time-dimension","position":12},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Reduce the time dimension"},"content":"To calculate the Snow-Covered Area (SCA) for the winter period of a given year, we need to reduce the values along the time dimension. Using Xarray, we achieve this by applying the median() operation along the time axis.\n\nInstead of performing the computation immediately, we leverage lazy loading in Xarray, which allows us to build a process graph that defines the computation steps without executing them. This approach optimizes memory usage and efficiency, as the computation will only be executed when explicitly triggered.\n\nsnow_map_2dcube = snow_map_3dcube.median([\"time\"])\nsnow_map_2dcube\n\n\n\nAdditionally, we update the metadata associated with the data to ensure that it remains accurate and informative throughout the process. By directly building upon the process graph we loaded earlier, we incrementally prepare the computation while keeping the workflow efficient and modular.\nWe add attributes such as statistics to the dataset, such as a summary of the values within the dataset and also some metadata, i.e. the legend (TIFFTAGS).  And we reduce the datatype to the lowest possible datatype supported by COG uint8, since only have three values to represent (0, 1, 2).\n\nsnow_map_2dcube = snow_map_2dcube.rename(\"snow_covered_area\")\nsnow_map_2dcube = snow_map_2dcube.astype(\"uint8\")\n\n\n\nsnow_map_2dcube.attrs[\"TIFFTAG_IMAGEDESCRIPTION\"] = \"SnowCoveredArea_0=nosnow_1=snow_2-nodatavalue=cloud\"\nsnow_map_2dcube.attrs[\"STATISTICS_MAXIMUM\"] = 1\nsnow_map_2dcube.attrs[\"STATISTICS_MEAN\"] = 1\nsnow_map_2dcube.attrs[\"STATISTICS_MINIMUM\"] = 1\nsnow_map_2dcube.attrs[\"STATISTICS_STDDEV\"] = 0\nsnow_map_2dcube.attrs[\"_FillValue\"] = 2\nsnow_map_2dcube.attrs[\"scale_factor\"] = 1\nsnow_map_2dcube.attrs[\"add_offset\"] = 0\nsnow_map_2dcube.attrs[\"long_name\"] = \"Snow Covered Area\" \nsnow_map_2dcube.attrs[\"temporal_extent\"] = temporal_extent[0].replace(\"-\",\"/\") + \"-\" + temporal_extent[1].replace(\"-\",\"/\")\nsnow_map_2dcube.attrs[\"spatial_extent\"] =  json.dumps({\"crs\": 4326,\n                                            \"east\": bbox[2],\n                                            \"north\": bbox[3],\n                                            \"south\": bbox[1],\n                                            \"west\": bbox[0]})\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#reduce-the-time-dimension","position":13},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Save the result locally"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#save-the-result-locally","position":14},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Save the result locally"},"content":"With the process graph complete, we can now compute the results and save them to a file in the GTiff format, which automatically generates a Cloud Optimized GeoTIFF (COG) for efficient storage and access.\n\nsnow_map_2dcube.rio.to_raster(\"./33_results/snowmap_cog_pangeo_uint8.tif\", driver=\"COG\")\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#save-the-result-locally","position":15},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Open, load results, and create STAC catalog to share results"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#open-load-results-and-create-stac-catalog-to-share-results","position":16},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Open, load results, and create STAC catalog to share results"},"content":"Now we can open the COG and visualize it.\n\nsnowmap = rio.open_rasterio(\"./33_results/snowmap_cog_pangeo_uint8.tif\", decode_coords=\"all\")\nsnowmap\n\n\n\nNow, we check if the nodata value can be determined directly from the COG metadata\n\nNow, we make a plot of the snowmap keeping in mind that 0 = no snow, 1 = snow, and 2 = clouds (nodata value)\n\nsnowmap.plot(levels=[0, 1, 2])\nplt.title(\"Spatial distribution of snow, no snow and cloudy pixels\")\nplt.ylabel(\"Latitude\")\nplt.xlabel(\"Longitude\")\nplt.tight_layout()\n\n\n\nLet‚Äôs have a look at the histogram to understand the distribution of the values in our map\n\ndata = snowmap.values.flatten()\nsnowmap.plot.hist(xticks = [0, 1, 2], weights=np.ones(len(data)) / len(data))\n\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.title(\"Distribution of snow, no snow and cloud pixels\")\nplt.show()\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#open-load-results-and-create-stac-catalog-to-share-results","position":17},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Adding STAC metadata"},"type":"lvl2","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#adding-stac-metadata","position":18},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl2":"Adding STAC metadata"},"content":"In addition to the COG we also need to STAC metadata for our result.\nLet‚Äôs have a look at the metadata from the snowmap dataset\n\nsnowmap.attrs\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#adding-stac-metadata","position":19},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl4":"Adding Providers and Author of the data","lvl2":"Adding STAC metadata"},"type":"lvl4","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#adding-providers-and-author-of-the-data","position":20},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl4":"Adding Providers and Author of the data","lvl2":"Adding STAC metadata"},"content":"Add your information to become visible as author of the data -  description of each field can be found here: \n\ncommons‚Äã/common‚Äã-metadata‚Äã.md\n\nPlease note that leaving the field empty will lead to failed validation of STAC item\n\nAttention: Enter your full name and a short description of the snowmap you generated e.g. `name = \"Jane Doe\"` and `description = \"snow map of Merano\"`\n\nname = \"Your Name\"\ndescription = \"Snow map generated with the Pangeo ecosystem\"\n\n\n\n# generate timestamp\ndatetime_utc = datetime.now(tz=timezone.utc)\nts = datetime_utc.isoformat().split(\"T\")[0]\n\nauthor = [{\n    \"name\": name,\n    \"description\": description,\n    \"roles\": [\"processor\"],\n}]\n\nproviders = author\n\nauthor_id = [name[:2] for name in author[0][\"name\"].split(\" \")]\n\nstart_time, end_time = snowmap.attrs[\"temporal_extent\"].split(\"-\")\n\nspatial_extent = json.loads(snowmap.attrs[\"spatial_extent\"])\nfootprint = {\"type\": \"Polygon\", \"coordinates\": [[\n    [spatial_extent[\"west\"], spatial_extent[\"south\"]],\n    [spatial_extent[\"east\"], spatial_extent[\"south\"]],\n    [spatial_extent[\"east\"], spatial_extent[\"north\"]],\n    [spatial_extent[\"west\"], spatial_extent[\"north\"]],\n    [spatial_extent[\"west\"], spatial_extent[\"south\"]] ]]}\n\nbbox = (spatial_extent[\"west\"], spatial_extent[\"south\"], \n        spatial_extent[\"east\"], spatial_extent[\"north\"])\n\ntemporal_extent = [start_time.replace(\"/\",\"-\"), end_time.replace(\"/\",\"-\")]\n\ndata_providers, links = extract_metadata_stac(bbox, temporal_extent)\n\nproperties = { \"providers\" : providers }\n\n\n\nnew_item = pystac.Item(id=\"snowcover_\" + \"\".join(author_id).lower()+ \"_pangeo_\" + str(ts),\n                       geometry=footprint,\n                       bbox=bbox,\n                       datetime=None,\n                       start_datetime=datetime.strptime(start_time, '%Y/%m/%d'),\n                       end_datetime=datetime.strptime(end_time, '%Y/%m/%d'),\n                       properties=properties)\nnew_item\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#adding-providers-and-author-of-the-data","position":21},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Add this Jupyter Notebook as a link","lvl2":"Adding STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#add-this-jupyter-notebook-as-a-link","position":22},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Add this Jupyter Notebook as a link","lvl2":"Adding STAC metadata"},"content":"We use the \n\nExample Links extension specification for this purpose.\n\nExtension Maturity Classification: This extension is currently classified as a Proposal, meaning it is not yet fully endorsed and may be subject to future changes.\n\nlink_to_notebook = {\n      \"rel\": \"example\",\n      \"href\": \"https://github.com/eo-college/cubes-and-clouds/blob/main/lectures/3.3_data_sharing/exercises/33_data_sharing_pangeo.ipynb\",\n      \"title\": \"3.3 Data Sharing with Pangeo\",\n      \"description\": \"This Jupyter notebook creates the snow-covered area from a user-selected Area Of Interest (AOI), save it as a COG and share it as a STAC item.\",\n      \"type\": \"application/x-ipynb+json\",\n      \"example:container\": True,\n      \"example:language\": \"Python\"\n    }\n\n\n\nnew_item.add_link(pystac.link.Link.from_dict(link_to_notebook))\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#add-this-jupyter-notebook-as-a-link","position":23},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Add Asset and all its information to Item","lvl2":"Adding STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#add-asset-and-all-its-information-to-item","position":24},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Add Asset and all its information to Item","lvl2":"Adding STAC metadata"},"content":"\n\nnew_item.add_asset(\n    key=\"image\",\n    asset=pystac.Asset(\n        href=\"snowmap_cog_pangeo_uint8.tif\",\n        media_type=pystac.MediaType.COG,\n        title = \"Snow coverage\",\n        roles=[\"data\"]\n    )\n)\n\n\n\nnew_item.to_dict()\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#add-asset-and-all-its-information-to-item","position":25},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Saving the resulting item as stac_item.json into results folder","lvl2":"Adding STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#saving-the-resulting-item-as-stac-item-json-into-results-folder","position":26},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Saving the resulting item as stac_item.json into results folder","lvl2":"Adding STAC metadata"},"content":"\n\nstac_json = json.dumps(new_item.to_dict())\nwith open(\"./33_results/stac_item.json\", \"w\") as file:\n    file.write(stac_json)\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#saving-the-resulting-item-as-stac-item-json-into-results-folder","position":27},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Validation of the STAC item","lvl2":"Adding STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#validation-of-the-stac-item","position":28},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Validation of the STAC item","lvl2":"Adding STAC metadata"},"content":"Validating that STAC item is important - non valid STAC will not be displayed in the STAC browser after upload\n\nfrom stac_validator import stac_validator\nimport requests\n\nstac = stac_validator.StacValidate()\nf = open('./33_results/stac_item.json')\ndata = json.load(f)\nstac.validate_dict(data)\nprint(stac.message)\n\n\n\nAttention: Check if `valid_stac` is True. If not, check you have filled the author and description properly.\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#validation-of-the-stac-item","position":29},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Now it is time to upload solution to the submission folder and make results visible in STAC browser","lvl2":"Adding STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#now-it-is-time-to-upload-solution-to-the-submission-folder-and-make-results-visible-in-stac-browser","position":30},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Now it is time to upload solution to the submission folder and make results visible in STAC browser","lvl2":"Adding STAC metadata"},"content":"\n\nUpload both the STAC json file and the final .tif file to ‚Äúsubmissions‚Äù folder in your home directory\n\nYou can use the code below to copy the results to the submissions folder\n\n!cp ./33_results/stac_item.json ~/submissions/\n!cp ./33_results/snowmap_cog_pangeo_uint8.tif ~/submissions/\n\n\n\nAnd now by executing the cell below, update of the STAC browser will start. By this, you are uploading your results to the openly available STAC browser. This might take some minutes.\n\nenv_var1 = os.getenv('JUPYTERHUB_ROOT_DIR').rsplit(\"/\")[-1]\ncurl_command = f\"curl -X POST -F token=glptt-42d31ac6f592a9e321d0e4877e654dc50dcf4854 -F ref=main -F 'variables[USER_DIRECTORY]=\\\"{env_var1}\\\"' https://gitlab.eox.at/api/v4/projects/554/trigger/pipeline\" \nprocess = subprocess.Popen(curl_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nstdout, stderr = process.communicate()\n\n\n\n","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#now-it-is-time-to-upload-solution-to-the-submission-folder-and-make-results-visible-in-stac-browser","position":31},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Your results are online!","lvl2":"Adding STAC metadata"},"type":"lvl3","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#your-results-are-online","position":32},{"hierarchy":{"lvl1":"3.3 Data Sharing with Pangeo","lvl3":"Your results are online!","lvl2":"Adding STAC metadata"},"content":"You can now browse your results together with all the other submissions at the publicly available STAC Catalog! You can check your snow cover map, that you are correctly listed as the author and that your contribution has the correct name. The license on the STAC Collection ‚ÄúCubes and Clouds: Snow Cover‚Äù is CC-BY-4.0. The STAC Collection also has it‚Äôs own DOI.\n\nCongratulations you have just contributed to a community mapping project that is completely open source, open data and FAIR! Make sure to show it also to your friends, colleagues or potential employers :)\n\nhttps://‚Äãesa‚Äã.pages‚Äã.eox‚Äã.at‚Äã/cubes‚Äã-and‚Äã-clouds‚Äã-catalog‚Äã/browser‚Äã/‚Äã#‚Äã/‚Äã?‚Äã.language‚Äã=en\n\nIf you would like to redo your submission, you can still update your files in submissions folder and once ready, run again the code in the cell above.\n\nAttention: If you have previously opened the STAC browser, your old submission will be cached and not directly displayed. To circumvent this, open a private window from your browser.\n\nHappy coding!","type":"content","url":"/lectures/data-sharing/exercises/data-sharing-pangeo#your-results-are-online","position":33},{"hierarchy":{"lvl1":"Lectures"},"type":"lvl1","url":"/lectures/readme","position":0},{"hierarchy":{"lvl1":"Lectures"},"content":"This is the place where the content of the lectures is stored. Every lecture has its own folder.","type":"content","url":"/lectures/readme","position":1},{"hierarchy":{"lvl1":"Lectures","lvl2":"Structure"},"type":"lvl2","url":"/lectures/readme#structure","position":2},{"hierarchy":{"lvl1":"Lectures","lvl2":"Structure"},"content":"In the lectures folder there is\n\nMarkdown File: The main markdown file compiling all the information that will be displayed in the course.\n\nAssets: A folder called assests. It holds all media used in the lecture, e.g. images, videos, interactive graphics, code snippets, etc.\n\nExercises: If there are coding exercises in a lesson, the according notebooks and data are stored here.","type":"content","url":"/lectures/readme#structure","position":3}]}